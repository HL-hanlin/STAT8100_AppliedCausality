{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"[BC]_Apr24_cartpole.ipynb","provenance":[],"collapsed_sections":["HYrmsDz4fppf"]},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"},"accelerator":"GPU"},"cells":[{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"cXttaQe-507l","executionInfo":{"status":"ok","timestamp":1650786891112,"user_tz":240,"elapsed":28961,"user":{"displayName":"3 Lin","userId":"16848054003967173878"}},"outputId":"204dfbd0-e359-43d3-e0cb-f7ab4186291a"},"outputs":[{"output_type":"stream","name":"stdout","text":["Mounted at /content/drive\n"]}],"source":["from google.colab import drive\n","drive.mount('/content/drive')\n","\n","\n","import os\n","\n","import torch\n","os.chdir('/content/drive/MyDrive/ImitationLearning/Invariant-Causal-Imitation-Learning-main/')\n"]},{"cell_type":"markdown","source":["# load"],"metadata":{"id":"CMy2nIGPEiAs"}},{"cell_type":"code","source":["!pip install mpi4py \n","!pip install box2d-py\n","!pip install box2d \n","!pip3 install gym[Box_2D] \n","!pip install gym==0.17.2 -qqq\n","!pip install numpy~=1.18.2 -qqq\n","!pip install pandas~=1.0.4 -qqq\n","!pip install PyYAML~=5.4.1 -qqq\n","!pip install scikit-learn~=0.22.2 -qqq\n","!pip install scipy~=1.1.0 -qqq\n","!pip install stable-baselines~=2.10.1 -qqq\n","!pip install tensorflow~=1.15.0 -qqq\n","!pip install torch>=1.6.0 -qqq\n","!pip install tqdm~=4.32.1 -qqq\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"AhOh_Fj16Eow","executionInfo":{"status":"ok","timestamp":1650787032370,"user_tz":240,"elapsed":137762,"user":{"displayName":"3 Lin","userId":"16848054003967173878"}},"outputId":"e8c35590-3e4c-4459-c117-a9c3a45b8c8c"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Collecting mpi4py\n","  Downloading mpi4py-3.1.3.tar.gz (2.5 MB)\n","\u001b[K     |████████████████████████████████| 2.5 MB 8.2 MB/s \n","\u001b[?25h  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n","  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n","    Preparing wheel metadata ... \u001b[?25l\u001b[?25hdone\n","Building wheels for collected packages: mpi4py\n","  Building wheel for mpi4py (PEP 517) ... \u001b[?25l\u001b[?25hdone\n","  Created wheel for mpi4py: filename=mpi4py-3.1.3-cp37-cp37m-linux_x86_64.whl size=2185263 sha256=7aaaa65425ef3e3ed39886e34d43123fb03c55562f9822a7184b2269e3061d09\n","  Stored in directory: /root/.cache/pip/wheels/7a/07/14/6a0c63fa2c6e473c6edc40985b7d89f05c61ff25ee7f0ad9ac\n","Successfully built mpi4py\n","Installing collected packages: mpi4py\n","Successfully installed mpi4py-3.1.3\n","Collecting box2d-py\n","  Downloading box2d_py-2.3.8-cp37-cp37m-manylinux1_x86_64.whl (448 kB)\n","\u001b[K     |████████████████████████████████| 448 kB 6.9 MB/s \n","\u001b[?25hInstalling collected packages: box2d-py\n","Successfully installed box2d-py-2.3.8\n","Collecting box2d\n","  Downloading Box2D-2.3.10-cp37-cp37m-manylinux1_x86_64.whl (1.3 MB)\n","\u001b[K     |████████████████████████████████| 1.3 MB 9.8 MB/s \n","\u001b[?25hInstalling collected packages: box2d\n","Successfully installed box2d-2.3.10\n","Requirement already satisfied: gym[Box_2D] in /usr/local/lib/python3.7/dist-packages (0.17.3)\n","\u001b[33mWARNING: gym 0.17.3 does not provide the extra 'box_2d'\u001b[0m\n","Requirement already satisfied: numpy>=1.10.4 in /usr/local/lib/python3.7/dist-packages (from gym[Box_2D]) (1.21.6)\n","Requirement already satisfied: cloudpickle<1.7.0,>=1.2.0 in /usr/local/lib/python3.7/dist-packages (from gym[Box_2D]) (1.3.0)\n","Requirement already satisfied: pyglet<=1.5.0,>=1.4.0 in /usr/local/lib/python3.7/dist-packages (from gym[Box_2D]) (1.5.0)\n","Requirement already satisfied: scipy in /usr/local/lib/python3.7/dist-packages (from gym[Box_2D]) (1.4.1)\n","Requirement already satisfied: future in /usr/local/lib/python3.7/dist-packages (from pyglet<=1.5.0,>=1.4.0->gym[Box_2D]) (0.16.0)\n","\u001b[K     |████████████████████████████████| 1.6 MB 9.0 MB/s \n","\u001b[?25h  Building wheel for gym (setup.py) ... \u001b[?25l\u001b[?25hdone\n","\u001b[K     |████████████████████████████████| 20.1 MB 1.3 MB/s \n","\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n","tensorflow 2.8.0 requires tf-estimator-nightly==2.8.0.dev2021122109, which is not installed.\n","tensorflow 2.8.0 requires numpy>=1.20, but you have numpy 1.18.5 which is incompatible.\n","tables 3.7.0 requires numpy>=1.19.0, but you have numpy 1.18.5 which is incompatible.\n","jaxlib 0.3.2+cuda11.cudnn805 requires numpy>=1.19, but you have numpy 1.18.5 which is incompatible.\n","jax 0.3.4 requires numpy>=1.19, but you have numpy 1.18.5 which is incompatible.\n","datascience 0.10.6 requires folium==0.2.1, but you have folium 0.8.3 which is incompatible.\n","albumentations 0.1.12 requires imgaug<0.2.7,>=0.2.5, but you have imgaug 0.2.9 which is incompatible.\u001b[0m\n","\u001b[K     |████████████████████████████████| 10.1 MB 8.0 MB/s \n","\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n","google-colab 1.0.0 requires pandas>=1.1.0; python_version >= \"3.0\", but you have pandas 1.0.5 which is incompatible.\u001b[0m\n","\u001b[K     |████████████████████████████████| 636 kB 8.7 MB/s \n","\u001b[K     |████████████████████████████████| 7.1 MB 8.8 MB/s \n","\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n","yellowbrick 1.4 requires scikit-learn>=1.0.0, but you have scikit-learn 0.22.2.post1 which is incompatible.\n","imbalanced-learn 0.8.1 requires scikit-learn>=0.24, but you have scikit-learn 0.22.2.post1 which is incompatible.\u001b[0m\n","\u001b[K     |████████████████████████████████| 31.2 MB 1.2 MB/s \n","\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n","yellowbrick 1.4 requires scikit-learn>=1.0.0, but you have scikit-learn 0.22.2.post1 which is incompatible.\n","pymc3 3.11.4 requires scipy>=1.2.0, but you have scipy 1.1.0 which is incompatible.\n","plotnine 0.6.0 requires scipy>=1.2.0, but you have scipy 1.1.0 which is incompatible.\n","jaxlib 0.3.2+cuda11.cudnn805 requires numpy>=1.19, but you have numpy 1.18.5 which is incompatible.\n","jax 0.3.4 requires numpy>=1.19, but you have numpy 1.18.5 which is incompatible.\n","jax 0.3.4 requires scipy>=1.2.1, but you have scipy 1.1.0 which is incompatible.\n","imbalanced-learn 0.8.1 requires scikit-learn>=0.24, but you have scikit-learn 0.22.2.post1 which is incompatible.\n","albumentations 0.1.12 requires imgaug<0.2.7,>=0.2.5, but you have imgaug 0.2.9 which is incompatible.\u001b[0m\n","\u001b[K     |████████████████████████████████| 240 kB 8.6 MB/s \n","\u001b[K     |████████████████████████████████| 110.5 MB 1.5 kB/s \n","\u001b[K     |████████████████████████████████| 50 kB 8.1 MB/s \n","\u001b[K     |████████████████████████████████| 3.8 MB 57.9 MB/s \n","\u001b[K     |████████████████████████████████| 503 kB 71.6 MB/s \n","\u001b[K     |████████████████████████████████| 2.9 MB 50.3 MB/s \n","\u001b[?25h  Building wheel for gast (setup.py) ... \u001b[?25l\u001b[?25hdone\n","\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n","tensorflow-probability 0.16.0 requires gast>=0.3.2, but you have gast 0.2.2 which is incompatible.\n","kapre 0.3.7 requires tensorflow>=2.0.0, but you have tensorflow 1.15.5 which is incompatible.\u001b[0m\n","\u001b[K     |████████████████████████████████| 50 kB 4.2 MB/s \n","\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n","spacy 2.2.4 requires tqdm<5.0.0,>=4.38.0, but you have tqdm 4.32.2 which is incompatible.\n","panel 0.12.1 requires tqdm>=4.48.0, but you have tqdm 4.32.2 which is incompatible.\n","fbprophet 0.7.1 requires tqdm>=4.36.1, but you have tqdm 4.32.2 which is incompatible.\u001b[0m\n","\u001b[?25h"]}]},{"cell_type":"markdown","source":["#config"],"metadata":{"id":"UoOvpQGcfM2h"}},{"cell_type":"code","source":["\n","config = {\n","    \"ENV\": \"CartPole-v1\",\n","    \"ALG\": \"BCIRMStudent_Apr17\",\n","    \"NUM_TRAJS_GIVEN\": 20, #\n","    \"NUM_TRAINING_ENVS\": 2,\n","    \"NOISE_DIM\": 4,\n","    \"REP_SIZE\": 16,\n","    \"TRAJ_SHIFT\": 20, # 20,\n","    \"SAMPLING_RATE\": 5,\n","    \"NUM_STEPS_TRAIN\": 10000,\n","    \"NUM_TRAJS_VALID\": 100,\n","    \"NUM_REPETITIONS\": 10,\n","    \"BATCH_SIZE\": 64,\n","    \"MLP_WIDTHS\": 64,\n","    \"ADAM_ALPHA\": 1e-3,\n","    \"SGLD_BUFFER_SIZE\": 10000,\n","    \"SGLD_LEARN_RATE\": 0.01,\n","    \"SGLD_NOISE_COEF\": 0.01,\n","    \"SGLD_NUM_STEPS\": 100,\n","    \"SGLD_REINIT_FREQ\": 0.05,\n","    \"NUM_STEPS_TRAIN_ENERGY_MODEL\": 1000,\n","    'TRIAL': 0\n","}\n","\n","\n","#config['ENV'] = \"LunarLander-v2\"\n","config['ENV'] = \"CartPole-v1\"\n","\n","#config['METHOD'] = \"BCIRM\"\n","config['METHOD'] = \"BC\"\n","\n","\n","\n","if config['METHOD'] == 'BCIRM':\n","    config['l2_regularizer_weight'] = 0.001\n","    config['penalty_weight'] = 10000\n","    config['penalty_anneal_iters'] = 2500\n","\n"],"metadata":{"id":"4qkwWpjMfNzN"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["#testing/il"],"metadata":{"id":"tXRGSCjZfbnX"}},{"cell_type":"code","source":["import argparse\n","import os\n","import pickle\n","\n","import gym\n","import numpy as np\n","import pandas as pd\n","import yaml\n","import numpy as np\n","\n","try:\n","    from paths import get_model_path, get_trajs_path  # noqa\n","except (ModuleNotFoundError, ImportError):\n","    from testing.paths import get_model_path, get_trajs_path  # pylint: disable=reimported\n","\n","from contrib.energy_model import EnergyModel\n","from contrib.env_wrapper import EnvWrapper, get_test_mult_factors\n","from network import (\n","    EnvDiscriminator,\n","    FeaturesDecoder,\n","    FeaturesEncoder,\n","    MineNetwork,\n","    ObservationsDecoder,\n","    StudentNetwork,\n",")\n","from student import ICILStudent, BCStudent, BCIRMStudent\n","from testing.train_utils import fill_buffer, make_agent, save_results\n"],"metadata":{"id":"JIj45lBefdU5","executionInfo":{"status":"ok","timestamp":1650787047966,"user_tz":240,"elapsed":15436,"user":{"displayName":"3 Lin","userId":"16848054003967173878"}},"colab":{"base_uri":"https://localhost:8080/"},"outputId":"d062bc0a-5646-4874-898a-6cf9fb2c4a1b"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.7/dist-packages/stable_baselines/__init__.py:33: UserWarning: stable-baselines is in maintenance mode, please use [Stable-Baselines3 (SB3)](https://github.com/DLR-RM/stable-baselines3) for an up-to-date version. You can find a [migration guide](https://stable-baselines3.readthedocs.io/en/master/guide/migration.html) in SB3 documentation.\n","  \"stable-baselines is in maintenance mode, please use [Stable-Baselines3 (SB3)](https://github.com/DLR-RM/stable-baselines3) for an up-to-date version. You can find a [migration guide](https://stable-baselines3.readthedocs.io/en/master/guide/migration.html) in SB3 documentation.\"\n"]}]},{"cell_type":"markdown","source":["# make student"],"metadata":{"id":"hsMME-Bm3oMT"}},{"cell_type":"code","source":["\n","\n","# pylint: disable=redefined-outer-name\n","def make_student(run_seed, config):\n","    env = gym.make(config[\"ENV\"])\n","    trajs_path = get_trajs_path(config[\"ENV\"], \"student_\" + config[\"ALG\"], env_id=\"student\", run_seed=run_seed)\n","    model_path = get_model_path(config[\"ENV\"], \"student_\" + config[\"ALG\"], run_seed=run_seed)\n","\n","    state_dim = env.observation_space.shape[0] + config[\"NOISE_DIM\"]\n","    action_dim = env.action_space.n\n","    num_training_envs = config[\"NUM_TRAINING_ENVS\"]\n","\n","    # run_seed = run_seed\n","    batch_size = config[\"BATCH_SIZE\"]\n","    teacher = make_agent(config[\"ENV\"], config[\"EXPERT_ALG\"], config[\"NUM_TRAINING_ENVS\"])\n","    teacher.load_pretrained()\n","\n","    buffer = fill_buffer(\n","        trajs_path=teacher.trajs_paths,\n","        batch_size=batch_size,\n","        run_seed=run_seed,\n","        traj_shift=config[\"TRAJ_SHIFT\"],\n","        buffer_size_in_trajs=config[\"NUM_TRAJS_GIVEN\"],\n","        sampling_rate=config[\"SAMPLING_RATE\"],\n","    )\n","\n","    if buffer.total_size < batch_size:\n","        batch_size = buffer.total_size\n","\n","\n","\n","    ##########################      COMMON      ##########################\n","\n","    print(\"state_dim\", state_dim)\n","\n","    causal_features_encoder = FeaturesEncoder(\n","        input_size=state_dim, representation_size=config[\"REP_SIZE\"], width=config[\"MLP_WIDTHS\"]\n","    )\n","\n","    policy_network = StudentNetwork(in_dim=config[\"REP_SIZE\"], out_dim=action_dim, width=config[\"MLP_WIDTHS\"])\n","\n","    #print(\"config method = \", config['METHOD'])\n","\n","\n","    ##########################       BC       #######################\n","\n","    if config['METHOD'] == 'BC':\n","\n","        return BCStudent(\n","            env=env,\n","            trajs_paths=trajs_path,\n","            model_path=model_path,\n","            num_training_envs=num_training_envs,\n","            teacher=teacher,\n","            causal_features_encoder=causal_features_encoder,\n","            policy_network=policy_network,\n","            buffer=buffer,\n","            adam_alpha=config[\"ADAM_ALPHA\"],\n","            config = config\n","        )\n","\n","\n","    ##########################       BC IRM       #######################\n","\n","\n","    elif config['METHOD'] == 'BCIRM':\n","\n","        return BCIRMStudent(\n","            env=env,\n","            trajs_paths=trajs_path,\n","            model_path=model_path,\n","            num_training_envs=num_training_envs,\n","            teacher=teacher,\n","            causal_features_encoder=causal_features_encoder,\n","            policy_network=policy_network,\n","            buffer=buffer,\n","            adam_alpha=config[\"ADAM_ALPHA\"],\n","            config = config\n","        )\n","\n","    ##########################       ICIL        #######################\n","\n","    elif config['METHOD'] == 'ICIL':\n","        energy_model = EnergyModel(\n","            in_dim=state_dim,\n","            width=config[\"MLP_WIDTHS\"],\n","            batch_size=batch_size,\n","            adam_alpha=config[\"ADAM_ALPHA\"],\n","            buffer=buffer,\n","            sgld_buffer_size=config[\"SGLD_BUFFER_SIZE\"],\n","            sgld_learn_rate=config[\"SGLD_LEARN_RATE\"],\n","            sgld_noise_coef=config[\"SGLD_NOISE_COEF\"],\n","            sgld_num_steps=config[\"SGLD_NUM_STEPS\"],\n","            sgld_reinit_freq=config[\"SGLD_REINIT_FREQ\"],\n","        )\n","        energy_model.train(num_updates=config[\"NUM_STEPS_TRAIN_ENERGY_MODEL\"])\n","\n","\n","        causal_features_decoder = FeaturesDecoder(\n","            action_size=action_dim, representation_size=config[\"REP_SIZE\"], width=config[\"MLP_WIDTHS\"]\n","        )\n","\n","        observations_decoder = ObservationsDecoder(\n","            representation_size=config[\"REP_SIZE\"], out_size=state_dim, width=config[\"MLP_WIDTHS\"]\n","        )\n","\n","\n","        env_discriminator = EnvDiscriminator(\n","            representation_size=config[\"REP_SIZE\"], num_envs=config[\"NUM_TRAINING_ENVS\"], width=config[\"MLP_WIDTHS\"]\n","        )\n","\n","        noise_features_encoders = [\n","            FeaturesEncoder(input_size=state_dim, representation_size=config[\"REP_SIZE\"], width=config[\"MLP_WIDTHS\"])\n","            for i in range(num_training_envs)\n","        ]\n","        noise_features_decoders = [\n","            FeaturesDecoder(action_size=action_dim, representation_size=config[\"REP_SIZE\"], width=config[\"MLP_WIDTHS\"])\n","            for i in range(num_training_envs)\n","        ]\n","\n","        mine_network = MineNetwork(x_dim=config[\"REP_SIZE\"], z_dim=config[\"REP_SIZE\"], width=config[\"MLP_WIDTHS\"])\n","\n","        return ICILStudent(\n","            env=env,\n","            trajs_paths=trajs_path,\n","            model_path=model_path,\n","            num_training_envs=num_training_envs,\n","            teacher=teacher,\n","            causal_features_encoder=causal_features_encoder,\n","            noise_features_encoders=noise_features_encoders,\n","            causal_features_decoder=causal_features_decoder,\n","            noise_features_decoders=noise_features_decoders,\n","            observations_decoder=observations_decoder,\n","            env_discriminator=env_discriminator,\n","            policy_network=policy_network,\n","            energy_model=energy_model,\n","            mine_network=mine_network,\n","            buffer=buffer,\n","            adam_alpha=config[\"ADAM_ALPHA\"],\n","            config = config\n","        )\n","\n","\n","def init_arg():\n","    parser = argparse.ArgumentParser()\n","    parser.add_argument(\"--env_name\", default=\"CartPole-v1\")\n","    parser.add_argument(\"--num_trajectories\", default=20, type=int)\n","    parser.add_argument(\"--trial\", default=0, type=int)\n","    return parser.parse_args()\n"],"metadata":{"id":"BYsHrHrlffKj"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["#10 Trails -- BCIRM"],"metadata":{"id":"PX5cO17BRrkC"}},{"cell_type":"code","source":["config['METHOD'] = \"BCIRM\"\n","\n","for traj_num in [128]:\n","    config[\"NUM_TRAJS_GIVEN\"] = traj_num\n","    config[\"TRAJ_SHIFT\"] = traj_num\n","\n","    config['ALG'] = \"FINAL_BCIRMStudent_replicatedata_trajnum\" + str(traj_num)\n","\n","\n","    ###############.  settings   ###############\n","    #config['ALG'] = \"BCIRMStudent_Apr19_replicatedata\"\n","    #config['METHOD'] = \"BCIRM\"\n","    #config['METHOD'] = \"ICIL\"\n","    #config[\"NUM_TRAJS_GIVEN\"] = 50\n","    #config[\"TRAJ_SHIFT\"] = 50\n","    #config['ENV'] == \"CartPole-v1\"\n","    ###############.  settings   ###############\n","\n","\n","    if config['METHOD'] == 'BCIRM':\n","        config['l2_regularizer_weight'] = 0.001\n","        config['penalty_weight'] = 10000\n","        config['penalty_anneal_iters'] = 5000\n","\n","    all_results_trail = []\n","\n","    for trail in range(1): \n","        config['TRIAL'] = trail \n","\n","\n","        ###############.  start a trail   ###############\n","\n","        config[\"EXPERT_ALG\"] = yaml.load(open(\"testing/config.yml\"), Loader=yaml.FullLoader)[config[\"ENV\"]]\n","        print(\"Config: %s\" % config)\n","\n","        TRIAL = config[\"TRIAL\"] #args.trial\n","        print(\"Trial number %s\" % TRIAL)\n","\n","        results_dir_base = \"testing/results/\"\n","        results_dir = os.path.join(results_dir_base, config[\"ENV\"], str(config[\"NUM_TRAJS_GIVEN\"]), config[\"ALG\"])\n","\n","        if not os.path.exists(results_dir):\n","            os.makedirs(results_dir)\n","\n","        config_file = \"trial_\" + str(TRIAL) + \"_\" + \"config.pkl\"\n","\n","        results_file_name = \"trial_\" + str(TRIAL) + \"_\" + \"results.csv\"\n","        results_file_path = os.path.join(results_dir, results_file_name)\n","\n","        if os.path.exists(os.path.join(results_dir, config_file)):\n","            raise NameError(\"CONFIG file already exists %s. Choose a different trial number.\" % config_file)\n","        pickle.dump(config, open(os.path.join(results_dir, config_file), \"wb\"))\n","\n","\n","\n","\n","        ###############.  10 runs for each trail   ###############\n","\n","        print(\"config method = \", config['METHOD'])\n","        print(\"config env = \", config['ENV'])\n","\n","        for run_seed in range(config[\"NUM_REPETITIONS\"]):\n","            print(\"Run %s out of %s\" % (run_seed + 1, config[\"NUM_REPETITIONS\"]))\n","            student = make_student(run_seed, config)\n","            student.train(num_updates=config[\"NUM_STEPS_TRAIN\"])\n","\n","            env_wrapper_out_of_sample = EnvWrapper(\n","                env=gym.make(config[\"ENV\"]), mult_factor=get_test_mult_factors(config['NOISE_DIM'] - 1), idx=3, seed=1\n","            )\n","\n","            env_wrapper_out_of_sample.noise = 0\n","\n","            action_match, return_mean, return_std = student.test(\n","                num_episodes=config[\"NUM_TRAJS_VALID\"], env_wrapper=env_wrapper_out_of_sample\n","            )\n","\n","            result = (action_match, return_mean, return_std)\n","            print(\"###############    Reward for test environment for run %s: %s.   ###############\\n\\n\" % (run_seed + 1, return_mean))\n","            save_results(results_file_path, run_seed, action_match, return_mean, return_std)\n","\n","        results_trial = pd.read_csv(\n","            \"testing/results/\"\n","            + config[\"ENV\"]\n","            + \"/\"\n","            + str(config[\"NUM_TRAJS_GIVEN\"])\n","            + \"/\"\n","            + config[\"ALG\"]\n","            + \"/trial_\"\n","            + str(TRIAL)\n","            + \"_results.csv\",\n","            header=None,\n","        )\n","\n","        print(\"Average reward for 10 repetitions: %s\" % np.mean(results_trial[2].values))\n","\n","        all_results_trail.append(np.mean(results_trial[2].values))\n","    print(\"ALL RESULTS TRAIL:\" , all_results_trail)\n","\n","\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"1fwAygz4RuFo","outputId":"94ae970b-b6a0-4094-bafe-6632fc81712c","executionInfo":{"status":"error","timestamp":1650529986602,"user_tz":240,"elapsed":809483,"user":{"displayName":"2 Lin","userId":"15119774230930098070"}}},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Config: {'ENV': 'CartPole-v1', 'ALG': 'FINAL_BCIRMStudent_replicatedata_trajnum128', 'NUM_TRAJS_GIVEN': 128, 'NUM_TRAINING_ENVS': 2, 'NOISE_DIM': 4, 'REP_SIZE': 16, 'TRAJ_SHIFT': 128, 'SAMPLING_RATE': 5, 'NUM_STEPS_TRAIN': 10000, 'NUM_TRAJS_VALID': 100, 'NUM_REPETITIONS': 15, 'BATCH_SIZE': 64, 'MLP_WIDTHS': 64, 'ADAM_ALPHA': 0.001, 'SGLD_BUFFER_SIZE': 10000, 'SGLD_LEARN_RATE': 0.01, 'SGLD_NOISE_COEF': 0.01, 'SGLD_NUM_STEPS': 100, 'SGLD_REINIT_FREQ': 0.05, 'NUM_STEPS_TRAIN_ENERGY_MODEL': 1000, 'TRIAL': 0, 'METHOD': 'BCIRM', 'l2_regularizer_weight': 0.001, 'penalty_weight': 10000, 'penalty_anneal_iters': 5000, 'EXPERT_ALG': 'dqn'}\n","Trial number 0\n","config method =  BCIRM\n","config env =  CartPole-v1\n","Run 1 out of 15\n","state_dim 8\n","200 tensor(0.3935, device='cuda:0', grad_fn=<AddBackward0>)\n","400 tensor(0.2807, device='cuda:0', grad_fn=<AddBackward0>)\n","600 tensor(0.3091, device='cuda:0', grad_fn=<AddBackward0>)\n","800 tensor(0.4803, device='cuda:0', grad_fn=<AddBackward0>)\n","1000 tensor(0.4519, device='cuda:0', grad_fn=<AddBackward0>)\n","1200 tensor(0.3638, device='cuda:0', grad_fn=<AddBackward0>)\n","1400 tensor(0.3266, device='cuda:0', grad_fn=<AddBackward0>)\n","1600 tensor(0.3338, device='cuda:0', grad_fn=<AddBackward0>)\n","1800 tensor(0.2571, device='cuda:0', grad_fn=<AddBackward0>)\n","2000 tensor(0.1828, device='cuda:0', grad_fn=<AddBackward0>)\n","2200 tensor(0.2614, device='cuda:0', grad_fn=<AddBackward0>)\n","2400 tensor(0.1991, device='cuda:0', grad_fn=<AddBackward0>)\n","2600 tensor(0.1662, device='cuda:0', grad_fn=<AddBackward0>)\n","2800 tensor(0.2508, device='cuda:0', grad_fn=<AddBackward0>)\n","3000 tensor(0.1496, device='cuda:0', grad_fn=<AddBackward0>)\n","3200 tensor(0.2505, device='cuda:0', grad_fn=<AddBackward0>)\n","3400 tensor(0.2874, device='cuda:0', grad_fn=<AddBackward0>)\n","3600 tensor(0.4081, device='cuda:0', grad_fn=<AddBackward0>)\n","3800 tensor(0.2287, device='cuda:0', grad_fn=<AddBackward0>)\n","4000 tensor(0.1823, device='cuda:0', grad_fn=<AddBackward0>)\n","4200 tensor(0.1881, device='cuda:0', grad_fn=<AddBackward0>)\n","4400 tensor(0.2324, device='cuda:0', grad_fn=<AddBackward0>)\n","4600 tensor(0.1906, device='cuda:0', grad_fn=<AddBackward0>)\n","4800 tensor(0.2368, device='cuda:0', grad_fn=<AddBackward0>)\n","5000 tensor(0.0007, device='cuda:0', grad_fn=<DivBackward0>)\n","5200 tensor(0.0010, device='cuda:0', grad_fn=<DivBackward0>)\n","5400 tensor(0.0059, device='cuda:0', grad_fn=<DivBackward0>)\n","5600 tensor(0.0046, device='cuda:0', grad_fn=<DivBackward0>)\n","5800 tensor(0.0028, device='cuda:0', grad_fn=<DivBackward0>)\n","6000 tensor(0.0135, device='cuda:0', grad_fn=<DivBackward0>)\n","6200 tensor(0.0043, device='cuda:0', grad_fn=<DivBackward0>)\n","6400 tensor(0.0026, device='cuda:0', grad_fn=<DivBackward0>)\n","6600 tensor(0.0012, device='cuda:0', grad_fn=<DivBackward0>)\n","6800 tensor(0.0009, device='cuda:0', grad_fn=<DivBackward0>)\n","7000 tensor(0.0031, device='cuda:0', grad_fn=<DivBackward0>)\n","7200 tensor(0.0104, device='cuda:0', grad_fn=<DivBackward0>)\n","7400 tensor(0.0164, device='cuda:0', grad_fn=<DivBackward0>)\n","7600 tensor(0.0036, device='cuda:0', grad_fn=<DivBackward0>)\n","7800 tensor(0.0015, device='cuda:0', grad_fn=<DivBackward0>)\n","8000 tensor(0.0042, device='cuda:0', grad_fn=<DivBackward0>)\n","8200 tensor(0.0166, device='cuda:0', grad_fn=<DivBackward0>)\n","8400 tensor(0.0003, device='cuda:0', grad_fn=<DivBackward0>)\n","8600 tensor(0.0061, device='cuda:0', grad_fn=<DivBackward0>)\n","8800 tensor(0.0001, device='cuda:0', grad_fn=<DivBackward0>)\n","9000 tensor(0.0044, device='cuda:0', grad_fn=<DivBackward0>)\n","9200 tensor(0.0034, device='cuda:0', grad_fn=<DivBackward0>)\n","9400 tensor(0.0032, device='cuda:0', grad_fn=<DivBackward0>)\n","9600 tensor(0.0005, device='cuda:0', grad_fn=<DivBackward0>)\n","9800 tensor(0.0007, device='cuda:0', grad_fn=<DivBackward0>)\n","10000 tensor(7.6830e-05, device='cuda:0', grad_fn=<DivBackward0>)\n","###############    Reward for test environment for run 1: 9.23.   ###############\n","\n","\n","Run 2 out of 15\n","state_dim 8\n","200 tensor(0.3841, device='cuda:0', grad_fn=<AddBackward0>)\n","400 tensor(0.2913, device='cuda:0', grad_fn=<AddBackward0>)\n","600 tensor(0.4285, device='cuda:0', grad_fn=<AddBackward0>)\n","800 tensor(0.2959, device='cuda:0', grad_fn=<AddBackward0>)\n","1000 tensor(0.3171, device='cuda:0', grad_fn=<AddBackward0>)\n","1200 tensor(0.3650, device='cuda:0', grad_fn=<AddBackward0>)\n","1400 tensor(0.3269, device='cuda:0', grad_fn=<AddBackward0>)\n","1600 tensor(0.3287, device='cuda:0', grad_fn=<AddBackward0>)\n","1800 tensor(0.2073, device='cuda:0', grad_fn=<AddBackward0>)\n","2000 tensor(0.2487, device='cuda:0', grad_fn=<AddBackward0>)\n","2200 tensor(0.2026, device='cuda:0', grad_fn=<AddBackward0>)\n","2400 tensor(0.3276, device='cuda:0', grad_fn=<AddBackward0>)\n","2600 tensor(0.1852, device='cuda:0', grad_fn=<AddBackward0>)\n","2800 tensor(0.1905, device='cuda:0', grad_fn=<AddBackward0>)\n","3000 tensor(0.2939, device='cuda:0', grad_fn=<AddBackward0>)\n","3200 tensor(0.2727, device='cuda:0', grad_fn=<AddBackward0>)\n","3400 tensor(0.2434, device='cuda:0', grad_fn=<AddBackward0>)\n","3600 tensor(0.4388, device='cuda:0', grad_fn=<AddBackward0>)\n","3800 tensor(0.4338, device='cuda:0', grad_fn=<AddBackward0>)\n","4000 tensor(0.2699, device='cuda:0', grad_fn=<AddBackward0>)\n","4200 tensor(0.1803, device='cuda:0', grad_fn=<AddBackward0>)\n","4400 tensor(0.2079, device='cuda:0', grad_fn=<AddBackward0>)\n","4600 tensor(0.3236, device='cuda:0', grad_fn=<AddBackward0>)\n","4800 tensor(0.2747, device='cuda:0', grad_fn=<AddBackward0>)\n","5000 tensor(0.0082, device='cuda:0', grad_fn=<DivBackward0>)\n","5200 tensor(0.0047, device='cuda:0', grad_fn=<DivBackward0>)\n","5400 tensor(0.0035, device='cuda:0', grad_fn=<DivBackward0>)\n","5600 tensor(0.0023, device='cuda:0', grad_fn=<DivBackward0>)\n","5800 tensor(0.0032, device='cuda:0', grad_fn=<DivBackward0>)\n","6000 tensor(0.0049, device='cuda:0', grad_fn=<DivBackward0>)\n","6200 tensor(0.0051, device='cuda:0', grad_fn=<DivBackward0>)\n","6400 tensor(0.0077, device='cuda:0', grad_fn=<DivBackward0>)\n","6600 tensor(0.0022, device='cuda:0', grad_fn=<DivBackward0>)\n","6800 tensor(0.0007, device='cuda:0', grad_fn=<DivBackward0>)\n","7000 tensor(0.0036, device='cuda:0', grad_fn=<DivBackward0>)\n","7200 tensor(0.0063, device='cuda:0', grad_fn=<DivBackward0>)\n","7400 tensor(0.0005, device='cuda:0', grad_fn=<DivBackward0>)\n","7600 tensor(0.0080, device='cuda:0', grad_fn=<DivBackward0>)\n","7800 tensor(0.0051, device='cuda:0', grad_fn=<DivBackward0>)\n","8000 tensor(0.0021, device='cuda:0', grad_fn=<DivBackward0>)\n","8200 tensor(0.0101, device='cuda:0', grad_fn=<DivBackward0>)\n","8400 tensor(0.0071, device='cuda:0', grad_fn=<DivBackward0>)\n","8600 tensor(0.0025, device='cuda:0', grad_fn=<DivBackward0>)\n","8800 tensor(0.0288, device='cuda:0', grad_fn=<DivBackward0>)\n","9000 tensor(0.0045, device='cuda:0', grad_fn=<DivBackward0>)\n","9200 tensor(0.0079, device='cuda:0', grad_fn=<DivBackward0>)\n","9400 tensor(0.0105, device='cuda:0', grad_fn=<DivBackward0>)\n","9600 tensor(0.0037, device='cuda:0', grad_fn=<DivBackward0>)\n","9800 tensor(0.0023, device='cuda:0', grad_fn=<DivBackward0>)\n","10000 tensor(0.0028, device='cuda:0', grad_fn=<DivBackward0>)\n","###############    Reward for test environment for run 2: 397.91.   ###############\n","\n","\n","Run 3 out of 15\n","state_dim 8\n","200 tensor(0.3878, device='cuda:0', grad_fn=<AddBackward0>)\n","400 tensor(0.3360, device='cuda:0', grad_fn=<AddBackward0>)\n","600 tensor(0.4275, device='cuda:0', grad_fn=<AddBackward0>)\n","800 tensor(0.4490, device='cuda:0', grad_fn=<AddBackward0>)\n","1000 tensor(0.2781, device='cuda:0', grad_fn=<AddBackward0>)\n","1200 tensor(0.2242, device='cuda:0', grad_fn=<AddBackward0>)\n","1400 tensor(0.2590, device='cuda:0', grad_fn=<AddBackward0>)\n","1600 tensor(0.3129, device='cuda:0', grad_fn=<AddBackward0>)\n","1800 tensor(0.2896, device='cuda:0', grad_fn=<AddBackward0>)\n","2000 tensor(0.1897, device='cuda:0', grad_fn=<AddBackward0>)\n","2200 tensor(0.1931, device='cuda:0', grad_fn=<AddBackward0>)\n","2400 tensor(0.2091, device='cuda:0', grad_fn=<AddBackward0>)\n","2600 tensor(0.2243, device='cuda:0', grad_fn=<AddBackward0>)\n","2800 tensor(0.2580, device='cuda:0', grad_fn=<AddBackward0>)\n","3000 tensor(0.2416, device='cuda:0', grad_fn=<AddBackward0>)\n","3200 tensor(0.1622, device='cuda:0', grad_fn=<AddBackward0>)\n","3400 tensor(0.1907, device='cuda:0', grad_fn=<AddBackward0>)\n","3600 tensor(0.2218, device='cuda:0', grad_fn=<AddBackward0>)\n","3800 tensor(0.2148, device='cuda:0', grad_fn=<AddBackward0>)\n","4000 tensor(0.1599, device='cuda:0', grad_fn=<AddBackward0>)\n","4200 tensor(0.2251, device='cuda:0', grad_fn=<AddBackward0>)\n","4400 tensor(0.2556, device='cuda:0', grad_fn=<AddBackward0>)\n","4600 tensor(0.1710, device='cuda:0', grad_fn=<AddBackward0>)\n","4800 tensor(0.2261, device='cuda:0', grad_fn=<AddBackward0>)\n","5000 tensor(0.0008, device='cuda:0', grad_fn=<DivBackward0>)\n","5200 tensor(0.0029, device='cuda:0', grad_fn=<DivBackward0>)\n","5400 tensor(0.0045, device='cuda:0', grad_fn=<DivBackward0>)\n","5600 tensor(0.0002, device='cuda:0', grad_fn=<DivBackward0>)\n","5800 tensor(0.0109, device='cuda:0', grad_fn=<DivBackward0>)\n","6000 tensor(0.0036, device='cuda:0', grad_fn=<DivBackward0>)\n","6200 tensor(0.0079, device='cuda:0', grad_fn=<DivBackward0>)\n","6400 tensor(5.4467e-05, device='cuda:0', grad_fn=<DivBackward0>)\n","6600 tensor(0.0024, device='cuda:0', grad_fn=<DivBackward0>)\n","6800 tensor(0.0002, device='cuda:0', grad_fn=<DivBackward0>)\n","7000 tensor(0.0051, device='cuda:0', grad_fn=<DivBackward0>)\n","7200 tensor(0.0107, device='cuda:0', grad_fn=<DivBackward0>)\n","7400 tensor(7.8766e-05, device='cuda:0', grad_fn=<DivBackward0>)\n","7600 tensor(0.0008, device='cuda:0', grad_fn=<DivBackward0>)\n","7800 tensor(0.0011, device='cuda:0', grad_fn=<DivBackward0>)\n","8000 tensor(0.0202, device='cuda:0', grad_fn=<DivBackward0>)\n","8200 tensor(0.0054, device='cuda:0', grad_fn=<DivBackward0>)\n","8400 tensor(0.0002, device='cuda:0', grad_fn=<DivBackward0>)\n","8600 tensor(0.0046, device='cuda:0', grad_fn=<DivBackward0>)\n","8800 tensor(0.0033, device='cuda:0', grad_fn=<DivBackward0>)\n","9000 tensor(0.0031, device='cuda:0', grad_fn=<DivBackward0>)\n","9200 tensor(0.0037, device='cuda:0', grad_fn=<DivBackward0>)\n","9400 tensor(0.0027, device='cuda:0', grad_fn=<DivBackward0>)\n","9600 tensor(0.0034, device='cuda:0', grad_fn=<DivBackward0>)\n","9800 tensor(0.0023, device='cuda:0', grad_fn=<DivBackward0>)\n","10000 tensor(0.0026, device='cuda:0', grad_fn=<DivBackward0>)\n","###############    Reward for test environment for run 3: 79.47.   ###############\n","\n","\n","Run 4 out of 15\n","state_dim 8\n","200 tensor(0.4889, device='cuda:0', grad_fn=<AddBackward0>)\n","400 tensor(0.3200, device='cuda:0', grad_fn=<AddBackward0>)\n","600 tensor(0.3426, device='cuda:0', grad_fn=<AddBackward0>)\n","800 tensor(0.3273, device='cuda:0', grad_fn=<AddBackward0>)\n","1000 tensor(0.3008, device='cuda:0', grad_fn=<AddBackward0>)\n","1200 tensor(0.1713, device='cuda:0', grad_fn=<AddBackward0>)\n","1400 tensor(0.3057, device='cuda:0', grad_fn=<AddBackward0>)\n","1600 tensor(0.1708, device='cuda:0', grad_fn=<AddBackward0>)\n","1800 tensor(0.2281, device='cuda:0', grad_fn=<AddBackward0>)\n","2000 tensor(0.2621, device='cuda:0', grad_fn=<AddBackward0>)\n","2200 tensor(0.2829, device='cuda:0', grad_fn=<AddBackward0>)\n","2400 tensor(0.2041, device='cuda:0', grad_fn=<AddBackward0>)\n","2600 tensor(0.2951, device='cuda:0', grad_fn=<AddBackward0>)\n","2800 tensor(0.2295, device='cuda:0', grad_fn=<AddBackward0>)\n","3000 tensor(0.3318, device='cuda:0', grad_fn=<AddBackward0>)\n","3200 tensor(0.2237, device='cuda:0', grad_fn=<AddBackward0>)\n","3400 tensor(0.1578, device='cuda:0', grad_fn=<AddBackward0>)\n","3600 tensor(0.2711, device='cuda:0', grad_fn=<AddBackward0>)\n","3800 tensor(0.2492, device='cuda:0', grad_fn=<AddBackward0>)\n","4000 tensor(0.2980, device='cuda:0', grad_fn=<AddBackward0>)\n","4200 tensor(0.2491, device='cuda:0', grad_fn=<AddBackward0>)\n","4400 tensor(0.2642, device='cuda:0', grad_fn=<AddBackward0>)\n","4600 tensor(0.1987, device='cuda:0', grad_fn=<AddBackward0>)\n","4800 tensor(0.3129, device='cuda:0', grad_fn=<AddBackward0>)\n","5000 tensor(0.0315, device='cuda:0', grad_fn=<DivBackward0>)\n","5200 tensor(0.0054, device='cuda:0', grad_fn=<DivBackward0>)\n","5400 tensor(0.0033, device='cuda:0', grad_fn=<DivBackward0>)\n","5600 tensor(0.0021, device='cuda:0', grad_fn=<DivBackward0>)\n","5800 tensor(0.0011, device='cuda:0', grad_fn=<DivBackward0>)\n","6000 tensor(0.0049, device='cuda:0', grad_fn=<DivBackward0>)\n","6200 tensor(0.0136, device='cuda:0', grad_fn=<DivBackward0>)\n","6400 tensor(0.0031, device='cuda:0', grad_fn=<DivBackward0>)\n","6600 tensor(0.0007, device='cuda:0', grad_fn=<DivBackward0>)\n","6800 tensor(0.0039, device='cuda:0', grad_fn=<DivBackward0>)\n","7000 tensor(0.0064, device='cuda:0', grad_fn=<DivBackward0>)\n","7200 tensor(0.0040, device='cuda:0', grad_fn=<DivBackward0>)\n","7400 tensor(0.0031, device='cuda:0', grad_fn=<DivBackward0>)\n","7600 tensor(0.0027, device='cuda:0', grad_fn=<DivBackward0>)\n","7800 tensor(0.0071, device='cuda:0', grad_fn=<DivBackward0>)\n","8000 tensor(0.0069, device='cuda:0', grad_fn=<DivBackward0>)\n","8200 tensor(0.0049, device='cuda:0', grad_fn=<DivBackward0>)\n","8400 tensor(0.0141, device='cuda:0', grad_fn=<DivBackward0>)\n","8600 tensor(0.0099, device='cuda:0', grad_fn=<DivBackward0>)\n","8800 tensor(0.0018, device='cuda:0', grad_fn=<DivBackward0>)\n","9000 tensor(0.0036, device='cuda:0', grad_fn=<DivBackward0>)\n","9200 tensor(0.0017, device='cuda:0', grad_fn=<DivBackward0>)\n","9400 tensor(0.0008, device='cuda:0', grad_fn=<DivBackward0>)\n","9600 tensor(0.0019, device='cuda:0', grad_fn=<DivBackward0>)\n","9800 tensor(0.0014, device='cuda:0', grad_fn=<DivBackward0>)\n","10000 tensor(0.0031, device='cuda:0', grad_fn=<DivBackward0>)\n","###############    Reward for test environment for run 4: 216.69.   ###############\n","\n","\n","Run 5 out of 15\n","state_dim 8\n","200 tensor(0.5322, device='cuda:0', grad_fn=<AddBackward0>)\n","400 tensor(0.2964, device='cuda:0', grad_fn=<AddBackward0>)\n","600 tensor(0.3303, device='cuda:0', grad_fn=<AddBackward0>)\n","800 tensor(0.3830, device='cuda:0', grad_fn=<AddBackward0>)\n","1000 tensor(0.3319, device='cuda:0', grad_fn=<AddBackward0>)\n","1200 tensor(0.1993, device='cuda:0', grad_fn=<AddBackward0>)\n","1400 tensor(0.2460, device='cuda:0', grad_fn=<AddBackward0>)\n","1600 tensor(0.2648, device='cuda:0', grad_fn=<AddBackward0>)\n","1800 tensor(0.3978, device='cuda:0', grad_fn=<AddBackward0>)\n","2000 tensor(0.1474, device='cuda:0', grad_fn=<AddBackward0>)\n","2200 tensor(0.1332, device='cuda:0', grad_fn=<AddBackward0>)\n","2400 tensor(0.2403, device='cuda:0', grad_fn=<AddBackward0>)\n","2600 tensor(0.1882, device='cuda:0', grad_fn=<AddBackward0>)\n","2800 tensor(0.1782, device='cuda:0', grad_fn=<AddBackward0>)\n","3000 tensor(0.2314, device='cuda:0', grad_fn=<AddBackward0>)\n","3200 tensor(0.2031, device='cuda:0', grad_fn=<AddBackward0>)\n","3400 tensor(0.2345, device='cuda:0', grad_fn=<AddBackward0>)\n","3600 tensor(0.2018, device='cuda:0', grad_fn=<AddBackward0>)\n","3800 tensor(0.1604, device='cuda:0', grad_fn=<AddBackward0>)\n","4000 tensor(0.1831, device='cuda:0', grad_fn=<AddBackward0>)\n","4200 tensor(0.1951, device='cuda:0', grad_fn=<AddBackward0>)\n","4400 tensor(0.2318, device='cuda:0', grad_fn=<AddBackward0>)\n","4600 tensor(0.2827, device='cuda:0', grad_fn=<AddBackward0>)\n","4800 tensor(0.2040, device='cuda:0', grad_fn=<AddBackward0>)\n","5000 tensor(0.0135, device='cuda:0', grad_fn=<DivBackward0>)\n","5200 tensor(0.0012, device='cuda:0', grad_fn=<DivBackward0>)\n","5400 tensor(0.0011, device='cuda:0', grad_fn=<DivBackward0>)\n","5600 tensor(0.0023, device='cuda:0', grad_fn=<DivBackward0>)\n","5800 tensor(0.0008, device='cuda:0', grad_fn=<DivBackward0>)\n","6000 tensor(0.0044, device='cuda:0', grad_fn=<DivBackward0>)\n","6200 tensor(0.0035, device='cuda:0', grad_fn=<DivBackward0>)\n","6400 tensor(0.0078, device='cuda:0', grad_fn=<DivBackward0>)\n","6600 tensor(0.0022, device='cuda:0', grad_fn=<DivBackward0>)\n","6800 tensor(0.0032, device='cuda:0', grad_fn=<DivBackward0>)\n","7000 tensor(0.0091, device='cuda:0', grad_fn=<DivBackward0>)\n","7200 tensor(0.0081, device='cuda:0', grad_fn=<DivBackward0>)\n","7400 tensor(0.0050, device='cuda:0', grad_fn=<DivBackward0>)\n","7600 tensor(0.0048, device='cuda:0', grad_fn=<DivBackward0>)\n","7800 tensor(0.0080, device='cuda:0', grad_fn=<DivBackward0>)\n","8000 tensor(0.0008, device='cuda:0', grad_fn=<DivBackward0>)\n","8200 tensor(0.0098, device='cuda:0', grad_fn=<DivBackward0>)\n","8400 tensor(0.0130, device='cuda:0', grad_fn=<DivBackward0>)\n","8600 tensor(0.0010, device='cuda:0', grad_fn=<DivBackward0>)\n","8800 tensor(0.0035, device='cuda:0', grad_fn=<DivBackward0>)\n","9000 tensor(0.0025, device='cuda:0', grad_fn=<DivBackward0>)\n","9200 tensor(0.0033, device='cuda:0', grad_fn=<DivBackward0>)\n","9400 tensor(0.0064, device='cuda:0', grad_fn=<DivBackward0>)\n","9600 tensor(0.0004, device='cuda:0', grad_fn=<DivBackward0>)\n","9800 tensor(0.0227, device='cuda:0', grad_fn=<DivBackward0>)\n","10000 tensor(0.0209, device='cuda:0', grad_fn=<DivBackward0>)\n","###############    Reward for test environment for run 5: 176.44.   ###############\n","\n","\n","Run 6 out of 15\n","state_dim 8\n","200 tensor(0.3427, device='cuda:0', grad_fn=<AddBackward0>)\n","400 tensor(0.3489, device='cuda:0', grad_fn=<AddBackward0>)\n","600 tensor(0.4198, device='cuda:0', grad_fn=<AddBackward0>)\n","800 tensor(0.4919, device='cuda:0', grad_fn=<AddBackward0>)\n","1000 tensor(0.3164, device='cuda:0', grad_fn=<AddBackward0>)\n","1200 tensor(0.3282, device='cuda:0', grad_fn=<AddBackward0>)\n","1400 tensor(0.2737, device='cuda:0', grad_fn=<AddBackward0>)\n","1600 tensor(0.3365, device='cuda:0', grad_fn=<AddBackward0>)\n","1800 tensor(0.2003, device='cuda:0', grad_fn=<AddBackward0>)\n","2000 tensor(0.2552, device='cuda:0', grad_fn=<AddBackward0>)\n","2200 tensor(0.3714, device='cuda:0', grad_fn=<AddBackward0>)\n","2400 tensor(0.2240, device='cuda:0', grad_fn=<AddBackward0>)\n","2600 tensor(0.2322, device='cuda:0', grad_fn=<AddBackward0>)\n","2800 tensor(0.2466, device='cuda:0', grad_fn=<AddBackward0>)\n","3000 tensor(0.2734, device='cuda:0', grad_fn=<AddBackward0>)\n","3200 tensor(0.1885, device='cuda:0', grad_fn=<AddBackward0>)\n","3400 tensor(0.1848, device='cuda:0', grad_fn=<AddBackward0>)\n","3600 tensor(0.2232, device='cuda:0', grad_fn=<AddBackward0>)\n","3800 tensor(0.2731, device='cuda:0', grad_fn=<AddBackward0>)\n","4000 tensor(0.1657, device='cuda:0', grad_fn=<AddBackward0>)\n","4200 tensor(0.2849, device='cuda:0', grad_fn=<AddBackward0>)\n","4400 tensor(0.1715, device='cuda:0', grad_fn=<AddBackward0>)\n","4600 tensor(0.2974, device='cuda:0', grad_fn=<AddBackward0>)\n","4800 tensor(0.1945, device='cuda:0', grad_fn=<AddBackward0>)\n","5000 tensor(0.0155, device='cuda:0', grad_fn=<DivBackward0>)\n","5200 tensor(0.0035, device='cuda:0', grad_fn=<DivBackward0>)\n","5400 tensor(0.0048, device='cuda:0', grad_fn=<DivBackward0>)\n","5600 tensor(0.0024, device='cuda:0', grad_fn=<DivBackward0>)\n","5800 tensor(0.0010, device='cuda:0', grad_fn=<DivBackward0>)\n","6000 tensor(0.0007, device='cuda:0', grad_fn=<DivBackward0>)\n","6200 tensor(0.0004, device='cuda:0', grad_fn=<DivBackward0>)\n","6400 tensor(0.0089, device='cuda:0', grad_fn=<DivBackward0>)\n","6600 tensor(0.0011, device='cuda:0', grad_fn=<DivBackward0>)\n","6800 tensor(0.0034, device='cuda:0', grad_fn=<DivBackward0>)\n","7000 tensor(0.0022, device='cuda:0', grad_fn=<DivBackward0>)\n","7200 tensor(0.0019, device='cuda:0', grad_fn=<DivBackward0>)\n","7400 tensor(0.0058, device='cuda:0', grad_fn=<DivBackward0>)\n","7600 tensor(0.0073, device='cuda:0', grad_fn=<DivBackward0>)\n","7800 tensor(0.0005, device='cuda:0', grad_fn=<DivBackward0>)\n","8000 tensor(0.0198, device='cuda:0', grad_fn=<DivBackward0>)\n","8200 tensor(0.0027, device='cuda:0', grad_fn=<DivBackward0>)\n","8400 tensor(0.0055, device='cuda:0', grad_fn=<DivBackward0>)\n","8600 tensor(0.0038, device='cuda:0', grad_fn=<DivBackward0>)\n","8800 tensor(0.0014, device='cuda:0', grad_fn=<DivBackward0>)\n","9000 tensor(0.0056, device='cuda:0', grad_fn=<DivBackward0>)\n","9200 tensor(0.0021, device='cuda:0', grad_fn=<DivBackward0>)\n","9400 tensor(0.0030, device='cuda:0', grad_fn=<DivBackward0>)\n","9600 tensor(0.0055, device='cuda:0', grad_fn=<DivBackward0>)\n","9800 tensor(0.0238, device='cuda:0', grad_fn=<DivBackward0>)\n","10000 tensor(0.0003, device='cuda:0', grad_fn=<DivBackward0>)\n","###############    Reward for test environment for run 6: 500.0.   ###############\n","\n","\n","Run 7 out of 15\n","state_dim 8\n","200 tensor(0.3001, device='cuda:0', grad_fn=<AddBackward0>)\n","400 tensor(0.2616, device='cuda:0', grad_fn=<AddBackward0>)\n","600 tensor(0.5230, device='cuda:0', grad_fn=<AddBackward0>)\n","800 tensor(0.4242, device='cuda:0', grad_fn=<AddBackward0>)\n","1000 tensor(0.2917, device='cuda:0', grad_fn=<AddBackward0>)\n","1200 tensor(0.3456, device='cuda:0', grad_fn=<AddBackward0>)\n","1400 tensor(0.2716, device='cuda:0', grad_fn=<AddBackward0>)\n","1600 tensor(0.3983, device='cuda:0', grad_fn=<AddBackward0>)\n","1800 tensor(0.2265, device='cuda:0', grad_fn=<AddBackward0>)\n","2000 tensor(0.2805, device='cuda:0', grad_fn=<AddBackward0>)\n","2200 tensor(0.2461, device='cuda:0', grad_fn=<AddBackward0>)\n","2400 tensor(0.2742, device='cuda:0', grad_fn=<AddBackward0>)\n","2600 tensor(0.2073, device='cuda:0', grad_fn=<AddBackward0>)\n","2800 tensor(0.1973, device='cuda:0', grad_fn=<AddBackward0>)\n","3000 tensor(0.2232, device='cuda:0', grad_fn=<AddBackward0>)\n","3200 tensor(0.1551, device='cuda:0', grad_fn=<AddBackward0>)\n","3400 tensor(0.1543, device='cuda:0', grad_fn=<AddBackward0>)\n","3600 tensor(0.2380, device='cuda:0', grad_fn=<AddBackward0>)\n","3800 tensor(0.2586, device='cuda:0', grad_fn=<AddBackward0>)\n","4000 tensor(0.3094, device='cuda:0', grad_fn=<AddBackward0>)\n","4200 tensor(0.2227, device='cuda:0', grad_fn=<AddBackward0>)\n","4400 tensor(0.2802, device='cuda:0', grad_fn=<AddBackward0>)\n","4600 tensor(0.2969, device='cuda:0', grad_fn=<AddBackward0>)\n","4800 tensor(0.3552, device='cuda:0', grad_fn=<AddBackward0>)\n","5000 tensor(0.0025, device='cuda:0', grad_fn=<DivBackward0>)\n","5200 tensor(0.0029, device='cuda:0', grad_fn=<DivBackward0>)\n","5400 tensor(0.0017, device='cuda:0', grad_fn=<DivBackward0>)\n","5600 tensor(0.0005, device='cuda:0', grad_fn=<DivBackward0>)\n","5800 tensor(0.0008, device='cuda:0', grad_fn=<DivBackward0>)\n","6000 tensor(0.0058, device='cuda:0', grad_fn=<DivBackward0>)\n","6200 tensor(0.0033, device='cuda:0', grad_fn=<DivBackward0>)\n","6400 tensor(0.0005, device='cuda:0', grad_fn=<DivBackward0>)\n","6600 tensor(0.0033, device='cuda:0', grad_fn=<DivBackward0>)\n","6800 tensor(0.0116, device='cuda:0', grad_fn=<DivBackward0>)\n","7000 tensor(0.0015, device='cuda:0', grad_fn=<DivBackward0>)\n","7200 tensor(0.0022, device='cuda:0', grad_fn=<DivBackward0>)\n","7400 tensor(0.0006, device='cuda:0', grad_fn=<DivBackward0>)\n","7600 tensor(0.0096, device='cuda:0', grad_fn=<DivBackward0>)\n","7800 tensor(0.0004, device='cuda:0', grad_fn=<DivBackward0>)\n","8000 tensor(0.0314, device='cuda:0', grad_fn=<DivBackward0>)\n","8200 tensor(0.0015, device='cuda:0', grad_fn=<DivBackward0>)\n","8400 tensor(0.0187, device='cuda:0', grad_fn=<DivBackward0>)\n","8600 tensor(0.0013, device='cuda:0', grad_fn=<DivBackward0>)\n","8800 tensor(0.0025, device='cuda:0', grad_fn=<DivBackward0>)\n","9000 tensor(0.0010, device='cuda:0', grad_fn=<DivBackward0>)\n","9200 tensor(0.0075, device='cuda:0', grad_fn=<DivBackward0>)\n","9400 tensor(0.0018, device='cuda:0', grad_fn=<DivBackward0>)\n","9600 tensor(0.0029, device='cuda:0', grad_fn=<DivBackward0>)\n","9800 tensor(0.0011, device='cuda:0', grad_fn=<DivBackward0>)\n","10000 tensor(0.0064, device='cuda:0', grad_fn=<DivBackward0>)\n","###############    Reward for test environment for run 7: 241.74.   ###############\n","\n","\n","Run 8 out of 15\n","state_dim 8\n","200 tensor(0.3960, device='cuda:0', grad_fn=<AddBackward0>)\n","400 tensor(0.2668, device='cuda:0', grad_fn=<AddBackward0>)\n","600 tensor(0.3570, device='cuda:0', grad_fn=<AddBackward0>)\n","800 tensor(0.2916, device='cuda:0', grad_fn=<AddBackward0>)\n","1000 tensor(0.2871, device='cuda:0', grad_fn=<AddBackward0>)\n","1200 tensor(0.2609, device='cuda:0', grad_fn=<AddBackward0>)\n","1400 tensor(0.4033, device='cuda:0', grad_fn=<AddBackward0>)\n","1600 tensor(0.3372, device='cuda:0', grad_fn=<AddBackward0>)\n","1800 tensor(0.3124, device='cuda:0', grad_fn=<AddBackward0>)\n","2000 tensor(0.2546, device='cuda:0', grad_fn=<AddBackward0>)\n","2200 tensor(0.4106, device='cuda:0', grad_fn=<AddBackward0>)\n","2400 tensor(0.4051, device='cuda:0', grad_fn=<AddBackward0>)\n","2600 tensor(0.2334, device='cuda:0', grad_fn=<AddBackward0>)\n","2800 tensor(0.2455, device='cuda:0', grad_fn=<AddBackward0>)\n","3000 tensor(0.1676, device='cuda:0', grad_fn=<AddBackward0>)\n","3200 tensor(0.2671, device='cuda:0', grad_fn=<AddBackward0>)\n","3400 tensor(0.2450, device='cuda:0', grad_fn=<AddBackward0>)\n","3600 tensor(0.3049, device='cuda:0', grad_fn=<AddBackward0>)\n","3800 tensor(0.2177, device='cuda:0', grad_fn=<AddBackward0>)\n","4000 tensor(0.2077, device='cuda:0', grad_fn=<AddBackward0>)\n","4200 tensor(0.2247, device='cuda:0', grad_fn=<AddBackward0>)\n","4400 tensor(0.2855, device='cuda:0', grad_fn=<AddBackward0>)\n","4600 tensor(0.1811, device='cuda:0', grad_fn=<AddBackward0>)\n","4800 tensor(0.2814, device='cuda:0', grad_fn=<AddBackward0>)\n","5000 tensor(0.0021, device='cuda:0', grad_fn=<DivBackward0>)\n","5200 tensor(0.0018, device='cuda:0', grad_fn=<DivBackward0>)\n","5400 tensor(0.0022, device='cuda:0', grad_fn=<DivBackward0>)\n","5600 tensor(0.0024, device='cuda:0', grad_fn=<DivBackward0>)\n","5800 tensor(0.0101, device='cuda:0', grad_fn=<DivBackward0>)\n","6000 tensor(0.0030, device='cuda:0', grad_fn=<DivBackward0>)\n","6200 tensor(0.0069, device='cuda:0', grad_fn=<DivBackward0>)\n","6400 tensor(0.0123, device='cuda:0', grad_fn=<DivBackward0>)\n","6600 tensor(0.0023, device='cuda:0', grad_fn=<DivBackward0>)\n","6800 tensor(0.0063, device='cuda:0', grad_fn=<DivBackward0>)\n","7000 tensor(0.0042, device='cuda:0', grad_fn=<DivBackward0>)\n","7200 tensor(0.0007, device='cuda:0', grad_fn=<DivBackward0>)\n","7400 tensor(0.0037, device='cuda:0', grad_fn=<DivBackward0>)\n","7600 tensor(0.0038, device='cuda:0', grad_fn=<DivBackward0>)\n","7800 tensor(0.0105, device='cuda:0', grad_fn=<DivBackward0>)\n","8000 tensor(0.0257, device='cuda:0', grad_fn=<DivBackward0>)\n","8200 tensor(6.8310e-05, device='cuda:0', grad_fn=<DivBackward0>)\n","8400 tensor(0.0074, device='cuda:0', grad_fn=<DivBackward0>)\n","8600 tensor(0.0073, device='cuda:0', grad_fn=<DivBackward0>)\n","8800 tensor(0.0013, device='cuda:0', grad_fn=<DivBackward0>)\n","9000 tensor(0.0030, device='cuda:0', grad_fn=<DivBackward0>)\n","9200 tensor(0.0123, device='cuda:0', grad_fn=<DivBackward0>)\n","9400 tensor(9.3515e-05, device='cuda:0', grad_fn=<DivBackward0>)\n","9600 tensor(7.5762e-05, device='cuda:0', grad_fn=<DivBackward0>)\n","9800 tensor(7.3896e-05, device='cuda:0', grad_fn=<DivBackward0>)\n","10000 tensor(7.3987e-05, device='cuda:0', grad_fn=<DivBackward0>)\n","###############    Reward for test environment for run 8: 9.45.   ###############\n","\n","\n","Run 9 out of 15\n"]},{"output_type":"stream","name":"stderr","text":["/content/drive/.shortcut-targets-by-id/15_BhZyJSvQWMTBbzA9iHkhBymeY5HxXY/Invariant-Causal-Imitation-Learning-main/testing/train_utils.py:106: UserWarning: Buffer smaller than batch size\n","  warnings.warn(\"Buffer smaller than batch size\")\n"]},{"output_type":"error","ename":"IndexError","evalue":"ignored","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)","\u001b[0;32m<ipython-input-6-2bb889716193>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     62\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mrun_seed\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mconfig\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"NUM_REPETITIONS\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     63\u001b[0m             \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Run %s out of %s\"\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mrun_seed\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mconfig\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"NUM_REPETITIONS\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 64\u001b[0;31m             \u001b[0mstudent\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmake_student\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrun_seed\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mconfig\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     65\u001b[0m             \u001b[0mstudent\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnum_updates\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mconfig\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"NUM_STEPS_TRAIN\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     66\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m<ipython-input-5-10eb4085855c>\u001b[0m in \u001b[0;36mmake_student\u001b[0;34m(run_seed, config)\u001b[0m\n\u001b[1;32m     22\u001b[0m         \u001b[0mtraj_shift\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mconfig\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"TRAJ_SHIFT\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     23\u001b[0m         \u001b[0mbuffer_size_in_trajs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mconfig\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"NUM_TRAJS_GIVEN\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 24\u001b[0;31m         \u001b[0msampling_rate\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mconfig\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"SAMPLING_RATE\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     25\u001b[0m     )\n\u001b[1;32m     26\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/content/drive/.shortcut-targets-by-id/15_BhZyJSvQWMTBbzA9iHkhBymeY5HxXY/Invariant-Causal-Imitation-Learning-main/testing/train_utils.py\u001b[0m in \u001b[0;36mfill_buffer\u001b[0;34m(trajs_path, batch_size, run_seed, traj_shift, buffer_size_in_trajs, sampling_rate, strictly_batch_data)\u001b[0m\n\u001b[1;32m    108\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    109\u001b[0m         buffer = ReplayBuffer(\n\u001b[0;32m--> 110\u001b[0;31m             \u001b[0mstate_dim\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mall_pairs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    111\u001b[0m             \u001b[0mtotal_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mall_pairs\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    112\u001b[0m             \u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mIndexError\u001b[0m: list index out of range"]}]},{"cell_type":"markdown","source":["#10 Trails -- BC -- cartpole"],"metadata":{"id":"Lkejd0k3UWIi"}},{"cell_type":"code","source":["config['METHOD'] = \"BC\"\n","\n","for traj_num in [1, 2, 5, 10, 20, 30, 40, 50]:\n","    config[\"NUM_TRAJS_GIVEN\"] = traj_num\n","    config[\"TRAJ_SHIFT\"] = traj_num\n","\n","\n","    config['ALG'] = \"FINAL_Apr24_BCStudent_origindata_trajnum\" + str(traj_num)\n","\n","\n","    ###############.  settings   ###############\n","    #config['ALG'] = \"BCStudent_Apr19_replicatedata\"\n","    #config['METHOD'] = \"BC\"\n","    #config['ENV'] == \"CartPole-v1\"\n","    #config['ENV'] == \"LunarLander-v2\"\n","    #config[\"NUM_TRAJS_GIVEN\"] = 20\n","    #config[\"TRAJ_SHIFT\"] = 20\n","    ###############.  settings   ###############\n","\n","\n","\n","    if config['METHOD'] == 'BCIRM':\n","        config['l2_regularizer_weight'] = 0.001\n","        config['penalty_weight'] = 10000\n","        config['penalty_anneal_iters'] = 100\n","\n","    all_results_trail = []\n","\n","    for trail in range(1): \n","        config['TRIAL'] = trail \n","\n","\n","        ###############.  start a trail   ###############\n","\n","        config[\"EXPERT_ALG\"] = yaml.load(open(\"testing/config.yml\"), Loader=yaml.FullLoader)[config[\"ENV\"]]\n","        print(\"Config: %s\" % config)\n","\n","        TRIAL = config[\"TRIAL\"] #args.trial\n","        print(\"Trial number %s\" % TRIAL)\n","\n","        results_dir_base = \"testing/results/\"\n","        results_dir = os.path.join(results_dir_base, config[\"ENV\"], str(config[\"NUM_TRAJS_GIVEN\"]), config[\"ALG\"])\n","\n","        if not os.path.exists(results_dir):\n","            os.makedirs(results_dir)\n","\n","        config_file = \"trial_\" + str(TRIAL) + \"_\" + \"config.pkl\"\n","\n","        results_file_name = \"trial_\" + str(TRIAL) + \"_\" + \"results.csv\"\n","        results_file_path = os.path.join(results_dir, results_file_name)\n","\n","        if os.path.exists(os.path.join(results_dir, config_file)):\n","            raise NameError(\"CONFIG file already exists %s. Choose a different trial number.\" % config_file)\n","        pickle.dump(config, open(os.path.join(results_dir, config_file), \"wb\"))\n","\n","\n","\n","\n","        ###############.  10 runs for each trail   ###############\n","\n","        print(\"config method = \", config['METHOD'])\n","        print(\"config env = \", config['ENV'])\n","\n","        for run_seed in range(config[\"NUM_REPETITIONS\"]):\n","            print(\"Run %s out of %s\" % (run_seed + 1, config[\"NUM_REPETITIONS\"]))\n","            student = make_student(run_seed, config)\n","            student.train(num_updates=config[\"NUM_STEPS_TRAIN\"])\n","\n","            env_wrapper_out_of_sample = EnvWrapper(\n","                env=gym.make(config[\"ENV\"]), mult_factor=get_test_mult_factors(config['NOISE_DIM'] - 1), idx=3, seed=1\n","            )\n","            action_match, return_mean, return_std = student.test(\n","                num_episodes=config[\"NUM_TRAJS_VALID\"], env_wrapper=env_wrapper_out_of_sample\n","            )\n","\n","            result = (action_match, return_mean, return_std)\n","            print(\"###############    Reward for test environment for run %s: %s.   ###############\\n\\n\" % (run_seed + 1, return_mean))\n","            save_results(results_file_path, run_seed, action_match, return_mean, return_std)\n","\n","        results_trial = pd.read_csv(\n","            \"testing/results/\"\n","            + config[\"ENV\"]\n","            + \"/\"\n","            + str(config[\"NUM_TRAJS_GIVEN\"])\n","            + \"/\"\n","            + config[\"ALG\"]\n","            + \"/trial_\"\n","            + str(TRIAL)\n","            + \"_results.csv\",\n","            header=None,\n","        )\n","\n","        print(\"Average reward for 10 repetitions: %s\" % np.mean(results_trial[2].values))\n","\n","        all_results_trail.append(np.mean(results_trial[2].values))\n","\n","    print(\"ALL RESULTS TRAIL:\" , all_results_trail)"],"metadata":{"id":"iS_lRP8MUWIi","executionInfo":{"status":"ok","timestamp":1650792697639,"user_tz":240,"elapsed":5557984,"user":{"displayName":"3 Lin","userId":"16848054003967173878"}},"colab":{"base_uri":"https://localhost:8080/"},"outputId":"c5d4fda6-63a8-45c5-dda7-1256f5006cd6"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Config: {'ENV': 'CartPole-v1', 'ALG': 'FINAL_Apr24_BCStudent_origindata_trajnum1', 'NUM_TRAJS_GIVEN': 1, 'NUM_TRAINING_ENVS': 2, 'NOISE_DIM': 4, 'REP_SIZE': 16, 'TRAJ_SHIFT': 1, 'SAMPLING_RATE': 5, 'NUM_STEPS_TRAIN': 10000, 'NUM_TRAJS_VALID': 100, 'NUM_REPETITIONS': 10, 'BATCH_SIZE': 64, 'MLP_WIDTHS': 64, 'ADAM_ALPHA': 0.001, 'SGLD_BUFFER_SIZE': 10000, 'SGLD_LEARN_RATE': 0.01, 'SGLD_NOISE_COEF': 0.01, 'SGLD_NUM_STEPS': 100, 'SGLD_REINIT_FREQ': 0.05, 'NUM_STEPS_TRAIN_ENERGY_MODEL': 1000, 'TRIAL': 0, 'METHOD': 'BC', 'EXPERT_ALG': 'dqn'}\n","Trial number 0\n","config method =  BC\n","config env =  CartPole-v1\n","Run 1 out of 10\n","state_dim 8\n","epoch 9000/10000, policy loss 0.0002972379152197391\tepoch 0/100 return: 48.0\n","epoch 10/100 return: 36.0\n","epoch 20/100 return: 53.0\n","epoch 30/100 return: 47.0\n","epoch 40/100 return: 41.0\n","epoch 50/100 return: 42.0\n","epoch 60/100 return: 41.0\n","epoch 70/100 return: 48.0\n","epoch 80/100 return: 48.0\n","epoch 90/100 return: 46.0\n","###############    Reward for test environment for run 1: 46.24.   ###############\n","\n","\n","Run 2 out of 10\n","state_dim 8\n","epoch 9000/10000, policy loss 0.005808426532894373\tepoch 0/100 return: 13.0\n","epoch 10/100 return: 12.0\n","epoch 20/100 return: 14.0\n","epoch 30/100 return: 13.0\n","epoch 40/100 return: 14.0\n","epoch 50/100 return: 11.0\n","epoch 60/100 return: 12.0\n","epoch 70/100 return: 12.0\n","epoch 80/100 return: 15.0\n","epoch 90/100 return: 15.0\n","###############    Reward for test environment for run 2: 12.74.   ###############\n","\n","\n","Run 3 out of 10\n","state_dim 8\n","epoch 9000/10000, policy loss 0.09978580474853516\tepoch 0/100 return: 19.0\n","epoch 10/100 return: 23.0\n","epoch 20/100 return: 14.0\n","epoch 30/100 return: 21.0\n","epoch 40/100 return: 20.0\n","epoch 50/100 return: 20.0\n","epoch 60/100 return: 22.0\n","epoch 70/100 return: 15.0\n","epoch 80/100 return: 21.0\n","epoch 90/100 return: 14.0\n","###############    Reward for test environment for run 3: 18.81.   ###############\n","\n","\n","Run 4 out of 10\n","state_dim 8\n","epoch 9000/10000, policy loss 0.00662978645414114\tepoch 0/100 return: 51.0\n","epoch 10/100 return: 49.0\n","epoch 20/100 return: 50.0\n","epoch 30/100 return: 64.0\n","epoch 40/100 return: 63.0\n","epoch 50/100 return: 57.0\n","epoch 60/100 return: 54.0\n","epoch 70/100 return: 51.0\n","epoch 80/100 return: 48.0\n","epoch 90/100 return: 46.0\n","###############    Reward for test environment for run 4: 52.19.   ###############\n","\n","\n","Run 5 out of 10\n","state_dim 8\n","epoch 9000/10000, policy loss 0.009191064164042473\tepoch 0/100 return: 500.0\n","epoch 10/100 return: 500.0\n","epoch 20/100 return: 500.0\n","epoch 30/100 return: 500.0\n","epoch 40/100 return: 500.0\n","epoch 50/100 return: 500.0\n","epoch 60/100 return: 500.0\n","epoch 70/100 return: 500.0\n","epoch 80/100 return: 500.0\n","epoch 90/100 return: 500.0\n","###############    Reward for test environment for run 5: 500.0.   ###############\n","\n","\n","Run 6 out of 10\n","state_dim 8\n","epoch 9000/10000, policy loss 0.025501646101474762\tepoch 0/100 return: 61.0\n","epoch 10/100 return: 57.0\n","epoch 20/100 return: 65.0\n","epoch 30/100 return: 61.0\n","epoch 40/100 return: 56.0\n","epoch 50/100 return: 51.0\n","epoch 60/100 return: 57.0\n","epoch 70/100 return: 56.0\n","epoch 80/100 return: 56.0\n","epoch 90/100 return: 67.0\n","###############    Reward for test environment for run 6: 58.45.   ###############\n","\n","\n","Run 7 out of 10\n","state_dim 8\n","epoch 9000/10000, policy loss 0.0010129037545993924\tepoch 0/100 return: 24.0\n","epoch 10/100 return: 17.0\n","epoch 20/100 return: 22.0\n","epoch 30/100 return: 24.0\n","epoch 40/100 return: 24.0\n","epoch 50/100 return: 18.0\n","epoch 60/100 return: 19.0\n","epoch 70/100 return: 20.0\n","epoch 80/100 return: 17.0\n","epoch 90/100 return: 19.0\n","###############    Reward for test environment for run 7: 20.8.   ###############\n","\n","\n","Run 8 out of 10\n","state_dim 8\n","epoch 9000/10000, policy loss 0.004778719507157803\tepoch 0/100 return: 25.0\n","epoch 10/100 return: 33.0\n","epoch 20/100 return: 27.0\n","epoch 30/100 return: 34.0\n","epoch 40/100 return: 34.0\n","epoch 50/100 return: 38.0\n","epoch 60/100 return: 42.0\n","epoch 70/100 return: 35.0\n","epoch 80/100 return: 37.0\n","epoch 90/100 return: 32.0\n","###############    Reward for test environment for run 8: 34.33.   ###############\n","\n","\n","Run 9 out of 10\n","state_dim 8\n","epoch 9000/10000, policy loss 0.00034592984593473375\tepoch 0/100 return: 26.0\n","epoch 10/100 return: 33.0\n","epoch 20/100 return: 30.0\n","epoch 30/100 return: 31.0\n","epoch 40/100 return: 33.0\n","epoch 50/100 return: 25.0\n","epoch 60/100 return: 24.0\n","epoch 70/100 return: 28.0\n","epoch 80/100 return: 26.0\n","epoch 90/100 return: 28.0\n","###############    Reward for test environment for run 9: 28.72.   ###############\n","\n","\n","Run 10 out of 10\n","state_dim 8\n","epoch 9000/10000, policy loss 0.0005835671327076852\tepoch 0/100 return: 36.0\n","epoch 10/100 return: 31.0\n","epoch 20/100 return: 47.0\n","epoch 30/100 return: 39.0\n","epoch 40/100 return: 29.0\n","epoch 50/100 return: 31.0\n","epoch 60/100 return: 47.0\n","epoch 70/100 return: 34.0\n","epoch 80/100 return: 29.0\n","epoch 90/100 return: 47.0\n","###############    Reward for test environment for run 10: 39.35.   ###############\n","\n","\n","Average reward for 10 repetitions: 81.16300000000001\n","ALL RESULTS TRAIL: [81.16300000000001]\n","Config: {'ENV': 'CartPole-v1', 'ALG': 'FINAL_Apr24_BCStudent_origindata_trajnum2', 'NUM_TRAJS_GIVEN': 2, 'NUM_TRAINING_ENVS': 2, 'NOISE_DIM': 4, 'REP_SIZE': 16, 'TRAJ_SHIFT': 2, 'SAMPLING_RATE': 5, 'NUM_STEPS_TRAIN': 10000, 'NUM_TRAJS_VALID': 100, 'NUM_REPETITIONS': 10, 'BATCH_SIZE': 64, 'MLP_WIDTHS': 64, 'ADAM_ALPHA': 0.001, 'SGLD_BUFFER_SIZE': 10000, 'SGLD_LEARN_RATE': 0.01, 'SGLD_NOISE_COEF': 0.01, 'SGLD_NUM_STEPS': 100, 'SGLD_REINIT_FREQ': 0.05, 'NUM_STEPS_TRAIN_ENERGY_MODEL': 1000, 'TRIAL': 0, 'METHOD': 'BC', 'EXPERT_ALG': 'dqn'}\n","Trial number 0\n","config method =  BC\n","config env =  CartPole-v1\n","Run 1 out of 10\n","state_dim 8\n","epoch 9000/10000, policy loss 0.13997791707515717\tepoch 0/100 return: 500.0\n","epoch 10/100 return: 500.0\n","epoch 20/100 return: 500.0\n","epoch 30/100 return: 500.0\n","epoch 40/100 return: 500.0\n","epoch 50/100 return: 500.0\n","epoch 60/100 return: 500.0\n","epoch 70/100 return: 500.0\n","epoch 80/100 return: 500.0\n","epoch 90/100 return: 500.0\n","###############    Reward for test environment for run 1: 475.68.   ###############\n","\n","\n","Run 2 out of 10\n","state_dim 8\n","epoch 9000/10000, policy loss 0.038782648742198944\tepoch 0/100 return: 16.0\n","epoch 10/100 return: 24.0\n","epoch 20/100 return: 9.0\n","epoch 30/100 return: 44.0\n","epoch 40/100 return: 42.0\n","epoch 50/100 return: 40.0\n","epoch 60/100 return: 9.0\n","epoch 70/100 return: 10.0\n","epoch 80/100 return: 11.0\n","epoch 90/100 return: 10.0\n","###############    Reward for test environment for run 2: 18.91.   ###############\n","\n","\n","Run 3 out of 10\n","state_dim 8\n","epoch 9000/10000, policy loss 0.14023374021053314\tepoch 0/100 return: 293.0\n","epoch 10/100 return: 289.0\n","epoch 20/100 return: 298.0\n","epoch 30/100 return: 261.0\n","epoch 40/100 return: 271.0\n","epoch 50/100 return: 280.0\n","epoch 60/100 return: 295.0\n","epoch 70/100 return: 246.0\n","epoch 80/100 return: 301.0\n","epoch 90/100 return: 296.0\n","###############    Reward for test environment for run 3: 283.33.   ###############\n","\n","\n","Run 4 out of 10\n","state_dim 8\n","epoch 9000/10000, policy loss 0.0413387268781662\tepoch 0/100 return: 74.0\n","epoch 10/100 return: 69.0\n","epoch 20/100 return: 63.0\n","epoch 30/100 return: 70.0\n","epoch 40/100 return: 74.0\n","epoch 50/100 return: 73.0\n","epoch 60/100 return: 65.0\n","epoch 70/100 return: 71.0\n","epoch 80/100 return: 71.0\n","epoch 90/100 return: 59.0\n","###############    Reward for test environment for run 4: 69.87.   ###############\n","\n","\n","Run 5 out of 10\n","state_dim 8\n","epoch 9000/10000, policy loss 0.11522527039051056\tepoch 0/100 return: 32.0\n","epoch 10/100 return: 36.0\n","epoch 20/100 return: 32.0\n","epoch 30/100 return: 40.0\n","epoch 40/100 return: 31.0\n","epoch 50/100 return: 39.0\n","epoch 60/100 return: 30.0\n","epoch 70/100 return: 44.0\n","epoch 80/100 return: 40.0\n","epoch 90/100 return: 44.0\n","###############    Reward for test environment for run 5: 37.23.   ###############\n","\n","\n","Run 6 out of 10\n","state_dim 8\n","epoch 9000/10000, policy loss 0.006324905902147293\tepoch 0/100 return: 500.0\n","epoch 10/100 return: 500.0\n","epoch 20/100 return: 500.0\n","epoch 30/100 return: 500.0\n","epoch 40/100 return: 500.0\n","epoch 50/100 return: 500.0\n","epoch 60/100 return: 500.0\n","epoch 70/100 return: 500.0\n","epoch 80/100 return: 500.0\n","epoch 90/100 return: 500.0\n","###############    Reward for test environment for run 6: 500.0.   ###############\n","\n","\n","Run 7 out of 10\n","state_dim 8\n","epoch 9000/10000, policy loss 0.09867573529481888\tepoch 0/100 return: 86.0\n","epoch 10/100 return: 131.0\n","epoch 20/100 return: 90.0\n","epoch 30/100 return: 116.0\n","epoch 40/100 return: 104.0\n","epoch 50/100 return: 231.0\n","epoch 60/100 return: 101.0\n","epoch 70/100 return: 110.0\n","epoch 80/100 return: 268.0\n","epoch 90/100 return: 92.0\n","###############    Reward for test environment for run 7: 165.37.   ###############\n","\n","\n","Run 8 out of 10\n","state_dim 8\n","epoch 9000/10000, policy loss 0.07366853952407837\tepoch 0/100 return: 15.0\n","epoch 10/100 return: 18.0\n","epoch 20/100 return: 16.0\n","epoch 30/100 return: 16.0\n","epoch 40/100 return: 17.0\n","epoch 50/100 return: 16.0\n","epoch 60/100 return: 18.0\n","epoch 70/100 return: 15.0\n","epoch 80/100 return: 17.0\n","epoch 90/100 return: 17.0\n","###############    Reward for test environment for run 8: 15.66.   ###############\n","\n","\n","Run 9 out of 10\n","state_dim 8\n","epoch 9000/10000, policy loss 0.027654755860567093\tepoch 0/100 return: 161.0\n","epoch 10/100 return: 178.0\n","epoch 20/100 return: 380.0\n","epoch 30/100 return: 166.0\n","epoch 40/100 return: 121.0\n","epoch 50/100 return: 158.0\n","epoch 60/100 return: 366.0\n","epoch 70/100 return: 500.0\n","epoch 80/100 return: 160.0\n","epoch 90/100 return: 179.0\n","###############    Reward for test environment for run 9: 239.46.   ###############\n","\n","\n","Run 10 out of 10\n","state_dim 8\n","epoch 9000/10000, policy loss 0.07342428714036942\tepoch 0/100 return: 143.0\n","epoch 10/100 return: 127.0\n","epoch 20/100 return: 169.0\n","epoch 30/100 return: 116.0\n","epoch 40/100 return: 167.0\n","epoch 50/100 return: 111.0\n","epoch 60/100 return: 256.0\n","epoch 70/100 return: 110.0\n","epoch 80/100 return: 111.0\n","epoch 90/100 return: 143.0\n","###############    Reward for test environment for run 10: 162.48.   ###############\n","\n","\n","Average reward for 10 repetitions: 196.799\n","ALL RESULTS TRAIL: [196.799]\n","Config: {'ENV': 'CartPole-v1', 'ALG': 'FINAL_Apr24_BCStudent_origindata_trajnum5', 'NUM_TRAJS_GIVEN': 5, 'NUM_TRAINING_ENVS': 2, 'NOISE_DIM': 4, 'REP_SIZE': 16, 'TRAJ_SHIFT': 5, 'SAMPLING_RATE': 5, 'NUM_STEPS_TRAIN': 10000, 'NUM_TRAJS_VALID': 100, 'NUM_REPETITIONS': 10, 'BATCH_SIZE': 64, 'MLP_WIDTHS': 64, 'ADAM_ALPHA': 0.001, 'SGLD_BUFFER_SIZE': 10000, 'SGLD_LEARN_RATE': 0.01, 'SGLD_NOISE_COEF': 0.01, 'SGLD_NUM_STEPS': 100, 'SGLD_REINIT_FREQ': 0.05, 'NUM_STEPS_TRAIN_ENERGY_MODEL': 1000, 'TRIAL': 0, 'METHOD': 'BC', 'EXPERT_ALG': 'dqn'}\n","Trial number 0\n","config method =  BC\n","config env =  CartPole-v1\n","Run 1 out of 10\n","state_dim 8\n","epoch 9000/10000, policy loss 0.2877825200557709\tepoch 0/100 return: 500.0\n","epoch 10/100 return: 500.0\n","epoch 20/100 return: 500.0\n","epoch 30/100 return: 500.0\n","epoch 40/100 return: 500.0\n","epoch 50/100 return: 500.0\n","epoch 60/100 return: 500.0\n","epoch 70/100 return: 500.0\n","epoch 80/100 return: 500.0\n","epoch 90/100 return: 500.0\n","###############    Reward for test environment for run 1: 500.0.   ###############\n","\n","\n","Run 2 out of 10\n","state_dim 8\n","epoch 9000/10000, policy loss 0.3643417954444885\tepoch 0/100 return: 500.0\n","epoch 10/100 return: 500.0\n","epoch 20/100 return: 500.0\n","epoch 30/100 return: 500.0\n","epoch 40/100 return: 500.0\n","epoch 50/100 return: 262.0\n","epoch 60/100 return: 500.0\n","epoch 70/100 return: 500.0\n","epoch 80/100 return: 500.0\n","epoch 90/100 return: 500.0\n","###############    Reward for test environment for run 2: 472.16.   ###############\n","\n","\n","Run 3 out of 10\n","state_dim 8\n","epoch 9000/10000, policy loss 0.25508299469947815\tepoch 0/100 return: 500.0\n","epoch 10/100 return: 421.0\n","epoch 20/100 return: 500.0\n","epoch 30/100 return: 500.0\n","epoch 40/100 return: 500.0\n","epoch 50/100 return: 500.0\n","epoch 60/100 return: 500.0\n","epoch 70/100 return: 447.0\n","epoch 80/100 return: 500.0\n","epoch 90/100 return: 500.0\n","###############    Reward for test environment for run 3: 491.59.   ###############\n","\n","\n","Run 4 out of 10\n","state_dim 8\n","epoch 9000/10000, policy loss 0.3777087330818176\tepoch 0/100 return: 500.0\n","epoch 10/100 return: 500.0\n","epoch 20/100 return: 500.0\n","epoch 30/100 return: 500.0\n","epoch 40/100 return: 500.0\n","epoch 50/100 return: 500.0\n","epoch 60/100 return: 500.0\n","epoch 70/100 return: 500.0\n","epoch 80/100 return: 500.0\n","epoch 90/100 return: 500.0\n","###############    Reward for test environment for run 4: 500.0.   ###############\n","\n","\n","Run 5 out of 10\n","state_dim 8\n","epoch 9000/10000, policy loss 0.366257905960083\tepoch 0/100 return: 203.0\n","epoch 10/100 return: 223.0\n","epoch 20/100 return: 398.0\n","epoch 30/100 return: 207.0\n","epoch 40/100 return: 259.0\n","epoch 50/100 return: 213.0\n","epoch 60/100 return: 231.0\n","epoch 70/100 return: 241.0\n","epoch 80/100 return: 229.0\n","epoch 90/100 return: 257.0\n","###############    Reward for test environment for run 5: 240.14.   ###############\n","\n","\n","Run 6 out of 10\n","state_dim 8\n","epoch 9000/10000, policy loss 0.41964825987815857\tepoch 0/100 return: 142.0\n","epoch 10/100 return: 199.0\n","epoch 20/100 return: 170.0\n","epoch 30/100 return: 200.0\n","epoch 40/100 return: 146.0\n","epoch 50/100 return: 144.0\n","epoch 60/100 return: 156.0\n","epoch 70/100 return: 160.0\n","epoch 80/100 return: 154.0\n","epoch 90/100 return: 164.0\n","###############    Reward for test environment for run 6: 168.3.   ###############\n","\n","\n","Run 7 out of 10\n","state_dim 8\n","epoch 9000/10000, policy loss 0.25431305170059204\tepoch 0/100 return: 500.0\n","epoch 10/100 return: 500.0\n","epoch 20/100 return: 500.0\n","epoch 30/100 return: 500.0\n","epoch 40/100 return: 500.0\n","epoch 50/100 return: 500.0\n","epoch 60/100 return: 500.0\n","epoch 70/100 return: 500.0\n","epoch 80/100 return: 500.0\n","epoch 90/100 return: 500.0\n","###############    Reward for test environment for run 7: 500.0.   ###############\n","\n","\n","Run 8 out of 10\n","state_dim 8\n","epoch 9000/10000, policy loss 0.3704964518547058\tepoch 0/100 return: 57.0\n","epoch 10/100 return: 67.0\n","epoch 20/100 return: 65.0\n","epoch 30/100 return: 64.0\n","epoch 40/100 return: 78.0\n","epoch 50/100 return: 71.0\n","epoch 60/100 return: 55.0\n","epoch 70/100 return: 48.0\n","epoch 80/100 return: 63.0\n","epoch 90/100 return: 63.0\n","###############    Reward for test environment for run 8: 69.45.   ###############\n","\n","\n","Run 9 out of 10\n","state_dim 8\n","epoch 9000/10000, policy loss 0.24913376569747925\tepoch 0/100 return: 85.0\n","epoch 10/100 return: 80.0\n","epoch 20/100 return: 91.0\n","epoch 30/100 return: 86.0\n","epoch 40/100 return: 85.0\n","epoch 50/100 return: 500.0\n","epoch 60/100 return: 84.0\n","epoch 70/100 return: 82.0\n","epoch 80/100 return: 86.0\n","epoch 90/100 return: 83.0\n","###############    Reward for test environment for run 9: 167.4.   ###############\n","\n","\n","Run 10 out of 10\n","state_dim 8\n","epoch 9000/10000, policy loss 0.32800835371017456\tepoch 0/100 return: 10.0\n","epoch 10/100 return: 9.0\n","epoch 20/100 return: 10.0\n","epoch 30/100 return: 11.0\n","epoch 40/100 return: 9.0\n","epoch 50/100 return: 9.0\n","epoch 60/100 return: 9.0\n","epoch 70/100 return: 9.0\n","epoch 80/100 return: 10.0\n","epoch 90/100 return: 9.0\n","###############    Reward for test environment for run 10: 9.35.   ###############\n","\n","\n","Average reward for 10 repetitions: 311.83900000000006\n","ALL RESULTS TRAIL: [311.83900000000006]\n","Config: {'ENV': 'CartPole-v1', 'ALG': 'FINAL_Apr24_BCStudent_origindata_trajnum10', 'NUM_TRAJS_GIVEN': 10, 'NUM_TRAINING_ENVS': 2, 'NOISE_DIM': 4, 'REP_SIZE': 16, 'TRAJ_SHIFT': 10, 'SAMPLING_RATE': 5, 'NUM_STEPS_TRAIN': 10000, 'NUM_TRAJS_VALID': 100, 'NUM_REPETITIONS': 10, 'BATCH_SIZE': 64, 'MLP_WIDTHS': 64, 'ADAM_ALPHA': 0.001, 'SGLD_BUFFER_SIZE': 10000, 'SGLD_LEARN_RATE': 0.01, 'SGLD_NOISE_COEF': 0.01, 'SGLD_NUM_STEPS': 100, 'SGLD_REINIT_FREQ': 0.05, 'NUM_STEPS_TRAIN_ENERGY_MODEL': 1000, 'TRIAL': 0, 'METHOD': 'BC', 'EXPERT_ALG': 'dqn'}\n","Trial number 0\n","config method =  BC\n","config env =  CartPole-v1\n","Run 1 out of 10\n","state_dim 8\n","epoch 9000/10000, policy loss 0.4096306562423706\tepoch 0/100 return: 389.0\n","epoch 10/100 return: 178.0\n","epoch 20/100 return: 395.0\n","epoch 30/100 return: 383.0\n","epoch 40/100 return: 396.0\n","epoch 50/100 return: 404.0\n","epoch 60/100 return: 385.0\n","epoch 70/100 return: 401.0\n","epoch 80/100 return: 389.0\n","epoch 90/100 return: 179.0\n","###############    Reward for test environment for run 1: 344.65.   ###############\n","\n","\n","Run 2 out of 10\n","state_dim 8\n","epoch 9000/10000, policy loss 0.4887372851371765\tepoch 0/100 return: 355.0\n","epoch 10/100 return: 419.0\n","epoch 20/100 return: 279.0\n","epoch 30/100 return: 415.0\n","epoch 40/100 return: 290.0\n","epoch 50/100 return: 305.0\n","epoch 60/100 return: 403.0\n","epoch 70/100 return: 319.0\n","epoch 80/100 return: 410.0\n","epoch 90/100 return: 315.0\n","###############    Reward for test environment for run 2: 330.72.   ###############\n","\n","\n","Run 3 out of 10\n","state_dim 8\n","epoch 9000/10000, policy loss 0.3388091027736664\tepoch 0/100 return: 143.0\n","epoch 10/100 return: 435.0\n","epoch 20/100 return: 153.0\n","epoch 30/100 return: 150.0\n","epoch 40/100 return: 158.0\n","epoch 50/100 return: 141.0\n","epoch 60/100 return: 133.0\n","epoch 70/100 return: 141.0\n","epoch 80/100 return: 143.0\n","epoch 90/100 return: 154.0\n","###############    Reward for test environment for run 3: 188.75.   ###############\n","\n","\n","Run 4 out of 10\n","state_dim 8\n","epoch 9000/10000, policy loss 0.3938397467136383\tepoch 0/100 return: 500.0\n","epoch 10/100 return: 500.0\n","epoch 20/100 return: 500.0\n","epoch 30/100 return: 500.0\n","epoch 40/100 return: 500.0\n","epoch 50/100 return: 500.0\n","epoch 60/100 return: 500.0\n","epoch 70/100 return: 500.0\n","epoch 80/100 return: 500.0\n","epoch 90/100 return: 500.0\n","###############    Reward for test environment for run 4: 500.0.   ###############\n","\n","\n","Run 5 out of 10\n","state_dim 8\n","epoch 9000/10000, policy loss 0.34710419178009033\tepoch 0/100 return: 125.0\n","epoch 10/100 return: 131.0\n","epoch 20/100 return: 121.0\n","epoch 30/100 return: 124.0\n","epoch 40/100 return: 126.0\n","epoch 50/100 return: 130.0\n","epoch 60/100 return: 128.0\n","epoch 70/100 return: 120.0\n","epoch 80/100 return: 124.0\n","epoch 90/100 return: 126.0\n","###############    Reward for test environment for run 5: 129.08.   ###############\n","\n","\n","Run 6 out of 10\n","state_dim 8\n","epoch 9000/10000, policy loss 0.3211829960346222\tepoch 0/100 return: 167.0\n","epoch 10/100 return: 201.0\n","epoch 20/100 return: 196.0\n","epoch 30/100 return: 170.0\n","epoch 40/100 return: 164.0\n","epoch 50/100 return: 187.0\n","epoch 60/100 return: 163.0\n","epoch 70/100 return: 178.0\n","epoch 80/100 return: 483.0\n","epoch 90/100 return: 165.0\n","###############    Reward for test environment for run 6: 228.27.   ###############\n","\n","\n","Run 7 out of 10\n","state_dim 8\n","epoch 9000/10000, policy loss 0.41605737805366516\tepoch 0/100 return: 500.0\n","epoch 10/100 return: 500.0\n","epoch 20/100 return: 500.0\n","epoch 30/100 return: 500.0\n","epoch 40/100 return: 500.0\n","epoch 50/100 return: 500.0\n","epoch 60/100 return: 500.0\n","epoch 70/100 return: 500.0\n","epoch 80/100 return: 500.0\n","epoch 90/100 return: 500.0\n","###############    Reward for test environment for run 7: 500.0.   ###############\n","\n","\n","Run 8 out of 10\n","state_dim 8\n","epoch 9000/10000, policy loss 0.3963673710823059\tepoch 0/100 return: 106.0\n","epoch 10/100 return: 116.0\n","epoch 20/100 return: 146.0\n","epoch 30/100 return: 106.0\n","epoch 40/100 return: 130.0\n","epoch 50/100 return: 132.0\n","epoch 60/100 return: 104.0\n","epoch 70/100 return: 102.0\n","epoch 80/100 return: 128.0\n","epoch 90/100 return: 108.0\n","###############    Reward for test environment for run 8: 116.65.   ###############\n","\n","\n","Run 9 out of 10\n","state_dim 8\n","epoch 9000/10000, policy loss 0.38256222009658813\tepoch 0/100 return: 181.0\n","epoch 10/100 return: 156.0\n","epoch 20/100 return: 166.0\n","epoch 30/100 return: 171.0\n","epoch 40/100 return: 183.0\n","epoch 50/100 return: 154.0\n","epoch 60/100 return: 167.0\n","epoch 70/100 return: 184.0\n","epoch 80/100 return: 170.0\n","epoch 90/100 return: 169.0\n","###############    Reward for test environment for run 9: 170.87.   ###############\n","\n","\n","Run 10 out of 10\n","state_dim 8\n","epoch 9000/10000, policy loss 0.39746591448783875\tepoch 0/100 return: 500.0\n","epoch 10/100 return: 500.0\n","epoch 20/100 return: 500.0\n","epoch 30/100 return: 500.0\n","epoch 40/100 return: 500.0\n","epoch 50/100 return: 500.0\n","epoch 60/100 return: 500.0\n","epoch 70/100 return: 500.0\n","epoch 80/100 return: 500.0\n","epoch 90/100 return: 500.0\n","###############    Reward for test environment for run 10: 496.76.   ###############\n","\n","\n","Average reward for 10 repetitions: 300.575\n","ALL RESULTS TRAIL: [300.575]\n","Config: {'ENV': 'CartPole-v1', 'ALG': 'FINAL_Apr24_BCStudent_origindata_trajnum20', 'NUM_TRAJS_GIVEN': 20, 'NUM_TRAINING_ENVS': 2, 'NOISE_DIM': 4, 'REP_SIZE': 16, 'TRAJ_SHIFT': 20, 'SAMPLING_RATE': 5, 'NUM_STEPS_TRAIN': 10000, 'NUM_TRAJS_VALID': 100, 'NUM_REPETITIONS': 10, 'BATCH_SIZE': 64, 'MLP_WIDTHS': 64, 'ADAM_ALPHA': 0.001, 'SGLD_BUFFER_SIZE': 10000, 'SGLD_LEARN_RATE': 0.01, 'SGLD_NOISE_COEF': 0.01, 'SGLD_NUM_STEPS': 100, 'SGLD_REINIT_FREQ': 0.05, 'NUM_STEPS_TRAIN_ENERGY_MODEL': 1000, 'TRIAL': 0, 'METHOD': 'BC', 'EXPERT_ALG': 'dqn'}\n","Trial number 0\n","config method =  BC\n","config env =  CartPole-v1\n","Run 1 out of 10\n","state_dim 8\n","epoch 9000/10000, policy loss 0.43659505248069763\tepoch 0/100 return: 500.0\n","epoch 10/100 return: 500.0\n","epoch 20/100 return: 500.0\n","epoch 30/100 return: 500.0\n","epoch 40/100 return: 500.0\n","epoch 50/100 return: 500.0\n","epoch 60/100 return: 500.0\n","epoch 70/100 return: 500.0\n","epoch 80/100 return: 500.0\n","epoch 90/100 return: 500.0\n","###############    Reward for test environment for run 1: 500.0.   ###############\n","\n","\n","Run 2 out of 10\n","state_dim 8\n","epoch 9000/10000, policy loss 0.28791096806526184\tepoch 0/100 return: 500.0\n","epoch 10/100 return: 500.0\n","epoch 20/100 return: 500.0\n","epoch 30/100 return: 500.0\n","epoch 40/100 return: 500.0\n","epoch 50/100 return: 500.0\n","epoch 60/100 return: 500.0\n","epoch 70/100 return: 500.0\n","epoch 80/100 return: 500.0\n","epoch 90/100 return: 500.0\n","###############    Reward for test environment for run 2: 500.0.   ###############\n","\n","\n","Run 3 out of 10\n","state_dim 8\n","epoch 9000/10000, policy loss 0.4071904122829437\tepoch 0/100 return: 500.0\n","epoch 10/100 return: 500.0\n","epoch 20/100 return: 500.0\n","epoch 30/100 return: 500.0\n","epoch 40/100 return: 500.0\n","epoch 50/100 return: 500.0\n","epoch 60/100 return: 500.0\n","epoch 70/100 return: 500.0\n","epoch 80/100 return: 500.0\n","epoch 90/100 return: 500.0\n","###############    Reward for test environment for run 3: 500.0.   ###############\n","\n","\n","Run 4 out of 10\n","state_dim 8\n","epoch 9000/10000, policy loss 0.40125763416290283\tepoch 0/100 return: 500.0\n","epoch 10/100 return: 500.0\n","epoch 20/100 return: 500.0\n","epoch 30/100 return: 500.0\n","epoch 40/100 return: 500.0\n","epoch 50/100 return: 500.0\n","epoch 60/100 return: 500.0\n","epoch 70/100 return: 500.0\n","epoch 80/100 return: 500.0\n","epoch 90/100 return: 500.0\n","###############    Reward for test environment for run 4: 500.0.   ###############\n","\n","\n","Run 5 out of 10\n","state_dim 8\n","epoch 9000/10000, policy loss 0.3965742290019989\tepoch 0/100 return: 341.0\n","epoch 10/100 return: 403.0\n","epoch 20/100 return: 334.0\n","epoch 30/100 return: 432.0\n","epoch 40/100 return: 321.0\n","epoch 50/100 return: 500.0\n","epoch 60/100 return: 391.0\n","epoch 70/100 return: 361.0\n","epoch 80/100 return: 322.0\n","epoch 90/100 return: 413.0\n","###############    Reward for test environment for run 5: 363.43.   ###############\n","\n","\n","Run 6 out of 10\n","state_dim 8\n","epoch 9000/10000, policy loss 0.3806159198284149\tepoch 0/100 return: 500.0\n","epoch 10/100 return: 500.0\n","epoch 20/100 return: 500.0\n","epoch 30/100 return: 500.0\n","epoch 40/100 return: 500.0\n","epoch 50/100 return: 500.0\n","epoch 60/100 return: 500.0\n","epoch 70/100 return: 500.0\n","epoch 80/100 return: 500.0\n","epoch 90/100 return: 500.0\n","###############    Reward for test environment for run 6: 500.0.   ###############\n","\n","\n","Run 7 out of 10\n","state_dim 8\n","epoch 9000/10000, policy loss 0.4519323706626892\tepoch 0/100 return: 500.0\n","epoch 10/100 return: 500.0\n","epoch 20/100 return: 500.0\n","epoch 30/100 return: 500.0\n","epoch 40/100 return: 500.0\n","epoch 50/100 return: 500.0\n","epoch 60/100 return: 500.0\n","epoch 70/100 return: 500.0\n","epoch 80/100 return: 500.0\n","epoch 90/100 return: 500.0\n","###############    Reward for test environment for run 7: 500.0.   ###############\n","\n","\n","Run 8 out of 10\n","state_dim 8\n","epoch 9000/10000, policy loss 0.35382118821144104\tepoch 0/100 return: 194.0\n","epoch 10/100 return: 199.0\n","epoch 20/100 return: 185.0\n","epoch 30/100 return: 182.0\n","epoch 40/100 return: 175.0\n","epoch 50/100 return: 220.0\n","epoch 60/100 return: 182.0\n","epoch 70/100 return: 157.0\n","epoch 80/100 return: 161.0\n","epoch 90/100 return: 171.0\n","###############    Reward for test environment for run 8: 177.37.   ###############\n","\n","\n","Run 9 out of 10\n","state_dim 8\n","epoch 9000/10000, policy loss 0.3803905248641968\tepoch 0/100 return: 500.0\n","epoch 10/100 return: 500.0\n","epoch 20/100 return: 500.0\n","epoch 30/100 return: 500.0\n","epoch 40/100 return: 500.0\n","epoch 50/100 return: 500.0\n","epoch 60/100 return: 500.0\n","epoch 70/100 return: 500.0\n","epoch 80/100 return: 500.0\n","epoch 90/100 return: 500.0\n","###############    Reward for test environment for run 9: 500.0.   ###############\n","\n","\n","Run 10 out of 10\n","state_dim 8\n","epoch 9000/10000, policy loss 0.4443674087524414\tepoch 0/100 return: 500.0\n","epoch 10/100 return: 500.0\n","epoch 20/100 return: 500.0\n","epoch 30/100 return: 500.0\n","epoch 40/100 return: 500.0\n","epoch 50/100 return: 500.0\n","epoch 60/100 return: 500.0\n","epoch 70/100 return: 500.0\n","epoch 80/100 return: 500.0\n","epoch 90/100 return: 500.0\n","###############    Reward for test environment for run 10: 500.0.   ###############\n","\n","\n","Average reward for 10 repetitions: 454.08000000000004\n","ALL RESULTS TRAIL: [454.08000000000004]\n","Config: {'ENV': 'CartPole-v1', 'ALG': 'FINAL_Apr24_BCStudent_origindata_trajnum30', 'NUM_TRAJS_GIVEN': 30, 'NUM_TRAINING_ENVS': 2, 'NOISE_DIM': 4, 'REP_SIZE': 16, 'TRAJ_SHIFT': 30, 'SAMPLING_RATE': 5, 'NUM_STEPS_TRAIN': 10000, 'NUM_TRAJS_VALID': 100, 'NUM_REPETITIONS': 10, 'BATCH_SIZE': 64, 'MLP_WIDTHS': 64, 'ADAM_ALPHA': 0.001, 'SGLD_BUFFER_SIZE': 10000, 'SGLD_LEARN_RATE': 0.01, 'SGLD_NOISE_COEF': 0.01, 'SGLD_NUM_STEPS': 100, 'SGLD_REINIT_FREQ': 0.05, 'NUM_STEPS_TRAIN_ENERGY_MODEL': 1000, 'TRIAL': 0, 'METHOD': 'BC', 'EXPERT_ALG': 'dqn'}\n","Trial number 0\n","config method =  BC\n","config env =  CartPole-v1\n","Run 1 out of 10\n","state_dim 8\n","epoch 9000/10000, policy loss 0.38250187039375305\tepoch 0/100 return: 500.0\n","epoch 10/100 return: 500.0\n","epoch 20/100 return: 500.0\n","epoch 30/100 return: 500.0\n","epoch 40/100 return: 500.0\n","epoch 50/100 return: 500.0\n","epoch 60/100 return: 500.0\n","epoch 70/100 return: 500.0\n","epoch 80/100 return: 500.0\n","epoch 90/100 return: 500.0\n","###############    Reward for test environment for run 1: 500.0.   ###############\n","\n","\n","Run 2 out of 10\n","state_dim 8\n","epoch 9000/10000, policy loss 0.3004324734210968\tepoch 0/100 return: 500.0\n","epoch 10/100 return: 500.0\n","epoch 20/100 return: 500.0\n","epoch 30/100 return: 500.0\n","epoch 40/100 return: 500.0\n","epoch 50/100 return: 500.0\n","epoch 60/100 return: 500.0\n","epoch 70/100 return: 500.0\n","epoch 80/100 return: 500.0\n","epoch 90/100 return: 500.0\n","###############    Reward for test environment for run 2: 500.0.   ###############\n","\n","\n","Run 3 out of 10\n","state_dim 8\n","epoch 9000/10000, policy loss 0.3147582709789276\tepoch 0/100 return: 500.0\n","epoch 10/100 return: 500.0\n","epoch 20/100 return: 500.0\n","epoch 30/100 return: 500.0\n","epoch 40/100 return: 500.0\n","epoch 50/100 return: 500.0\n","epoch 60/100 return: 500.0\n","epoch 70/100 return: 500.0\n","epoch 80/100 return: 500.0\n","epoch 90/100 return: 500.0\n","###############    Reward for test environment for run 3: 500.0.   ###############\n","\n","\n","Run 4 out of 10\n","state_dim 8\n","epoch 9000/10000, policy loss 0.42002660036087036\tepoch 0/100 return: 500.0\n","epoch 10/100 return: 500.0\n","epoch 20/100 return: 500.0\n","epoch 30/100 return: 500.0\n","epoch 40/100 return: 500.0\n","epoch 50/100 return: 500.0\n","epoch 60/100 return: 500.0\n","epoch 70/100 return: 500.0\n","epoch 80/100 return: 500.0\n","epoch 90/100 return: 500.0\n","###############    Reward for test environment for run 4: 500.0.   ###############\n","\n","\n","Run 5 out of 10\n","state_dim 8\n","epoch 9000/10000, policy loss 0.39635413885116577\tepoch 0/100 return: 500.0\n","epoch 10/100 return: 500.0\n","epoch 20/100 return: 500.0\n","epoch 30/100 return: 500.0\n","epoch 40/100 return: 500.0\n","epoch 50/100 return: 500.0\n","epoch 60/100 return: 500.0\n","epoch 70/100 return: 500.0\n","epoch 80/100 return: 500.0\n","epoch 90/100 return: 500.0\n","###############    Reward for test environment for run 5: 500.0.   ###############\n","\n","\n","Run 6 out of 10\n","state_dim 8\n","epoch 9000/10000, policy loss 0.27527081966400146\tepoch 0/100 return: 500.0\n","epoch 10/100 return: 500.0\n","epoch 20/100 return: 500.0\n","epoch 30/100 return: 500.0\n","epoch 40/100 return: 500.0\n","epoch 50/100 return: 500.0\n","epoch 60/100 return: 500.0\n","epoch 70/100 return: 498.0\n","epoch 80/100 return: 500.0\n","epoch 90/100 return: 500.0\n","###############    Reward for test environment for run 6: 498.26.   ###############\n","\n","\n","Run 7 out of 10\n","state_dim 8\n","epoch 9000/10000, policy loss 0.4060079753398895\tepoch 0/100 return: 500.0\n","epoch 10/100 return: 500.0\n","epoch 20/100 return: 500.0\n","epoch 30/100 return: 500.0\n","epoch 40/100 return: 500.0\n","epoch 50/100 return: 500.0\n","epoch 60/100 return: 500.0\n","epoch 70/100 return: 500.0\n","epoch 80/100 return: 500.0\n","epoch 90/100 return: 500.0\n","###############    Reward for test environment for run 7: 500.0.   ###############\n","\n","\n","Run 8 out of 10\n","state_dim 8\n","epoch 9000/10000, policy loss 0.33991172909736633\tepoch 0/100 return: 500.0\n","epoch 10/100 return: 500.0\n","epoch 20/100 return: 500.0\n","epoch 30/100 return: 500.0\n","epoch 40/100 return: 500.0\n","epoch 50/100 return: 500.0\n","epoch 60/100 return: 500.0\n","epoch 70/100 return: 500.0\n","epoch 80/100 return: 500.0\n","epoch 90/100 return: 500.0\n","###############    Reward for test environment for run 8: 500.0.   ###############\n","\n","\n","Run 9 out of 10\n","state_dim 8\n","epoch 9000/10000, policy loss 0.4035148322582245\tepoch 0/100 return: 500.0\n","epoch 10/100 return: 500.0\n","epoch 20/100 return: 500.0\n","epoch 30/100 return: 500.0\n","epoch 40/100 return: 500.0\n","epoch 50/100 return: 500.0\n","epoch 60/100 return: 500.0\n","epoch 70/100 return: 500.0\n","epoch 80/100 return: 500.0\n","epoch 90/100 return: 500.0\n","###############    Reward for test environment for run 9: 500.0.   ###############\n","\n","\n","Run 10 out of 10\n","state_dim 8\n","epoch 9000/10000, policy loss 0.3684833347797394\tepoch 0/100 return: 500.0\n","epoch 10/100 return: 500.0\n","epoch 20/100 return: 500.0\n","epoch 30/100 return: 500.0\n","epoch 40/100 return: 500.0\n","epoch 50/100 return: 500.0\n","epoch 60/100 return: 500.0\n","epoch 70/100 return: 500.0\n","epoch 80/100 return: 500.0\n","epoch 90/100 return: 500.0\n","###############    Reward for test environment for run 10: 500.0.   ###############\n","\n","\n","Average reward for 10 repetitions: 499.826\n","ALL RESULTS TRAIL: [499.826]\n","Config: {'ENV': 'CartPole-v1', 'ALG': 'FINAL_Apr24_BCStudent_origindata_trajnum40', 'NUM_TRAJS_GIVEN': 40, 'NUM_TRAINING_ENVS': 2, 'NOISE_DIM': 4, 'REP_SIZE': 16, 'TRAJ_SHIFT': 40, 'SAMPLING_RATE': 5, 'NUM_STEPS_TRAIN': 10000, 'NUM_TRAJS_VALID': 100, 'NUM_REPETITIONS': 10, 'BATCH_SIZE': 64, 'MLP_WIDTHS': 64, 'ADAM_ALPHA': 0.001, 'SGLD_BUFFER_SIZE': 10000, 'SGLD_LEARN_RATE': 0.01, 'SGLD_NOISE_COEF': 0.01, 'SGLD_NUM_STEPS': 100, 'SGLD_REINIT_FREQ': 0.05, 'NUM_STEPS_TRAIN_ENERGY_MODEL': 1000, 'TRIAL': 0, 'METHOD': 'BC', 'EXPERT_ALG': 'dqn'}\n","Trial number 0\n","config method =  BC\n","config env =  CartPole-v1\n","Run 1 out of 10\n","state_dim 8\n","epoch 9000/10000, policy loss 0.5270767211914062\tepoch 0/100 return: 500.0\n","epoch 10/100 return: 500.0\n","epoch 20/100 return: 500.0\n","epoch 30/100 return: 500.0\n","epoch 40/100 return: 500.0\n","epoch 50/100 return: 500.0\n","epoch 60/100 return: 500.0\n","epoch 70/100 return: 500.0\n","epoch 80/100 return: 500.0\n","epoch 90/100 return: 500.0\n","###############    Reward for test environment for run 1: 500.0.   ###############\n","\n","\n","Run 2 out of 10\n","state_dim 8\n","epoch 9000/10000, policy loss 0.4183540344238281\tepoch 0/100 return: 500.0\n","epoch 10/100 return: 500.0\n","epoch 20/100 return: 500.0\n","epoch 30/100 return: 500.0\n","epoch 40/100 return: 500.0\n","epoch 50/100 return: 500.0\n","epoch 60/100 return: 500.0\n","epoch 70/100 return: 500.0\n","epoch 80/100 return: 500.0\n","epoch 90/100 return: 500.0\n","###############    Reward for test environment for run 2: 500.0.   ###############\n","\n","\n","Run 3 out of 10\n","state_dim 8\n","epoch 9000/10000, policy loss 0.32791954278945923\tepoch 0/100 return: 500.0\n","epoch 10/100 return: 500.0\n","epoch 20/100 return: 500.0\n","epoch 30/100 return: 500.0\n","epoch 40/100 return: 500.0\n","epoch 50/100 return: 500.0\n","epoch 60/100 return: 500.0\n","epoch 70/100 return: 500.0\n","epoch 80/100 return: 500.0\n","epoch 90/100 return: 500.0\n","###############    Reward for test environment for run 3: 500.0.   ###############\n","\n","\n","Run 4 out of 10\n","state_dim 8\n","epoch 9000/10000, policy loss 0.46380600333213806\tepoch 0/100 return: 500.0\n","epoch 10/100 return: 500.0\n","epoch 20/100 return: 500.0\n","epoch 30/100 return: 500.0\n","epoch 40/100 return: 500.0\n","epoch 50/100 return: 500.0\n","epoch 60/100 return: 500.0\n","epoch 70/100 return: 500.0\n","epoch 80/100 return: 500.0\n","epoch 90/100 return: 500.0\n","###############    Reward for test environment for run 4: 500.0.   ###############\n","\n","\n","Run 5 out of 10\n","state_dim 8\n","epoch 9000/10000, policy loss 0.41301506757736206\tepoch 0/100 return: 500.0\n","epoch 10/100 return: 500.0\n","epoch 20/100 return: 500.0\n","epoch 30/100 return: 500.0\n","epoch 40/100 return: 500.0\n","epoch 50/100 return: 500.0\n","epoch 60/100 return: 500.0\n","epoch 70/100 return: 500.0\n","epoch 80/100 return: 500.0\n","epoch 90/100 return: 500.0\n","###############    Reward for test environment for run 5: 500.0.   ###############\n","\n","\n","Run 6 out of 10\n","state_dim 8\n","epoch 9000/10000, policy loss 0.45526498556137085\tepoch 0/100 return: 500.0\n","epoch 10/100 return: 500.0\n","epoch 20/100 return: 500.0\n","epoch 30/100 return: 500.0\n","epoch 40/100 return: 500.0\n","epoch 50/100 return: 500.0\n","epoch 60/100 return: 500.0\n","epoch 70/100 return: 500.0\n","epoch 80/100 return: 500.0\n","epoch 90/100 return: 500.0\n","###############    Reward for test environment for run 6: 500.0.   ###############\n","\n","\n","Run 7 out of 10\n","state_dim 8\n","epoch 9000/10000, policy loss 0.25461530685424805\tepoch 0/100 return: 500.0\n","epoch 10/100 return: 500.0\n","epoch 20/100 return: 500.0\n","epoch 30/100 return: 500.0\n","epoch 40/100 return: 500.0\n","epoch 50/100 return: 500.0\n","epoch 60/100 return: 500.0\n","epoch 70/100 return: 500.0\n","epoch 80/100 return: 500.0\n","epoch 90/100 return: 500.0\n","###############    Reward for test environment for run 7: 500.0.   ###############\n","\n","\n","Run 8 out of 10\n","state_dim 8\n","epoch 9000/10000, policy loss 0.3554292619228363\tepoch 0/100 return: 500.0\n","epoch 10/100 return: 500.0\n","epoch 20/100 return: 500.0\n","epoch 30/100 return: 500.0\n","epoch 40/100 return: 500.0\n","epoch 50/100 return: 500.0\n","epoch 60/100 return: 500.0\n","epoch 70/100 return: 500.0\n","epoch 80/100 return: 500.0\n","epoch 90/100 return: 500.0\n","###############    Reward for test environment for run 8: 500.0.   ###############\n","\n","\n","Run 9 out of 10\n","state_dim 8\n","epoch 9000/10000, policy loss 0.47154727578163147\tepoch 0/100 return: 500.0\n","epoch 10/100 return: 500.0\n","epoch 20/100 return: 500.0\n","epoch 30/100 return: 500.0\n","epoch 40/100 return: 500.0\n","epoch 50/100 return: 500.0\n","epoch 60/100 return: 500.0\n","epoch 70/100 return: 500.0\n","epoch 80/100 return: 500.0\n","epoch 90/100 return: 500.0\n","###############    Reward for test environment for run 9: 500.0.   ###############\n","\n","\n","Run 10 out of 10\n","state_dim 8\n","epoch 9000/10000, policy loss 0.3420974910259247\tepoch 0/100 return: 500.0\n","epoch 10/100 return: 500.0\n","epoch 20/100 return: 500.0\n","epoch 30/100 return: 500.0\n","epoch 40/100 return: 500.0\n","epoch 50/100 return: 500.0\n","epoch 60/100 return: 500.0\n","epoch 70/100 return: 500.0\n","epoch 80/100 return: 500.0\n","epoch 90/100 return: 500.0\n","###############    Reward for test environment for run 10: 500.0.   ###############\n","\n","\n","Average reward for 10 repetitions: 500.0\n","ALL RESULTS TRAIL: [500.0]\n","Config: {'ENV': 'CartPole-v1', 'ALG': 'FINAL_Apr24_BCStudent_origindata_trajnum50', 'NUM_TRAJS_GIVEN': 50, 'NUM_TRAINING_ENVS': 2, 'NOISE_DIM': 4, 'REP_SIZE': 16, 'TRAJ_SHIFT': 50, 'SAMPLING_RATE': 5, 'NUM_STEPS_TRAIN': 10000, 'NUM_TRAJS_VALID': 100, 'NUM_REPETITIONS': 10, 'BATCH_SIZE': 64, 'MLP_WIDTHS': 64, 'ADAM_ALPHA': 0.001, 'SGLD_BUFFER_SIZE': 10000, 'SGLD_LEARN_RATE': 0.01, 'SGLD_NOISE_COEF': 0.01, 'SGLD_NUM_STEPS': 100, 'SGLD_REINIT_FREQ': 0.05, 'NUM_STEPS_TRAIN_ENERGY_MODEL': 1000, 'TRIAL': 0, 'METHOD': 'BC', 'EXPERT_ALG': 'dqn'}\n","Trial number 0\n","config method =  BC\n","config env =  CartPole-v1\n","Run 1 out of 10\n","state_dim 8\n","epoch 9000/10000, policy loss 0.4306268095970154\tepoch 0/100 return: 500.0\n","epoch 10/100 return: 500.0\n","epoch 20/100 return: 500.0\n","epoch 30/100 return: 500.0\n","epoch 40/100 return: 500.0\n","epoch 50/100 return: 500.0\n","epoch 60/100 return: 500.0\n","epoch 70/100 return: 500.0\n","epoch 80/100 return: 500.0\n","epoch 90/100 return: 500.0\n","###############    Reward for test environment for run 1: 500.0.   ###############\n","\n","\n","Run 2 out of 10\n","state_dim 8\n","epoch 9000/10000, policy loss 0.4173005521297455\tepoch 0/100 return: 500.0\n","epoch 10/100 return: 500.0\n","epoch 20/100 return: 500.0\n","epoch 30/100 return: 500.0\n","epoch 40/100 return: 500.0\n","epoch 50/100 return: 500.0\n","epoch 60/100 return: 500.0\n","epoch 70/100 return: 500.0\n","epoch 80/100 return: 500.0\n","epoch 90/100 return: 500.0\n","###############    Reward for test environment for run 2: 500.0.   ###############\n","\n","\n","Run 3 out of 10\n","state_dim 8\n","epoch 9000/10000, policy loss 0.5014075040817261\tepoch 0/100 return: 500.0\n","epoch 10/100 return: 500.0\n","epoch 20/100 return: 500.0\n","epoch 30/100 return: 500.0\n","epoch 40/100 return: 500.0\n","epoch 50/100 return: 500.0\n","epoch 60/100 return: 500.0\n","epoch 70/100 return: 500.0\n","epoch 80/100 return: 500.0\n","epoch 90/100 return: 500.0\n","###############    Reward for test environment for run 3: 500.0.   ###############\n","\n","\n","Run 4 out of 10\n","state_dim 8\n","epoch 9000/10000, policy loss 0.5324825048446655\tepoch 0/100 return: 500.0\n","epoch 10/100 return: 500.0\n","epoch 20/100 return: 500.0\n","epoch 30/100 return: 500.0\n","epoch 40/100 return: 500.0\n","epoch 50/100 return: 500.0\n","epoch 60/100 return: 500.0\n","epoch 70/100 return: 500.0\n","epoch 80/100 return: 500.0\n","epoch 90/100 return: 500.0\n","###############    Reward for test environment for run 4: 500.0.   ###############\n","\n","\n","Run 5 out of 10\n","state_dim 8\n","epoch 9000/10000, policy loss 0.38457155227661133\tepoch 0/100 return: 500.0\n","epoch 10/100 return: 500.0\n","epoch 20/100 return: 500.0\n","epoch 30/100 return: 500.0\n","epoch 40/100 return: 500.0\n","epoch 50/100 return: 500.0\n","epoch 60/100 return: 500.0\n","epoch 70/100 return: 500.0\n","epoch 80/100 return: 500.0\n","epoch 90/100 return: 500.0\n","###############    Reward for test environment for run 5: 500.0.   ###############\n","\n","\n","Run 6 out of 10\n","state_dim 8\n","epoch 9000/10000, policy loss 0.3715464472770691\tepoch 0/100 return: 500.0\n","epoch 10/100 return: 500.0\n","epoch 20/100 return: 500.0\n","epoch 30/100 return: 500.0\n","epoch 40/100 return: 500.0\n","epoch 50/100 return: 500.0\n","epoch 60/100 return: 500.0\n","epoch 70/100 return: 500.0\n","epoch 80/100 return: 500.0\n","epoch 90/100 return: 500.0\n","###############    Reward for test environment for run 6: 500.0.   ###############\n","\n","\n","Run 7 out of 10\n","state_dim 8\n","epoch 9000/10000, policy loss 0.3585476577281952\tepoch 0/100 return: 500.0\n","epoch 10/100 return: 500.0\n","epoch 20/100 return: 500.0\n","epoch 30/100 return: 500.0\n","epoch 40/100 return: 500.0\n","epoch 50/100 return: 500.0\n","epoch 60/100 return: 500.0\n","epoch 70/100 return: 500.0\n","epoch 80/100 return: 500.0\n","epoch 90/100 return: 500.0\n","###############    Reward for test environment for run 7: 500.0.   ###############\n","\n","\n","Run 8 out of 10\n","state_dim 8\n","epoch 9000/10000, policy loss 0.34910106658935547\tepoch 0/100 return: 500.0\n","epoch 10/100 return: 500.0\n","epoch 20/100 return: 500.0\n","epoch 30/100 return: 500.0\n","epoch 40/100 return: 500.0\n","epoch 50/100 return: 500.0\n","epoch 60/100 return: 500.0\n","epoch 70/100 return: 500.0\n","epoch 80/100 return: 500.0\n","epoch 90/100 return: 500.0\n","###############    Reward for test environment for run 8: 500.0.   ###############\n","\n","\n","Run 9 out of 10\n","state_dim 8\n","epoch 9000/10000, policy loss 0.3894158899784088\tepoch 0/100 return: 500.0\n","epoch 10/100 return: 482.0\n","epoch 20/100 return: 380.0\n","epoch 30/100 return: 498.0\n","epoch 40/100 return: 487.0\n","epoch 50/100 return: 483.0\n","epoch 60/100 return: 500.0\n","epoch 70/100 return: 483.0\n","epoch 80/100 return: 500.0\n","epoch 90/100 return: 500.0\n","###############    Reward for test environment for run 9: 482.75.   ###############\n","\n","\n","Run 10 out of 10\n","state_dim 8\n","epoch 9000/10000, policy loss 0.5587459206581116\tepoch 0/100 return: 500.0\n","epoch 10/100 return: 500.0\n","epoch 20/100 return: 500.0\n","epoch 30/100 return: 500.0\n","epoch 40/100 return: 500.0\n","epoch 50/100 return: 500.0\n","epoch 60/100 return: 500.0\n","epoch 70/100 return: 500.0\n","epoch 80/100 return: 500.0\n","epoch 90/100 return: 500.0\n","###############    Reward for test environment for run 10: 500.0.   ###############\n","\n","\n","Average reward for 10 repetitions: 498.275\n","ALL RESULTS TRAIL: [498.275]\n"]}]},{"cell_type":"code","source":[""],"metadata":{"id":"8EsDN4t12vnB"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["#10 Trails -- BC -- acrobot"],"metadata":{"id":"66KPbPKOony4"}},{"cell_type":"code","source":["config['METHOD'] = \"BC\"\n","config['ENV'] = 'Acrobot-v1'\n","\n","for traj_num in [1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 20]:\n","    config[\"NUM_TRAJS_GIVEN\"] = traj_num\n","    config[\"TRAJ_SHIFT\"] = traj_num\n","\n","\n","    config['ALG'] = \"FINAL_Apr24_BCStudent_origindata_trajnum\" + str(traj_num)\n","\n","\n","    ###############.  settings   ###############\n","    #config['ALG'] = \"BCStudent_Apr19_replicatedata\"\n","    #config['METHOD'] = \"BC\"\n","    #config['ENV'] == \"CartPole-v1\"\n","    #config['ENV'] == \"LunarLander-v2\"\n","    #config[\"NUM_TRAJS_GIVEN\"] = 20\n","    #config[\"TRAJ_SHIFT\"] = 20\n","    ###############.  settings   ###############\n","\n","\n","\n","    if config['METHOD'] == 'BCIRM':\n","        config['l2_regularizer_weight'] = 0.001\n","        config['penalty_weight'] = 10000\n","        config['penalty_anneal_iters'] = 100\n","\n","    all_results_trail = []\n","\n","    for trail in range(1): \n","        config['TRIAL'] = trail \n","\n","\n","        ###############.  start a trail   ###############\n","\n","        config[\"EXPERT_ALG\"] = yaml.load(open(\"testing/config.yml\"), Loader=yaml.FullLoader)[config[\"ENV\"]]\n","        print(\"Config: %s\" % config)\n","\n","        TRIAL = config[\"TRIAL\"] #args.trial\n","        print(\"Trial number %s\" % TRIAL)\n","\n","        results_dir_base = \"testing/results/\"\n","        results_dir = os.path.join(results_dir_base, config[\"ENV\"], str(config[\"NUM_TRAJS_GIVEN\"]), config[\"ALG\"])\n","\n","        if not os.path.exists(results_dir):\n","            os.makedirs(results_dir)\n","\n","        config_file = \"trial_\" + str(TRIAL) + \"_\" + \"config.pkl\"\n","\n","        results_file_name = \"trial_\" + str(TRIAL) + \"_\" + \"results.csv\"\n","        results_file_path = os.path.join(results_dir, results_file_name)\n","\n","        if os.path.exists(os.path.join(results_dir, config_file)):\n","            raise NameError(\"CONFIG file already exists %s. Choose a different trial number.\" % config_file)\n","        pickle.dump(config, open(os.path.join(results_dir, config_file), \"wb\"))\n","\n","\n","\n","\n","        ###############.  10 runs for each trail   ###############\n","\n","        print(\"config method = \", config['METHOD'])\n","        print(\"config env = \", config['ENV'])\n","\n","        for run_seed in range(config[\"NUM_REPETITIONS\"]):\n","            print(\"Run %s out of %s\" % (run_seed + 1, config[\"NUM_REPETITIONS\"]))\n","            student = make_student(run_seed, config)\n","            student.train(num_updates=config[\"NUM_STEPS_TRAIN\"])\n","\n","            env_wrapper_out_of_sample = EnvWrapper(\n","                env=gym.make(config[\"ENV\"]), mult_factor=get_test_mult_factors(config['NOISE_DIM'] - 1), idx=3, seed=1\n","            )\n","            action_match, return_mean, return_std = student.test(\n","                num_episodes=config[\"NUM_TRAJS_VALID\"], env_wrapper=env_wrapper_out_of_sample\n","            )\n","\n","            result = (action_match, return_mean, return_std)\n","            print(\"###############    Reward for test environment for run %s: %s.   ###############\\n\\n\" % (run_seed + 1, return_mean))\n","            save_results(results_file_path, run_seed, action_match, return_mean, return_std)\n","\n","        results_trial = pd.read_csv(\n","            \"testing/results/\"\n","            + config[\"ENV\"]\n","            + \"/\"\n","            + str(config[\"NUM_TRAJS_GIVEN\"])\n","            + \"/\"\n","            + config[\"ALG\"]\n","            + \"/trial_\"\n","            + str(TRIAL)\n","            + \"_results.csv\",\n","            header=None,\n","        )\n","\n","        print(\"Average reward for 10 repetitions: %s\" % np.mean(results_trial[2].values))\n","\n","        all_results_trail.append(np.mean(results_trial[2].values))\n","\n","    print(\"ALL RESULTS TRAIL:\" , all_results_trail)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"outputId":"36cad804-b5b3-4077-c46b-4c7bb98943cd","id":"LtppeiX3ony5","executionInfo":{"status":"ok","timestamp":1650797083385,"user_tz":240,"elapsed":4385055,"user":{"displayName":"3 Lin","userId":"16848054003967173878"}}},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Config: {'ENV': 'Acrobot-v1', 'ALG': 'FINAL_Apr24_BCStudent_origindata_trajnum1', 'NUM_TRAJS_GIVEN': 1, 'NUM_TRAINING_ENVS': 2, 'NOISE_DIM': 4, 'REP_SIZE': 16, 'TRAJ_SHIFT': 1, 'SAMPLING_RATE': 5, 'NUM_STEPS_TRAIN': 10000, 'NUM_TRAJS_VALID': 100, 'NUM_REPETITIONS': 10, 'BATCH_SIZE': 64, 'MLP_WIDTHS': 64, 'ADAM_ALPHA': 0.001, 'SGLD_BUFFER_SIZE': 10000, 'SGLD_LEARN_RATE': 0.01, 'SGLD_NOISE_COEF': 0.01, 'SGLD_NUM_STEPS': 100, 'SGLD_REINIT_FREQ': 0.05, 'NUM_STEPS_TRAIN_ENERGY_MODEL': 1000, 'TRIAL': 0, 'METHOD': 'BC', 'EXPERT_ALG': 'dqn'}\n","Trial number 0\n","config method =  BC\n","config env =  Acrobot-v1\n","Run 1 out of 10\n"]},{"output_type":"stream","name":"stderr","text":["/content/drive/.shortcut-targets-by-id/1OMkIazT1mDvTsmoHYHfcjKlWdwFWLOpV/ImitationLearning/Invariant-Causal-Imitation-Learning-main/testing/train_utils.py:106: UserWarning: Buffer smaller than batch size\n","  warnings.warn(\"Buffer smaller than batch size\")\n"]},{"output_type":"stream","name":"stdout","text":["state_dim 10\n","epoch 9000/10000, policy loss 1.2331994092562581e-08\tepoch 0/100 return: -90.0\n","epoch 10/100 return: -91.0\n","epoch 20/100 return: -138.0\n","epoch 30/100 return: -173.0\n","epoch 40/100 return: -101.0\n","epoch 50/100 return: -79.0\n","epoch 60/100 return: -500.0\n","epoch 70/100 return: -76.0\n","epoch 80/100 return: -77.0\n","epoch 90/100 return: -118.0\n","###############    Reward for test environment for run 1: -168.79.   ###############\n","\n","\n","Run 2 out of 10\n"]},{"output_type":"stream","name":"stderr","text":["/content/drive/.shortcut-targets-by-id/1OMkIazT1mDvTsmoHYHfcjKlWdwFWLOpV/ImitationLearning/Invariant-Causal-Imitation-Learning-main/testing/train_utils.py:106: UserWarning: Buffer smaller than batch size\n","  warnings.warn(\"Buffer smaller than batch size\")\n"]},{"output_type":"stream","name":"stdout","text":["state_dim 10\n","epoch 9000/10000, policy loss 1.43051135381711e-08\tepoch 0/100 return: -99.0\n","epoch 10/100 return: -86.0\n","epoch 20/100 return: -82.0\n","epoch 30/100 return: -83.0\n","epoch 40/100 return: -79.0\n","epoch 50/100 return: -108.0\n","epoch 60/100 return: -79.0\n","epoch 70/100 return: -85.0\n","epoch 80/100 return: -90.0\n","epoch 90/100 return: -86.0\n","###############    Reward for test environment for run 2: -86.37.   ###############\n","\n","\n","Run 3 out of 10\n"]},{"output_type":"stream","name":"stderr","text":["/content/drive/.shortcut-targets-by-id/1OMkIazT1mDvTsmoHYHfcjKlWdwFWLOpV/ImitationLearning/Invariant-Causal-Imitation-Learning-main/testing/train_utils.py:106: UserWarning: Buffer smaller than batch size\n","  warnings.warn(\"Buffer smaller than batch size\")\n"]},{"output_type":"stream","name":"stdout","text":["state_dim 10\n","epoch 9000/10000, policy loss 1.80620123302333e-08\tepoch 0/100 return: -78.0\n","epoch 10/100 return: -79.0\n","epoch 20/100 return: -74.0\n","epoch 30/100 return: -86.0\n","epoch 40/100 return: -91.0\n","epoch 50/100 return: -85.0\n","epoch 60/100 return: -90.0\n","epoch 70/100 return: -78.0\n","epoch 80/100 return: -78.0\n","epoch 90/100 return: -69.0\n","###############    Reward for test environment for run 3: -85.84.   ###############\n","\n","\n","Run 4 out of 10\n"]},{"output_type":"stream","name":"stderr","text":["/content/drive/.shortcut-targets-by-id/1OMkIazT1mDvTsmoHYHfcjKlWdwFWLOpV/ImitationLearning/Invariant-Causal-Imitation-Learning-main/testing/train_utils.py:106: UserWarning: Buffer smaller than batch size\n","  warnings.warn(\"Buffer smaller than batch size\")\n"]},{"output_type":"stream","name":"stdout","text":["state_dim 10\n","epoch 9000/10000, policy loss 1.5894569216357013e-08\tepoch 0/100 return: -500.0\n","epoch 10/100 return: -500.0\n","epoch 20/100 return: -500.0\n","epoch 30/100 return: -500.0\n","epoch 40/100 return: -500.0\n","epoch 50/100 return: -500.0\n","epoch 60/100 return: -500.0\n","epoch 70/100 return: -500.0\n","epoch 80/100 return: -500.0\n","epoch 90/100 return: -500.0\n","###############    Reward for test environment for run 4: -496.71.   ###############\n","\n","\n","Run 5 out of 10\n"]},{"output_type":"stream","name":"stderr","text":["/content/drive/.shortcut-targets-by-id/1OMkIazT1mDvTsmoHYHfcjKlWdwFWLOpV/ImitationLearning/Invariant-Causal-Imitation-Learning-main/testing/train_utils.py:106: UserWarning: Buffer smaller than batch size\n","  warnings.warn(\"Buffer smaller than batch size\")\n"]},{"output_type":"stream","name":"stdout","text":["state_dim 10\n","epoch 9000/10000, policy loss 1.5381843354589364e-08\tepoch 0/100 return: -70.0\n","epoch 10/100 return: -86.0\n","epoch 20/100 return: -393.0\n","epoch 30/100 return: -87.0\n","epoch 40/100 return: -90.0\n","epoch 50/100 return: -75.0\n","epoch 60/100 return: -74.0\n","epoch 70/100 return: -92.0\n","epoch 80/100 return: -78.0\n","epoch 90/100 return: -116.0\n","###############    Reward for test environment for run 5: -108.45.   ###############\n","\n","\n","Run 6 out of 10\n"]},{"output_type":"stream","name":"stderr","text":["/content/drive/.shortcut-targets-by-id/1OMkIazT1mDvTsmoHYHfcjKlWdwFWLOpV/ImitationLearning/Invariant-Causal-Imitation-Learning-main/testing/train_utils.py:106: UserWarning: Buffer smaller than batch size\n","  warnings.warn(\"Buffer smaller than batch size\")\n"]},{"output_type":"stream","name":"stdout","text":["state_dim 10\n","epoch 9000/10000, policy loss 2.6490949878166248e-08\tepoch 0/100 return: -500.0\n","epoch 10/100 return: -500.0\n","epoch 20/100 return: -500.0\n","epoch 30/100 return: -188.0\n","epoch 40/100 return: -92.0\n","epoch 50/100 return: -500.0\n","epoch 60/100 return: -500.0\n","epoch 70/100 return: -95.0\n","epoch 80/100 return: -500.0\n","epoch 90/100 return: -110.0\n","###############    Reward for test environment for run 6: -353.58.   ###############\n","\n","\n","Run 7 out of 10\n"]},{"output_type":"stream","name":"stderr","text":["/content/drive/.shortcut-targets-by-id/1OMkIazT1mDvTsmoHYHfcjKlWdwFWLOpV/ImitationLearning/Invariant-Causal-Imitation-Learning-main/testing/train_utils.py:106: UserWarning: Buffer smaller than batch size\n","  warnings.warn(\"Buffer smaller than batch size\")\n"]},{"output_type":"stream","name":"stdout","text":["state_dim 10\n","epoch 9000/10000, policy loss 1.80620123302333e-08\tepoch 0/100 return: -80.0\n","epoch 10/100 return: -144.0\n","epoch 20/100 return: -180.0\n","epoch 30/100 return: -292.0\n","epoch 40/100 return: -73.0\n","epoch 50/100 return: -84.0\n","epoch 60/100 return: -118.0\n","epoch 70/100 return: -116.0\n","epoch 80/100 return: -93.0\n","epoch 90/100 return: -75.0\n","###############    Reward for test environment for run 7: -100.45.   ###############\n","\n","\n","Run 8 out of 10\n"]},{"output_type":"stream","name":"stderr","text":["/content/drive/.shortcut-targets-by-id/1OMkIazT1mDvTsmoHYHfcjKlWdwFWLOpV/ImitationLearning/Invariant-Causal-Imitation-Learning-main/testing/train_utils.py:106: UserWarning: Buffer smaller than batch size\n","  warnings.warn(\"Buffer smaller than batch size\")\n"]},{"output_type":"stream","name":"stdout","text":["state_dim 10\n","epoch 9000/10000, policy loss 1.7660633844229778e-08\tepoch 0/100 return: -99.0\n","epoch 10/100 return: -96.0\n","epoch 20/100 return: -92.0\n","epoch 30/100 return: -66.0\n","epoch 40/100 return: -88.0\n","epoch 50/100 return: -65.0\n","epoch 60/100 return: -99.0\n","epoch 70/100 return: -74.0\n","epoch 80/100 return: -97.0\n","epoch 90/100 return: -75.0\n","###############    Reward for test environment for run 8: -88.82.   ###############\n","\n","\n","Run 9 out of 10\n"]},{"output_type":"stream","name":"stderr","text":["/content/drive/.shortcut-targets-by-id/1OMkIazT1mDvTsmoHYHfcjKlWdwFWLOpV/ImitationLearning/Invariant-Causal-Imitation-Learning-main/testing/train_utils.py:106: UserWarning: Buffer smaller than batch size\n","  warnings.warn(\"Buffer smaller than batch size\")\n"]},{"output_type":"stream","name":"stdout","text":["state_dim 10\n","epoch 9000/10000, policy loss 1.362391799375473e-08\tepoch 0/100 return: -500.0\n","epoch 10/100 return: -500.0\n","epoch 20/100 return: -500.0\n","epoch 30/100 return: -500.0\n","epoch 40/100 return: -500.0\n","epoch 50/100 return: -500.0\n","epoch 60/100 return: -500.0\n","epoch 70/100 return: -500.0\n","epoch 80/100 return: -500.0\n","epoch 90/100 return: -500.0\n","###############    Reward for test environment for run 9: -496.03.   ###############\n","\n","\n","Run 10 out of 10\n"]},{"output_type":"stream","name":"stderr","text":["/content/drive/.shortcut-targets-by-id/1OMkIazT1mDvTsmoHYHfcjKlWdwFWLOpV/ImitationLearning/Invariant-Causal-Imitation-Learning-main/testing/train_utils.py:106: UserWarning: Buffer smaller than batch size\n","  warnings.warn(\"Buffer smaller than batch size\")\n"]},{"output_type":"stream","name":"stdout","text":["state_dim 10\n","epoch 9000/10000, policy loss 2.0553324375782722e-08\tepoch 0/100 return: -86.0\n","epoch 10/100 return: -133.0\n","epoch 20/100 return: -100.0\n","epoch 30/100 return: -74.0\n","epoch 40/100 return: -112.0\n","epoch 50/100 return: -500.0\n","epoch 60/100 return: -72.0\n","epoch 70/100 return: -65.0\n","epoch 80/100 return: -500.0\n","epoch 90/100 return: -96.0\n","###############    Reward for test environment for run 10: -182.41.   ###############\n","\n","\n","Average reward for 10 repetitions: -216.74499999999998\n","ALL RESULTS TRAIL: [-216.74499999999998]\n","Config: {'ENV': 'Acrobot-v1', 'ALG': 'FINAL_Apr24_BCStudent_origindata_trajnum2', 'NUM_TRAJS_GIVEN': 2, 'NUM_TRAINING_ENVS': 2, 'NOISE_DIM': 4, 'REP_SIZE': 16, 'TRAJ_SHIFT': 2, 'SAMPLING_RATE': 5, 'NUM_STEPS_TRAIN': 10000, 'NUM_TRAJS_VALID': 100, 'NUM_REPETITIONS': 10, 'BATCH_SIZE': 64, 'MLP_WIDTHS': 64, 'ADAM_ALPHA': 0.001, 'SGLD_BUFFER_SIZE': 10000, 'SGLD_LEARN_RATE': 0.01, 'SGLD_NOISE_COEF': 0.01, 'SGLD_NUM_STEPS': 100, 'SGLD_REINIT_FREQ': 0.05, 'NUM_STEPS_TRAIN_ENERGY_MODEL': 1000, 'TRIAL': 0, 'METHOD': 'BC', 'EXPERT_ALG': 'dqn'}\n","Trial number 0\n","config method =  BC\n","config env =  Acrobot-v1\n","Run 1 out of 10\n"]},{"output_type":"stream","name":"stderr","text":["/content/drive/.shortcut-targets-by-id/1OMkIazT1mDvTsmoHYHfcjKlWdwFWLOpV/ImitationLearning/Invariant-Causal-Imitation-Learning-main/testing/train_utils.py:106: UserWarning: Buffer smaller than batch size\n","  warnings.warn(\"Buffer smaller than batch size\")\n"]},{"output_type":"stream","name":"stdout","text":["state_dim 10\n","epoch 9000/10000, policy loss 2.6009296050233388e-08\tepoch 0/100 return: -96.0\n","epoch 10/100 return: -78.0\n","epoch 20/100 return: -77.0\n","epoch 30/100 return: -162.0\n","epoch 40/100 return: -85.0\n","epoch 50/100 return: -66.0\n","epoch 60/100 return: -148.0\n","epoch 70/100 return: -79.0\n","epoch 80/100 return: -80.0\n","epoch 90/100 return: -69.0\n","###############    Reward for test environment for run 1: -89.26.   ###############\n","\n","\n","Run 2 out of 10\n","state_dim 10\n","epoch 9000/10000, policy loss 5.960457016840337e-08\tepoch 0/100 return: -72.0\n","epoch 10/100 return: -71.0\n","epoch 20/100 return: -82.0\n","epoch 30/100 return: -64.0\n","epoch 40/100 return: -89.0\n","epoch 50/100 return: -83.0\n","epoch 60/100 return: -90.0\n","epoch 70/100 return: -74.0\n","epoch 80/100 return: -74.0\n","epoch 90/100 return: -78.0\n","###############    Reward for test environment for run 2: -88.9.   ###############\n","\n","\n","Run 3 out of 10\n"]},{"output_type":"stream","name":"stderr","text":["/content/drive/.shortcut-targets-by-id/1OMkIazT1mDvTsmoHYHfcjKlWdwFWLOpV/ImitationLearning/Invariant-Causal-Imitation-Learning-main/testing/train_utils.py:106: UserWarning: Buffer smaller than batch size\n","  warnings.warn(\"Buffer smaller than batch size\")\n"]},{"output_type":"stream","name":"stdout","text":["state_dim 10\n","epoch 9000/10000, policy loss 3.6368927425201036e-08\tepoch 0/100 return: -71.0\n","epoch 10/100 return: -110.0\n","epoch 20/100 return: -72.0\n","epoch 30/100 return: -82.0\n","epoch 40/100 return: -99.0\n","epoch 50/100 return: -84.0\n","epoch 60/100 return: -217.0\n","epoch 70/100 return: -88.0\n","epoch 80/100 return: -91.0\n","epoch 90/100 return: -93.0\n","###############    Reward for test environment for run 3: -90.4.   ###############\n","\n","\n","Run 4 out of 10\n"]},{"output_type":"stream","name":"stderr","text":["/content/drive/.shortcut-targets-by-id/1OMkIazT1mDvTsmoHYHfcjKlWdwFWLOpV/ImitationLearning/Invariant-Causal-Imitation-Learning-main/testing/train_utils.py:106: UserWarning: Buffer smaller than batch size\n","  warnings.warn(\"Buffer smaller than batch size\")\n"]},{"output_type":"stream","name":"stdout","text":["state_dim 10\n","epoch 9000/10000, policy loss 2.540525478877953e-08\tepoch 0/100 return: -87.0\n","epoch 10/100 return: -84.0\n","epoch 20/100 return: -74.0\n","epoch 30/100 return: -74.0\n","epoch 40/100 return: -65.0\n","epoch 50/100 return: -103.0\n","epoch 60/100 return: -128.0\n","epoch 70/100 return: -102.0\n","epoch 80/100 return: -65.0\n","epoch 90/100 return: -85.0\n","###############    Reward for test environment for run 4: -89.34.   ###############\n","\n","\n","Run 5 out of 10\n","state_dim 10\n","epoch 9000/10000, policy loss 9.8720079222403e-08\tepoch 0/100 return: -77.0\n","epoch 10/100 return: -64.0\n","epoch 20/100 return: -75.0\n","epoch 30/100 return: -72.0\n","epoch 40/100 return: -126.0\n","epoch 50/100 return: -74.0\n","epoch 60/100 return: -91.0\n","epoch 70/100 return: -142.0\n","epoch 80/100 return: -73.0\n","epoch 90/100 return: -83.0\n","###############    Reward for test environment for run 5: -87.47.   ###############\n","\n","\n","Run 6 out of 10\n"]},{"output_type":"stream","name":"stderr","text":["/content/drive/.shortcut-targets-by-id/1OMkIazT1mDvTsmoHYHfcjKlWdwFWLOpV/ImitationLearning/Invariant-Causal-Imitation-Learning-main/testing/train_utils.py:106: UserWarning: Buffer smaller than batch size\n","  warnings.warn(\"Buffer smaller than batch size\")\n"]},{"output_type":"stream","name":"stdout","text":["state_dim 10\n","epoch 9000/10000, policy loss 6.253596041005949e-08\tepoch 0/100 return: -76.0\n","epoch 10/100 return: -93.0\n","epoch 20/100 return: -78.0\n","epoch 30/100 return: -86.0\n","epoch 40/100 return: -78.0\n","epoch 50/100 return: -101.0\n","epoch 60/100 return: -87.0\n","epoch 70/100 return: -77.0\n","epoch 80/100 return: -74.0\n","epoch 90/100 return: -78.0\n","###############    Reward for test environment for run 6: -86.6.   ###############\n","\n","\n","Run 7 out of 10\n"]},{"output_type":"stream","name":"stderr","text":["/content/drive/.shortcut-targets-by-id/1OMkIazT1mDvTsmoHYHfcjKlWdwFWLOpV/ImitationLearning/Invariant-Causal-Imitation-Learning-main/testing/train_utils.py:106: UserWarning: Buffer smaller than batch size\n","  warnings.warn(\"Buffer smaller than batch size\")\n"]},{"output_type":"stream","name":"stdout","text":["state_dim 10\n","epoch 9000/10000, policy loss 2.1496752822258713e-08\tepoch 0/100 return: -105.0\n","epoch 10/100 return: -96.0\n","epoch 20/100 return: -65.0\n","epoch 30/100 return: -66.0\n","epoch 40/100 return: -66.0\n","epoch 50/100 return: -108.0\n","epoch 60/100 return: -119.0\n","epoch 70/100 return: -85.0\n","epoch 80/100 return: -65.0\n","epoch 90/100 return: -105.0\n","###############    Reward for test environment for run 7: -109.03.   ###############\n","\n","\n","Run 8 out of 10\n","state_dim 10\n","epoch 9000/10000, policy loss 3.5390250729960826e-08\tepoch 0/100 return: -90.0\n","epoch 10/100 return: -73.0\n","epoch 20/100 return: -74.0\n","epoch 30/100 return: -77.0\n","epoch 40/100 return: -79.0\n","epoch 50/100 return: -78.0\n","epoch 60/100 return: -77.0\n","epoch 70/100 return: -92.0\n","epoch 80/100 return: -81.0\n","epoch 90/100 return: -86.0\n","###############    Reward for test environment for run 8: -87.73.   ###############\n","\n","\n","Run 9 out of 10\n"]},{"output_type":"stream","name":"stderr","text":["/content/drive/.shortcut-targets-by-id/1OMkIazT1mDvTsmoHYHfcjKlWdwFWLOpV/ImitationLearning/Invariant-Causal-Imitation-Learning-main/testing/train_utils.py:106: UserWarning: Buffer smaller than batch size\n","  warnings.warn(\"Buffer smaller than batch size\")\n"]},{"output_type":"stream","name":"stdout","text":["state_dim 10\n","epoch 9000/10000, policy loss 4.8101963301405704e-08\tepoch 0/100 return: -92.0\n","epoch 10/100 return: -159.0\n","epoch 20/100 return: -120.0\n","epoch 30/100 return: -73.0\n","epoch 40/100 return: -72.0\n","epoch 50/100 return: -95.0\n","epoch 60/100 return: -88.0\n","epoch 70/100 return: -102.0\n","epoch 80/100 return: -128.0\n","epoch 90/100 return: -95.0\n","###############    Reward for test environment for run 9: -106.6.   ###############\n","\n","\n","Run 10 out of 10\n"]},{"output_type":"stream","name":"stderr","text":["/content/drive/.shortcut-targets-by-id/1OMkIazT1mDvTsmoHYHfcjKlWdwFWLOpV/ImitationLearning/Invariant-Causal-Imitation-Learning-main/testing/train_utils.py:106: UserWarning: Buffer smaller than batch size\n","  warnings.warn(\"Buffer smaller than batch size\")\n"]},{"output_type":"stream","name":"stdout","text":["state_dim 10\n","epoch 9000/10000, policy loss 3.555364003204886e-08\tepoch 0/100 return: -85.0\n","epoch 10/100 return: -84.0\n","epoch 20/100 return: -85.0\n","epoch 30/100 return: -99.0\n","epoch 40/100 return: -72.0\n","epoch 50/100 return: -69.0\n","epoch 60/100 return: -71.0\n","epoch 70/100 return: -71.0\n","epoch 80/100 return: -78.0\n","epoch 90/100 return: -81.0\n","###############    Reward for test environment for run 10: -83.97.   ###############\n","\n","\n","Average reward for 10 repetitions: -91.93\n","ALL RESULTS TRAIL: [-91.93]\n","Config: {'ENV': 'Acrobot-v1', 'ALG': 'FINAL_Apr24_BCStudent_origindata_trajnum3', 'NUM_TRAJS_GIVEN': 3, 'NUM_TRAINING_ENVS': 2, 'NOISE_DIM': 4, 'REP_SIZE': 16, 'TRAJ_SHIFT': 3, 'SAMPLING_RATE': 5, 'NUM_STEPS_TRAIN': 10000, 'NUM_TRAJS_VALID': 100, 'NUM_REPETITIONS': 10, 'BATCH_SIZE': 64, 'MLP_WIDTHS': 64, 'ADAM_ALPHA': 0.001, 'SGLD_BUFFER_SIZE': 10000, 'SGLD_LEARN_RATE': 0.01, 'SGLD_NOISE_COEF': 0.01, 'SGLD_NUM_STEPS': 100, 'SGLD_REINIT_FREQ': 0.05, 'NUM_STEPS_TRAIN_ENERGY_MODEL': 1000, 'TRIAL': 0, 'METHOD': 'BC', 'EXPERT_ALG': 'dqn'}\n","Trial number 0\n","config method =  BC\n","config env =  Acrobot-v1\n","Run 1 out of 10\n","state_dim 10\n","epoch 9000/10000, policy loss 5.4016645378851535e-08\tepoch 0/100 return: -91.0\n","epoch 10/100 return: -72.0\n","epoch 20/100 return: -76.0\n","epoch 30/100 return: -73.0\n","epoch 40/100 return: -84.0\n","epoch 50/100 return: -72.0\n","epoch 60/100 return: -80.0\n","epoch 70/100 return: -84.0\n","epoch 80/100 return: -87.0\n","epoch 90/100 return: -129.0\n","###############    Reward for test environment for run 1: -89.91.   ###############\n","\n","\n","Run 2 out of 10\n","state_dim 10\n","epoch 9000/10000, policy loss 1.0803334049569457e-07\tepoch 0/100 return: -77.0\n","epoch 10/100 return: -80.0\n","epoch 20/100 return: -113.0\n","epoch 30/100 return: -78.0\n","epoch 40/100 return: -84.0\n","epoch 50/100 return: -64.0\n","epoch 60/100 return: -98.0\n","epoch 70/100 return: -85.0\n","epoch 80/100 return: -273.0\n","epoch 90/100 return: -82.0\n","###############    Reward for test environment for run 2: -89.04.   ###############\n","\n","\n","Run 3 out of 10\n","state_dim 10\n","epoch 9000/10000, policy loss 1.564620077942891e-07\tepoch 0/100 return: -78.0\n","epoch 10/100 return: -76.0\n","epoch 20/100 return: -78.0\n","epoch 30/100 return: -78.0\n","epoch 40/100 return: -74.0\n","epoch 50/100 return: -77.0\n","epoch 60/100 return: -71.0\n","epoch 70/100 return: -78.0\n","epoch 80/100 return: -88.0\n","epoch 90/100 return: -79.0\n","###############    Reward for test environment for run 3: -82.72.   ###############\n","\n","\n","Run 4 out of 10\n","state_dim 10\n","epoch 9000/10000, policy loss 4.954600285600463e-07\tepoch 0/100 return: -72.0\n","epoch 10/100 return: -71.0\n","epoch 20/100 return: -72.0\n","epoch 30/100 return: -78.0\n","epoch 40/100 return: -85.0\n","epoch 50/100 return: -75.0\n","epoch 60/100 return: -132.0\n","epoch 70/100 return: -97.0\n","epoch 80/100 return: -87.0\n","epoch 90/100 return: -85.0\n","###############    Reward for test environment for run 4: -82.91.   ###############\n","\n","\n","Run 5 out of 10\n","state_dim 10\n","epoch 9000/10000, policy loss 6.519255180137407e-08\tepoch 0/100 return: -76.0\n","epoch 10/100 return: -94.0\n","epoch 20/100 return: -79.0\n","epoch 30/100 return: -73.0\n","epoch 40/100 return: -75.0\n","epoch 50/100 return: -72.0\n","epoch 60/100 return: -65.0\n","epoch 70/100 return: -97.0\n","epoch 80/100 return: -85.0\n","epoch 90/100 return: -73.0\n","###############    Reward for test environment for run 5: -84.17.   ###############\n","\n","\n","Run 6 out of 10\n","state_dim 10\n","epoch 9000/10000, policy loss 2.477311511484004e-07\tepoch 0/100 return: -74.0\n","epoch 10/100 return: -94.0\n","epoch 20/100 return: -78.0\n","epoch 30/100 return: -121.0\n","epoch 40/100 return: -72.0\n","epoch 50/100 return: -500.0\n","epoch 60/100 return: -101.0\n","epoch 70/100 return: -97.0\n","epoch 80/100 return: -90.0\n","epoch 90/100 return: -101.0\n","###############    Reward for test environment for run 6: -106.83.   ###############\n","\n","\n","Run 7 out of 10\n","state_dim 10\n","epoch 9000/10000, policy loss 8.75442012215899e-08\tepoch 0/100 return: -66.0\n","epoch 10/100 return: -79.0\n","epoch 20/100 return: -76.0\n","epoch 30/100 return: -140.0\n","epoch 40/100 return: -66.0\n","epoch 50/100 return: -73.0\n","epoch 60/100 return: -77.0\n","epoch 70/100 return: -81.0\n","epoch 80/100 return: -98.0\n","epoch 90/100 return: -90.0\n","###############    Reward for test environment for run 7: -82.16.   ###############\n","\n","\n","Run 8 out of 10\n","state_dim 10\n","epoch 9000/10000, policy loss 1.639125741803582e-07\tepoch 0/100 return: -73.0\n","epoch 10/100 return: -76.0\n","epoch 20/100 return: -84.0\n","epoch 30/100 return: -77.0\n","epoch 40/100 return: -73.0\n","epoch 50/100 return: -144.0\n","epoch 60/100 return: -79.0\n","epoch 70/100 return: -107.0\n","epoch 80/100 return: -77.0\n","epoch 90/100 return: -95.0\n","###############    Reward for test environment for run 8: -89.9.   ###############\n","\n","\n","Run 9 out of 10\n","state_dim 10\n","epoch 9000/10000, policy loss 7.431913218169939e-07\tepoch 0/100 return: -103.0\n","epoch 10/100 return: -500.0\n","epoch 20/100 return: -500.0\n","epoch 30/100 return: -500.0\n","epoch 40/100 return: -500.0\n","epoch 50/100 return: -500.0\n","epoch 60/100 return: -500.0\n","epoch 70/100 return: -500.0\n","epoch 80/100 return: -157.0\n","epoch 90/100 return: -109.0\n","###############    Reward for test environment for run 9: -435.76.   ###############\n","\n","\n","Run 10 out of 10\n","state_dim 10\n","epoch 9000/10000, policy loss 1.154838713546269e-07\tepoch 0/100 return: -71.0\n","epoch 10/100 return: -73.0\n","epoch 20/100 return: -73.0\n","epoch 30/100 return: -78.0\n","epoch 40/100 return: -72.0\n","epoch 50/100 return: -72.0\n","epoch 60/100 return: -191.0\n","epoch 70/100 return: -76.0\n","epoch 80/100 return: -78.0\n","epoch 90/100 return: -79.0\n","###############    Reward for test environment for run 10: -85.01.   ###############\n","\n","\n","Average reward for 10 repetitions: -122.84100000000001\n","ALL RESULTS TRAIL: [-122.84100000000001]\n","Config: {'ENV': 'Acrobot-v1', 'ALG': 'FINAL_Apr24_BCStudent_origindata_trajnum4', 'NUM_TRAJS_GIVEN': 4, 'NUM_TRAINING_ENVS': 2, 'NOISE_DIM': 4, 'REP_SIZE': 16, 'TRAJ_SHIFT': 4, 'SAMPLING_RATE': 5, 'NUM_STEPS_TRAIN': 10000, 'NUM_TRAJS_VALID': 100, 'NUM_REPETITIONS': 10, 'BATCH_SIZE': 64, 'MLP_WIDTHS': 64, 'ADAM_ALPHA': 0.001, 'SGLD_BUFFER_SIZE': 10000, 'SGLD_LEARN_RATE': 0.01, 'SGLD_NOISE_COEF': 0.01, 'SGLD_NUM_STEPS': 100, 'SGLD_REINIT_FREQ': 0.05, 'NUM_STEPS_TRAIN_ENERGY_MODEL': 1000, 'TRIAL': 0, 'METHOD': 'BC', 'EXPERT_ALG': 'dqn'}\n","Trial number 0\n","config method =  BC\n","config env =  Acrobot-v1\n","Run 1 out of 10\n","state_dim 10\n","epoch 9000/10000, policy loss 1.3038481938565383e-07\tepoch 0/100 return: -78.0\n","epoch 10/100 return: -64.0\n","epoch 20/100 return: -73.0\n","epoch 30/100 return: -73.0\n","epoch 40/100 return: -66.0\n","epoch 50/100 return: -64.0\n","epoch 60/100 return: -75.0\n","epoch 70/100 return: -79.0\n","epoch 80/100 return: -151.0\n","epoch 90/100 return: -64.0\n","###############    Reward for test environment for run 1: -79.17.   ###############\n","\n","\n","Run 2 out of 10\n","state_dim 10\n","epoch 9000/10000, policy loss 1.4528612268804864e-07\tepoch 0/100 return: -116.0\n","epoch 10/100 return: -280.0\n","epoch 20/100 return: -94.0\n","epoch 30/100 return: -112.0\n","epoch 40/100 return: -80.0\n","epoch 50/100 return: -98.0\n","epoch 60/100 return: -238.0\n","epoch 70/100 return: -103.0\n","epoch 80/100 return: -88.0\n","epoch 90/100 return: -113.0\n","###############    Reward for test environment for run 2: -112.08.   ###############\n","\n","\n","Run 3 out of 10\n","state_dim 10\n","epoch 9000/10000, policy loss 2.9391217140073422e-06\tepoch 0/100 return: -105.0\n","epoch 10/100 return: -63.0\n","epoch 20/100 return: -91.0\n","epoch 30/100 return: -71.0\n","epoch 40/100 return: -79.0\n","epoch 50/100 return: -73.0\n","epoch 60/100 return: -83.0\n","epoch 70/100 return: -89.0\n","epoch 80/100 return: -72.0\n","epoch 90/100 return: -77.0\n","###############    Reward for test environment for run 3: -84.94.   ###############\n","\n","\n","Run 4 out of 10\n","state_dim 10\n","epoch 9000/10000, policy loss 5.029136218581698e-08\tepoch 0/100 return: -78.0\n","epoch 10/100 return: -86.0\n","epoch 20/100 return: -79.0\n","epoch 30/100 return: -109.0\n","epoch 40/100 return: -91.0\n","epoch 50/100 return: -80.0\n","epoch 60/100 return: -75.0\n","epoch 70/100 return: -155.0\n","epoch 80/100 return: -70.0\n","epoch 90/100 return: -81.0\n","###############    Reward for test environment for run 4: -85.77.   ###############\n","\n","\n","Run 5 out of 10\n","state_dim 10\n","epoch 9000/10000, policy loss 4.451707695807272e-07\tepoch 0/100 return: -97.0\n","epoch 10/100 return: -90.0\n","epoch 20/100 return: -86.0\n","epoch 30/100 return: -130.0\n","epoch 40/100 return: -84.0\n","epoch 50/100 return: -72.0\n","epoch 60/100 return: -113.0\n","epoch 70/100 return: -297.0\n","epoch 80/100 return: -95.0\n","epoch 90/100 return: -96.0\n","###############    Reward for test environment for run 5: -116.42.   ###############\n","\n","\n","Run 6 out of 10\n","state_dim 10\n","epoch 9000/10000, policy loss 8.865939093993802e-07\tepoch 0/100 return: -77.0\n","epoch 10/100 return: -72.0\n","epoch 20/100 return: -77.0\n","epoch 30/100 return: -89.0\n","epoch 40/100 return: -77.0\n","epoch 50/100 return: -79.0\n","epoch 60/100 return: -77.0\n","epoch 70/100 return: -78.0\n","epoch 80/100 return: -77.0\n","epoch 90/100 return: -77.0\n","###############    Reward for test environment for run 6: -83.82.   ###############\n","\n","\n","Run 7 out of 10\n","state_dim 10\n","epoch 9000/10000, policy loss 2.855388856914942e-06\tepoch 0/100 return: -92.0\n","epoch 10/100 return: -83.0\n","epoch 20/100 return: -66.0\n","epoch 30/100 return: -97.0\n","epoch 40/100 return: -84.0\n","epoch 50/100 return: -76.0\n","epoch 60/100 return: -90.0\n","epoch 70/100 return: -102.0\n","epoch 80/100 return: -103.0\n","epoch 90/100 return: -88.0\n","###############    Reward for test environment for run 7: -122.53.   ###############\n","\n","\n","Run 8 out of 10\n","state_dim 10\n","epoch 9000/10000, policy loss 4.060555056639714e-07\tepoch 0/100 return: -252.0\n","epoch 10/100 return: -133.0\n","epoch 20/100 return: -88.0\n","epoch 30/100 return: -97.0\n","epoch 40/100 return: -112.0\n","epoch 50/100 return: -148.0\n","epoch 60/100 return: -104.0\n","epoch 70/100 return: -89.0\n","epoch 80/100 return: -114.0\n","epoch 90/100 return: -120.0\n","###############    Reward for test environment for run 8: -146.85.   ###############\n","\n","\n","Run 9 out of 10\n","state_dim 10\n","epoch 9000/10000, policy loss 4.0978014226311643e-07\tepoch 0/100 return: -75.0\n","epoch 10/100 return: -76.0\n","epoch 20/100 return: -87.0\n","epoch 30/100 return: -102.0\n","epoch 40/100 return: -76.0\n","epoch 50/100 return: -86.0\n","epoch 60/100 return: -78.0\n","epoch 70/100 return: -89.0\n","epoch 80/100 return: -81.0\n","epoch 90/100 return: -181.0\n","###############    Reward for test environment for run 9: -90.51.   ###############\n","\n","\n","Run 10 out of 10\n","state_dim 10\n","epoch 9000/10000, policy loss 1.4901141298651055e-07\tepoch 0/100 return: -98.0\n","epoch 10/100 return: -105.0\n","epoch 20/100 return: -73.0\n","epoch 30/100 return: -97.0\n","epoch 40/100 return: -95.0\n","epoch 50/100 return: -96.0\n","epoch 60/100 return: -98.0\n","epoch 70/100 return: -72.0\n","epoch 80/100 return: -97.0\n","epoch 90/100 return: -92.0\n","###############    Reward for test environment for run 10: -88.74.   ###############\n","\n","\n","Average reward for 10 repetitions: -101.083\n","ALL RESULTS TRAIL: [-101.083]\n","Config: {'ENV': 'Acrobot-v1', 'ALG': 'FINAL_Apr24_BCStudent_origindata_trajnum5', 'NUM_TRAJS_GIVEN': 5, 'NUM_TRAINING_ENVS': 2, 'NOISE_DIM': 4, 'REP_SIZE': 16, 'TRAJ_SHIFT': 5, 'SAMPLING_RATE': 5, 'NUM_STEPS_TRAIN': 10000, 'NUM_TRAJS_VALID': 100, 'NUM_REPETITIONS': 10, 'BATCH_SIZE': 64, 'MLP_WIDTHS': 64, 'ADAM_ALPHA': 0.001, 'SGLD_BUFFER_SIZE': 10000, 'SGLD_LEARN_RATE': 0.01, 'SGLD_NOISE_COEF': 0.01, 'SGLD_NUM_STEPS': 100, 'SGLD_REINIT_FREQ': 0.05, 'NUM_STEPS_TRAIN_ENERGY_MODEL': 1000, 'TRIAL': 0, 'METHOD': 'BC', 'EXPERT_ALG': 'dqn'}\n","Trial number 0\n","config method =  BC\n","config env =  Acrobot-v1\n","Run 1 out of 10\n","state_dim 10\n","epoch 9000/10000, policy loss 0.0\tepoch 0/100 return: -162.0\n","epoch 10/100 return: -90.0\n","epoch 20/100 return: -89.0\n","epoch 30/100 return: -79.0\n","epoch 40/100 return: -64.0\n","epoch 50/100 return: -82.0\n","epoch 60/100 return: -89.0\n","epoch 70/100 return: -64.0\n","epoch 80/100 return: -64.0\n","epoch 90/100 return: -74.0\n","###############    Reward for test environment for run 1: -87.83.   ###############\n","\n","\n","Run 2 out of 10\n","state_dim 10\n","epoch 9000/10000, policy loss 7.227006335597252e-07\tepoch 0/100 return: -95.0\n","epoch 10/100 return: -86.0\n","epoch 20/100 return: -72.0\n","epoch 30/100 return: -78.0\n","epoch 40/100 return: -91.0\n","epoch 50/100 return: -80.0\n","epoch 60/100 return: -99.0\n","epoch 70/100 return: -94.0\n","epoch 80/100 return: -81.0\n","epoch 90/100 return: -72.0\n","###############    Reward for test environment for run 2: -82.11.   ###############\n","\n","\n","Run 3 out of 10\n","state_dim 10\n","epoch 9000/10000, policy loss 5.211394636717159e-06\tepoch 0/100 return: -193.0\n","epoch 10/100 return: -74.0\n","epoch 20/100 return: -73.0\n","epoch 30/100 return: -112.0\n","epoch 40/100 return: -90.0\n","epoch 50/100 return: -177.0\n","epoch 60/100 return: -83.0\n","epoch 70/100 return: -80.0\n","epoch 80/100 return: -87.0\n","epoch 90/100 return: -171.0\n","###############    Reward for test environment for run 3: -100.38.   ###############\n","\n","\n","Run 4 out of 10\n","state_dim 10\n","epoch 9000/10000, policy loss 7.543676474597305e-07\tepoch 0/100 return: -86.0\n","epoch 10/100 return: -151.0\n","epoch 20/100 return: -79.0\n","epoch 30/100 return: -73.0\n","epoch 40/100 return: -79.0\n","epoch 50/100 return: -90.0\n","epoch 60/100 return: -85.0\n","epoch 70/100 return: -72.0\n","epoch 80/100 return: -73.0\n","epoch 90/100 return: -85.0\n","###############    Reward for test environment for run 4: -88.07.   ###############\n","\n","\n","Run 5 out of 10\n","state_dim 10\n","epoch 9000/10000, policy loss 5.251956281426828e-06\tepoch 0/100 return: -73.0\n","epoch 10/100 return: -65.0\n","epoch 20/100 return: -86.0\n","epoch 30/100 return: -72.0\n","epoch 40/100 return: -115.0\n","epoch 50/100 return: -72.0\n","epoch 60/100 return: -71.0\n","epoch 70/100 return: -108.0\n","epoch 80/100 return: -84.0\n","epoch 90/100 return: -77.0\n","###############    Reward for test environment for run 5: -83.32.   ###############\n","\n","\n","Run 6 out of 10\n","state_dim 10\n","epoch 9000/10000, policy loss 5.364400408325309e-07\tepoch 0/100 return: -79.0\n","epoch 10/100 return: -171.0\n","epoch 20/100 return: -72.0\n","epoch 30/100 return: -71.0\n","epoch 40/100 return: -98.0\n","epoch 50/100 return: -72.0\n","epoch 60/100 return: -85.0\n","epoch 70/100 return: -73.0\n","epoch 80/100 return: -73.0\n","epoch 90/100 return: -72.0\n","###############    Reward for test environment for run 6: -82.16.   ###############\n","\n","\n","Run 7 out of 10\n","state_dim 10\n","epoch 9000/10000, policy loss 8.940688189795765e-08\tepoch 0/100 return: -84.0\n","epoch 10/100 return: -89.0\n","epoch 20/100 return: -85.0\n","epoch 30/100 return: -82.0\n","epoch 40/100 return: -73.0\n","epoch 50/100 return: -72.0\n","epoch 60/100 return: -94.0\n","epoch 70/100 return: -76.0\n","epoch 80/100 return: -82.0\n","epoch 90/100 return: -77.0\n","###############    Reward for test environment for run 7: -91.39.   ###############\n","\n","\n","Run 8 out of 10\n","state_dim 10\n","epoch 9000/10000, policy loss 1.825388551424112e-07\tepoch 0/100 return: -79.0\n","epoch 10/100 return: -84.0\n","epoch 20/100 return: -70.0\n","epoch 30/100 return: -78.0\n","epoch 40/100 return: -100.0\n","epoch 50/100 return: -98.0\n","epoch 60/100 return: -102.0\n","epoch 70/100 return: -74.0\n","epoch 80/100 return: -83.0\n","epoch 90/100 return: -80.0\n","###############    Reward for test environment for run 8: -82.76.   ###############\n","\n","\n","Run 9 out of 10\n","state_dim 10\n","epoch 9000/10000, policy loss 4.675200102610688e-07\tepoch 0/100 return: -121.0\n","epoch 10/100 return: -152.0\n","epoch 20/100 return: -65.0\n","epoch 30/100 return: -72.0\n","epoch 40/100 return: -74.0\n","epoch 50/100 return: -66.0\n","epoch 60/100 return: -117.0\n","epoch 70/100 return: -90.0\n","epoch 80/100 return: -89.0\n","epoch 90/100 return: -73.0\n","###############    Reward for test environment for run 9: -101.67.   ###############\n","\n","\n","Run 10 out of 10\n","state_dim 10\n","epoch 9000/10000, policy loss 2.4400566189797246e-07\tepoch 0/100 return: -83.0\n","epoch 10/100 return: -74.0\n","epoch 20/100 return: -84.0\n","epoch 30/100 return: -78.0\n","epoch 40/100 return: -72.0\n","epoch 50/100 return: -105.0\n","epoch 60/100 return: -72.0\n","epoch 70/100 return: -72.0\n","epoch 80/100 return: -83.0\n","epoch 90/100 return: -77.0\n","###############    Reward for test environment for run 10: -84.93.   ###############\n","\n","\n","Average reward for 10 repetitions: -88.46199999999999\n","ALL RESULTS TRAIL: [-88.46199999999999]\n","Config: {'ENV': 'Acrobot-v1', 'ALG': 'FINAL_Apr24_BCStudent_origindata_trajnum6', 'NUM_TRAJS_GIVEN': 6, 'NUM_TRAINING_ENVS': 2, 'NOISE_DIM': 4, 'REP_SIZE': 16, 'TRAJ_SHIFT': 6, 'SAMPLING_RATE': 5, 'NUM_STEPS_TRAIN': 10000, 'NUM_TRAJS_VALID': 100, 'NUM_REPETITIONS': 10, 'BATCH_SIZE': 64, 'MLP_WIDTHS': 64, 'ADAM_ALPHA': 0.001, 'SGLD_BUFFER_SIZE': 10000, 'SGLD_LEARN_RATE': 0.01, 'SGLD_NOISE_COEF': 0.01, 'SGLD_NUM_STEPS': 100, 'SGLD_REINIT_FREQ': 0.05, 'NUM_STEPS_TRAIN_ENERGY_MODEL': 1000, 'TRIAL': 0, 'METHOD': 'BC', 'EXPERT_ALG': 'dqn'}\n","Trial number 0\n","config method =  BC\n","config env =  Acrobot-v1\n","Run 1 out of 10\n","state_dim 10\n","epoch 9000/10000, policy loss 1.6763802790364934e-08\tepoch 0/100 return: -95.0\n","epoch 10/100 return: -85.0\n","epoch 20/100 return: -79.0\n","epoch 30/100 return: -100.0\n","epoch 40/100 return: -66.0\n","epoch 50/100 return: -64.0\n","epoch 60/100 return: -81.0\n","epoch 70/100 return: -73.0\n","epoch 80/100 return: -149.0\n","epoch 90/100 return: -95.0\n","###############    Reward for test environment for run 1: -81.39.   ###############\n","\n","\n","Run 2 out of 10\n","state_dim 10\n","epoch 9000/10000, policy loss 4.4553791667567566e-05\tepoch 0/100 return: -208.0\n","epoch 10/100 return: -89.0\n","epoch 20/100 return: -79.0\n","epoch 30/100 return: -71.0\n","epoch 40/100 return: -78.0\n","epoch 50/100 return: -78.0\n","epoch 60/100 return: -72.0\n","epoch 70/100 return: -84.0\n","epoch 80/100 return: -72.0\n","epoch 90/100 return: -78.0\n","###############    Reward for test environment for run 2: -84.69.   ###############\n","\n","\n","Run 3 out of 10\n","state_dim 10\n","epoch 9000/10000, policy loss 5.159494094186812e-07\tepoch 0/100 return: -76.0\n","epoch 10/100 return: -80.0\n","epoch 20/100 return: -87.0\n","epoch 30/100 return: -78.0\n","epoch 40/100 return: -81.0\n","epoch 50/100 return: -87.0\n","epoch 60/100 return: -74.0\n","epoch 70/100 return: -74.0\n","epoch 80/100 return: -131.0\n","epoch 90/100 return: -77.0\n","###############    Reward for test environment for run 3: -89.53.   ###############\n","\n","\n","Run 4 out of 10\n","state_dim 10\n","epoch 9000/10000, policy loss 0.012958304025232792\tepoch 0/100 return: -84.0\n","epoch 10/100 return: -91.0\n","epoch 20/100 return: -89.0\n","epoch 30/100 return: -187.0\n","epoch 40/100 return: -72.0\n","epoch 50/100 return: -72.0\n","epoch 60/100 return: -103.0\n","epoch 70/100 return: -77.0\n","epoch 80/100 return: -113.0\n","epoch 90/100 return: -86.0\n","###############    Reward for test environment for run 4: -90.1.   ###############\n","\n","\n","Run 5 out of 10\n","state_dim 10\n","epoch 9000/10000, policy loss 1.5906821317912545e-06\tepoch 0/100 return: -101.0\n","epoch 10/100 return: -89.0\n","epoch 20/100 return: -74.0\n","epoch 30/100 return: -99.0\n","epoch 40/100 return: -64.0\n","epoch 50/100 return: -86.0\n","epoch 60/100 return: -83.0\n","epoch 70/100 return: -70.0\n","epoch 80/100 return: -78.0\n","epoch 90/100 return: -64.0\n","###############    Reward for test environment for run 5: -88.57.   ###############\n","\n","\n","Run 6 out of 10\n","state_dim 10\n","epoch 9000/10000, policy loss 4.0232933429251716e-07\tepoch 0/100 return: -279.0\n","epoch 10/100 return: -90.0\n","epoch 20/100 return: -78.0\n","epoch 30/100 return: -73.0\n","epoch 40/100 return: -110.0\n","epoch 50/100 return: -74.0\n","epoch 60/100 return: -84.0\n","epoch 70/100 return: -72.0\n","epoch 80/100 return: -77.0\n","epoch 90/100 return: -72.0\n","###############    Reward for test environment for run 6: -86.78.   ###############\n","\n","\n","Run 7 out of 10\n","state_dim 10\n","epoch 9000/10000, policy loss 3.8929098877815704e-07\tepoch 0/100 return: -174.0\n","epoch 10/100 return: -77.0\n","epoch 20/100 return: -111.0\n","epoch 30/100 return: -71.0\n","epoch 40/100 return: -90.0\n","epoch 50/100 return: -73.0\n","epoch 60/100 return: -89.0\n","epoch 70/100 return: -132.0\n","epoch 80/100 return: -64.0\n","epoch 90/100 return: -87.0\n","###############    Reward for test environment for run 7: -84.09.   ###############\n","\n","\n","Run 8 out of 10\n","state_dim 10\n","epoch 9000/10000, policy loss 1.1231568350922316e-06\tepoch 0/100 return: -71.0\n","epoch 10/100 return: -90.0\n","epoch 20/100 return: -76.0\n","epoch 30/100 return: -80.0\n","epoch 40/100 return: -72.0\n","epoch 50/100 return: -69.0\n","epoch 60/100 return: -84.0\n","epoch 70/100 return: -77.0\n","epoch 80/100 return: -93.0\n","epoch 90/100 return: -77.0\n","###############    Reward for test environment for run 8: -82.71.   ###############\n","\n","\n","Run 9 out of 10\n","state_dim 10\n","epoch 9000/10000, policy loss 2.019035264311242e-06\tepoch 0/100 return: -71.0\n","epoch 10/100 return: -91.0\n","epoch 20/100 return: -69.0\n","epoch 30/100 return: -86.0\n","epoch 40/100 return: -91.0\n","epoch 50/100 return: -69.0\n","epoch 60/100 return: -88.0\n","epoch 70/100 return: -71.0\n","epoch 80/100 return: -69.0\n","epoch 90/100 return: -77.0\n","###############    Reward for test environment for run 9: -84.91.   ###############\n","\n","\n","Run 10 out of 10\n","state_dim 10\n","epoch 9000/10000, policy loss 9.83461632131366e-07\tepoch 0/100 return: -93.0\n","epoch 10/100 return: -74.0\n","epoch 20/100 return: -99.0\n","epoch 30/100 return: -73.0\n","epoch 40/100 return: -73.0\n","epoch 50/100 return: -73.0\n","epoch 60/100 return: -65.0\n","epoch 70/100 return: -66.0\n","epoch 80/100 return: -66.0\n","epoch 90/100 return: -95.0\n","###############    Reward for test environment for run 10: -94.94.   ###############\n","\n","\n","Average reward for 10 repetitions: -86.77099999999999\n","ALL RESULTS TRAIL: [-86.77099999999999]\n","Config: {'ENV': 'Acrobot-v1', 'ALG': 'FINAL_Apr24_BCStudent_origindata_trajnum7', 'NUM_TRAJS_GIVEN': 7, 'NUM_TRAINING_ENVS': 2, 'NOISE_DIM': 4, 'REP_SIZE': 16, 'TRAJ_SHIFT': 7, 'SAMPLING_RATE': 5, 'NUM_STEPS_TRAIN': 10000, 'NUM_TRAJS_VALID': 100, 'NUM_REPETITIONS': 10, 'BATCH_SIZE': 64, 'MLP_WIDTHS': 64, 'ADAM_ALPHA': 0.001, 'SGLD_BUFFER_SIZE': 10000, 'SGLD_LEARN_RATE': 0.01, 'SGLD_NOISE_COEF': 0.01, 'SGLD_NUM_STEPS': 100, 'SGLD_REINIT_FREQ': 0.05, 'NUM_STEPS_TRAIN_ENERGY_MODEL': 1000, 'TRIAL': 0, 'METHOD': 'BC', 'EXPERT_ALG': 'dqn'}\n","Trial number 0\n","config method =  BC\n","config env =  Acrobot-v1\n","Run 1 out of 10\n","state_dim 10\n","epoch 9000/10000, policy loss 8.065127872214362e-07\tepoch 0/100 return: -86.0\n","epoch 10/100 return: -91.0\n","epoch 20/100 return: -74.0\n","epoch 30/100 return: -72.0\n","epoch 40/100 return: -72.0\n","epoch 50/100 return: -83.0\n","epoch 60/100 return: -74.0\n","epoch 70/100 return: -93.0\n","epoch 80/100 return: -65.0\n","epoch 90/100 return: -82.0\n","###############    Reward for test environment for run 1: -93.17.   ###############\n","\n","\n","Run 2 out of 10\n","state_dim 10\n","epoch 9000/10000, policy loss 2.058438622043468e-05\tepoch 0/100 return: -78.0\n","epoch 10/100 return: -71.0\n","epoch 20/100 return: -85.0\n","epoch 30/100 return: -79.0\n","epoch 40/100 return: -93.0\n","epoch 50/100 return: -86.0\n","epoch 60/100 return: -77.0\n","epoch 70/100 return: -78.0\n","epoch 80/100 return: -69.0\n","epoch 90/100 return: -88.0\n","###############    Reward for test environment for run 2: -81.14.   ###############\n","\n","\n","Run 3 out of 10\n","state_dim 10\n","epoch 9000/10000, policy loss 5.024703568778932e-06\tepoch 0/100 return: -155.0\n","epoch 10/100 return: -86.0\n","epoch 20/100 return: -77.0\n","epoch 30/100 return: -78.0\n","epoch 40/100 return: -78.0\n","epoch 50/100 return: -72.0\n","epoch 60/100 return: -86.0\n","epoch 70/100 return: -73.0\n","epoch 80/100 return: -65.0\n","epoch 90/100 return: -78.0\n","###############    Reward for test environment for run 3: -86.46.   ###############\n","\n","\n","Run 4 out of 10\n","state_dim 10\n","epoch 9000/10000, policy loss 3.719539563462604e-06\tepoch 0/100 return: -95.0\n","epoch 10/100 return: -88.0\n","epoch 20/100 return: -86.0\n","epoch 30/100 return: -79.0\n","epoch 40/100 return: -77.0\n","epoch 50/100 return: -71.0\n","epoch 60/100 return: -77.0\n","epoch 70/100 return: -72.0\n","epoch 80/100 return: -95.0\n","epoch 90/100 return: -152.0\n","###############    Reward for test environment for run 4: -92.52.   ###############\n","\n","\n","Run 5 out of 10\n","state_dim 10\n","epoch 9000/10000, policy loss 9.303200386057142e-06\tepoch 0/100 return: -198.0\n","epoch 10/100 return: -128.0\n","epoch 20/100 return: -87.0\n","epoch 30/100 return: -91.0\n","epoch 40/100 return: -87.0\n","epoch 50/100 return: -65.0\n","epoch 60/100 return: -93.0\n","epoch 70/100 return: -91.0\n","epoch 80/100 return: -118.0\n","epoch 90/100 return: -72.0\n","###############    Reward for test environment for run 5: -93.04.   ###############\n","\n","\n","Run 6 out of 10\n","state_dim 10\n","epoch 9000/10000, policy loss 1.9967087609984446e-06\tepoch 0/100 return: -85.0\n","epoch 10/100 return: -86.0\n","epoch 20/100 return: -86.0\n","epoch 30/100 return: -71.0\n","epoch 40/100 return: -70.0\n","epoch 50/100 return: -86.0\n","epoch 60/100 return: -71.0\n","epoch 70/100 return: -87.0\n","epoch 80/100 return: -89.0\n","epoch 90/100 return: -71.0\n","###############    Reward for test environment for run 6: -82.33.   ###############\n","\n","\n","Run 7 out of 10\n","state_dim 10\n","epoch 9000/10000, policy loss 9.126716236096399e-07\tepoch 0/100 return: -81.0\n","epoch 10/100 return: -69.0\n","epoch 20/100 return: -64.0\n","epoch 30/100 return: -72.0\n","epoch 40/100 return: -88.0\n","epoch 50/100 return: -87.0\n","epoch 60/100 return: -77.0\n","epoch 70/100 return: -64.0\n","epoch 80/100 return: -78.0\n","epoch 90/100 return: -94.0\n","###############    Reward for test environment for run 7: -85.01.   ###############\n","\n","\n","Run 8 out of 10\n","state_dim 10\n","epoch 9000/10000, policy loss 5.699643566003942e-07\tepoch 0/100 return: -90.0\n","epoch 10/100 return: -78.0\n","epoch 20/100 return: -92.0\n","epoch 30/100 return: -71.0\n","epoch 40/100 return: -78.0\n","epoch 50/100 return: -71.0\n","epoch 60/100 return: -114.0\n","epoch 70/100 return: -78.0\n","epoch 80/100 return: -99.0\n","epoch 90/100 return: -97.0\n","###############    Reward for test environment for run 8: -82.62.   ###############\n","\n","\n","Run 9 out of 10\n","state_dim 10\n","epoch 9000/10000, policy loss 4.302676757106383e-07\tepoch 0/100 return: -72.0\n","epoch 10/100 return: -72.0\n","epoch 20/100 return: -79.0\n","epoch 30/100 return: -71.0\n","epoch 40/100 return: -73.0\n","epoch 50/100 return: -66.0\n","epoch 60/100 return: -102.0\n","epoch 70/100 return: -89.0\n","epoch 80/100 return: -79.0\n","epoch 90/100 return: -72.0\n","###############    Reward for test environment for run 9: -83.71.   ###############\n","\n","\n","Run 10 out of 10\n","state_dim 10\n","epoch 9000/10000, policy loss 7.096592753441655e-07\tepoch 0/100 return: -64.0\n","epoch 10/100 return: -75.0\n","epoch 20/100 return: -89.0\n","epoch 30/100 return: -96.0\n","epoch 40/100 return: -89.0\n","epoch 50/100 return: -72.0\n","epoch 60/100 return: -64.0\n","epoch 70/100 return: -72.0\n","epoch 80/100 return: -74.0\n","epoch 90/100 return: -102.0\n","###############    Reward for test environment for run 10: -83.81.   ###############\n","\n","\n","Average reward for 10 repetitions: -86.381\n","ALL RESULTS TRAIL: [-86.381]\n","Config: {'ENV': 'Acrobot-v1', 'ALG': 'FINAL_Apr24_BCStudent_origindata_trajnum8', 'NUM_TRAJS_GIVEN': 8, 'NUM_TRAINING_ENVS': 2, 'NOISE_DIM': 4, 'REP_SIZE': 16, 'TRAJ_SHIFT': 8, 'SAMPLING_RATE': 5, 'NUM_STEPS_TRAIN': 10000, 'NUM_TRAJS_VALID': 100, 'NUM_REPETITIONS': 10, 'BATCH_SIZE': 64, 'MLP_WIDTHS': 64, 'ADAM_ALPHA': 0.001, 'SGLD_BUFFER_SIZE': 10000, 'SGLD_LEARN_RATE': 0.01, 'SGLD_NOISE_COEF': 0.01, 'SGLD_NUM_STEPS': 100, 'SGLD_REINIT_FREQ': 0.05, 'NUM_STEPS_TRAIN_ENERGY_MODEL': 1000, 'TRIAL': 0, 'METHOD': 'BC', 'EXPERT_ALG': 'dqn'}\n","Trial number 0\n","config method =  BC\n","config env =  Acrobot-v1\n","Run 1 out of 10\n","state_dim 10\n","epoch 9000/10000, policy loss 9.90915395959746e-07\tepoch 0/100 return: -126.0\n","epoch 10/100 return: -64.0\n","epoch 20/100 return: -84.0\n","epoch 30/100 return: -72.0\n","epoch 40/100 return: -77.0\n","epoch 50/100 return: -81.0\n","epoch 60/100 return: -82.0\n","epoch 70/100 return: -89.0\n","epoch 80/100 return: -84.0\n","epoch 90/100 return: -89.0\n","###############    Reward for test environment for run 1: -92.65.   ###############\n","\n","\n","Run 2 out of 10\n","state_dim 10\n","epoch 9000/10000, policy loss 1.5236031458698562e-06\tepoch 0/100 return: -75.0\n","epoch 10/100 return: -86.0\n","epoch 20/100 return: -73.0\n","epoch 30/100 return: -84.0\n","epoch 40/100 return: -73.0\n","epoch 50/100 return: -89.0\n","epoch 60/100 return: -72.0\n","epoch 70/100 return: -73.0\n","epoch 80/100 return: -83.0\n","epoch 90/100 return: -81.0\n","###############    Reward for test environment for run 2: -84.51.   ###############\n","\n","\n","Run 3 out of 10\n","state_dim 10\n","epoch 9000/10000, policy loss 0.0006114878924563527\tepoch 0/100 return: -69.0\n","epoch 10/100 return: -72.0\n","epoch 20/100 return: -69.0\n","epoch 30/100 return: -110.0\n","epoch 40/100 return: -71.0\n","epoch 50/100 return: -77.0\n","epoch 60/100 return: -100.0\n","epoch 70/100 return: -76.0\n","epoch 80/100 return: -81.0\n","epoch 90/100 return: -77.0\n","###############    Reward for test environment for run 3: -82.24.   ###############\n","\n","\n","Run 4 out of 10\n","state_dim 10\n","epoch 9000/10000, policy loss 6.1744603954139166e-06\tepoch 0/100 return: -76.0\n","epoch 10/100 return: -69.0\n","epoch 20/100 return: -73.0\n","epoch 30/100 return: -135.0\n","epoch 40/100 return: -70.0\n","epoch 50/100 return: -71.0\n","epoch 60/100 return: -73.0\n","epoch 70/100 return: -72.0\n","epoch 80/100 return: -78.0\n","epoch 90/100 return: -91.0\n","###############    Reward for test environment for run 4: -80.65.   ###############\n","\n","\n","Run 5 out of 10\n","state_dim 10\n","epoch 9000/10000, policy loss 3.2856169127626345e-06\tepoch 0/100 return: -79.0\n","epoch 10/100 return: -78.0\n","epoch 20/100 return: -86.0\n","epoch 30/100 return: -77.0\n","epoch 40/100 return: -88.0\n","epoch 50/100 return: -78.0\n","epoch 60/100 return: -76.0\n","epoch 70/100 return: -71.0\n","epoch 80/100 return: -78.0\n","epoch 90/100 return: -71.0\n","###############    Reward for test environment for run 5: -85.4.   ###############\n","\n","\n","Run 6 out of 10\n","state_dim 10\n","epoch 9000/10000, policy loss 1.2181394595245365e-06\tepoch 0/100 return: -94.0\n","epoch 10/100 return: -81.0\n","epoch 20/100 return: -87.0\n","epoch 30/100 return: -72.0\n","epoch 40/100 return: -106.0\n","epoch 50/100 return: -74.0\n","epoch 60/100 return: -65.0\n","epoch 70/100 return: -86.0\n","epoch 80/100 return: -79.0\n","epoch 90/100 return: -78.0\n","###############    Reward for test environment for run 6: -82.37.   ###############\n","\n","\n","Run 7 out of 10\n","state_dim 10\n","epoch 9000/10000, policy loss 3.6469193673838163e-06\tepoch 0/100 return: -93.0\n","epoch 10/100 return: -69.0\n","epoch 20/100 return: -90.0\n","epoch 30/100 return: -69.0\n","epoch 40/100 return: -174.0\n","epoch 50/100 return: -80.0\n","epoch 60/100 return: -84.0\n","epoch 70/100 return: -84.0\n","epoch 80/100 return: -73.0\n","epoch 90/100 return: -77.0\n","###############    Reward for test environment for run 7: -83.22.   ###############\n","\n","\n","Run 8 out of 10\n","state_dim 10\n","epoch 9000/10000, policy loss 9.124352800427005e-06\tepoch 0/100 return: -83.0\n","epoch 10/100 return: -78.0\n","epoch 20/100 return: -98.0\n","epoch 30/100 return: -79.0\n","epoch 40/100 return: -77.0\n","epoch 50/100 return: -77.0\n","epoch 60/100 return: -78.0\n","epoch 70/100 return: -69.0\n","epoch 80/100 return: -90.0\n","epoch 90/100 return: -77.0\n","###############    Reward for test environment for run 8: -85.17.   ###############\n","\n","\n","Run 9 out of 10\n","state_dim 10\n","epoch 9000/10000, policy loss 1.8322549294680357e-05\tepoch 0/100 return: -77.0\n","epoch 10/100 return: -83.0\n","epoch 20/100 return: -96.0\n","epoch 30/100 return: -76.0\n","epoch 40/100 return: -78.0\n","epoch 50/100 return: -99.0\n","epoch 60/100 return: -99.0\n","epoch 70/100 return: -101.0\n","epoch 80/100 return: -93.0\n","epoch 90/100 return: -85.0\n","###############    Reward for test environment for run 9: -86.4.   ###############\n","\n","\n","Run 10 out of 10\n","state_dim 10\n","epoch 9000/10000, policy loss 1.2113839147787075e-05\tepoch 0/100 return: -84.0\n","epoch 10/100 return: -65.0\n","epoch 20/100 return: -80.0\n","epoch 30/100 return: -81.0\n","epoch 40/100 return: -80.0\n","epoch 50/100 return: -78.0\n","epoch 60/100 return: -71.0\n","epoch 70/100 return: -84.0\n","epoch 80/100 return: -71.0\n","epoch 90/100 return: -71.0\n","###############    Reward for test environment for run 10: -81.33.   ###############\n","\n","\n","Average reward for 10 repetitions: -84.394\n","ALL RESULTS TRAIL: [-84.394]\n","Config: {'ENV': 'Acrobot-v1', 'ALG': 'FINAL_Apr24_BCStudent_origindata_trajnum9', 'NUM_TRAJS_GIVEN': 9, 'NUM_TRAINING_ENVS': 2, 'NOISE_DIM': 4, 'REP_SIZE': 16, 'TRAJ_SHIFT': 9, 'SAMPLING_RATE': 5, 'NUM_STEPS_TRAIN': 10000, 'NUM_TRAJS_VALID': 100, 'NUM_REPETITIONS': 10, 'BATCH_SIZE': 64, 'MLP_WIDTHS': 64, 'ADAM_ALPHA': 0.001, 'SGLD_BUFFER_SIZE': 10000, 'SGLD_LEARN_RATE': 0.01, 'SGLD_NOISE_COEF': 0.01, 'SGLD_NUM_STEPS': 100, 'SGLD_REINIT_FREQ': 0.05, 'NUM_STEPS_TRAIN_ENERGY_MODEL': 1000, 'TRIAL': 0, 'METHOD': 'BC', 'EXPERT_ALG': 'dqn'}\n","Trial number 0\n","config method =  BC\n","config env =  Acrobot-v1\n","Run 1 out of 10\n","state_dim 10\n","epoch 9000/10000, policy loss 2.014513847825583e-05\tepoch 0/100 return: -87.0\n","epoch 10/100 return: -221.0\n","epoch 20/100 return: -73.0\n","epoch 30/100 return: -128.0\n","epoch 40/100 return: -96.0\n","epoch 50/100 return: -89.0\n","epoch 60/100 return: -90.0\n","epoch 70/100 return: -79.0\n","epoch 80/100 return: -64.0\n","epoch 90/100 return: -64.0\n","###############    Reward for test environment for run 1: -87.87.   ###############\n","\n","\n","Run 2 out of 10\n","state_dim 10\n","epoch 9000/10000, policy loss 9.111194231081754e-06\tepoch 0/100 return: -91.0\n","epoch 10/100 return: -183.0\n","epoch 20/100 return: -66.0\n","epoch 30/100 return: -89.0\n","epoch 40/100 return: -79.0\n","epoch 50/100 return: -66.0\n","epoch 60/100 return: -78.0\n","epoch 70/100 return: -73.0\n","epoch 80/100 return: -73.0\n","epoch 90/100 return: -82.0\n","###############    Reward for test environment for run 2: -84.9.   ###############\n","\n","\n","Run 3 out of 10\n","state_dim 10\n","epoch 9000/10000, policy loss 0.004904515575617552\tepoch 0/100 return: -82.0\n","epoch 10/100 return: -74.0\n","epoch 20/100 return: -73.0\n","epoch 30/100 return: -73.0\n","epoch 40/100 return: -82.0\n","epoch 50/100 return: -100.0\n","epoch 60/100 return: -93.0\n","epoch 70/100 return: -65.0\n","epoch 80/100 return: -66.0\n","epoch 90/100 return: -204.0\n","###############    Reward for test environment for run 3: -89.52.   ###############\n","\n","\n","Run 4 out of 10\n","state_dim 10\n","epoch 9000/10000, policy loss 3.7646444980055094e-05\tepoch 0/100 return: -73.0\n","epoch 10/100 return: -240.0\n","epoch 20/100 return: -77.0\n","epoch 30/100 return: -75.0\n","epoch 40/100 return: -74.0\n","epoch 50/100 return: -84.0\n","epoch 60/100 return: -77.0\n","epoch 70/100 return: -71.0\n","epoch 80/100 return: -500.0\n","epoch 90/100 return: -84.0\n","###############    Reward for test environment for run 4: -92.31.   ###############\n","\n","\n","Run 5 out of 10\n","state_dim 10\n","epoch 9000/10000, policy loss 1.0861642294912599e-05\tepoch 0/100 return: -77.0\n","epoch 10/100 return: -84.0\n","epoch 20/100 return: -85.0\n","epoch 30/100 return: -85.0\n","epoch 40/100 return: -105.0\n","epoch 50/100 return: -64.0\n","epoch 60/100 return: -76.0\n","epoch 70/100 return: -139.0\n","epoch 80/100 return: -72.0\n","epoch 90/100 return: -85.0\n","###############    Reward for test environment for run 5: -84.47.   ###############\n","\n","\n","Run 6 out of 10\n","state_dim 10\n","epoch 9000/10000, policy loss 1.272163217436173e-06\tepoch 0/100 return: -80.0\n","epoch 10/100 return: -75.0\n","epoch 20/100 return: -91.0\n","epoch 30/100 return: -93.0\n","epoch 40/100 return: -110.0\n","epoch 50/100 return: -106.0\n","epoch 60/100 return: -76.0\n","epoch 70/100 return: -82.0\n","epoch 80/100 return: -76.0\n","epoch 90/100 return: -126.0\n","###############    Reward for test environment for run 6: -88.14.   ###############\n","\n","\n","Run 7 out of 10\n","state_dim 10\n","epoch 9000/10000, policy loss 4.0152135625248775e-05\tepoch 0/100 return: -86.0\n","epoch 10/100 return: -70.0\n","epoch 20/100 return: -87.0\n","epoch 30/100 return: -85.0\n","epoch 40/100 return: -74.0\n","epoch 50/100 return: -90.0\n","epoch 60/100 return: -84.0\n","epoch 70/100 return: -72.0\n","epoch 80/100 return: -83.0\n","epoch 90/100 return: -74.0\n","###############    Reward for test environment for run 7: -83.7.   ###############\n","\n","\n","Run 8 out of 10\n","state_dim 10\n","epoch 9000/10000, policy loss 2.1236455722828396e-05\tepoch 0/100 return: -87.0\n","epoch 10/100 return: -76.0\n","epoch 20/100 return: -76.0\n","epoch 30/100 return: -96.0\n","epoch 40/100 return: -86.0\n","epoch 50/100 return: -85.0\n","epoch 60/100 return: -74.0\n","epoch 70/100 return: -93.0\n","epoch 80/100 return: -78.0\n","epoch 90/100 return: -85.0\n","###############    Reward for test environment for run 8: -87.67.   ###############\n","\n","\n","Run 9 out of 10\n","state_dim 10\n","epoch 9000/10000, policy loss 1.690507269813679e-05\tepoch 0/100 return: -84.0\n","epoch 10/100 return: -76.0\n","epoch 20/100 return: -84.0\n","epoch 30/100 return: -79.0\n","epoch 40/100 return: -71.0\n","epoch 50/100 return: -70.0\n","epoch 60/100 return: -85.0\n","epoch 70/100 return: -89.0\n","epoch 80/100 return: -84.0\n","epoch 90/100 return: -88.0\n","###############    Reward for test environment for run 9: -81.47.   ###############\n","\n","\n","Run 10 out of 10\n","state_dim 10\n","epoch 9000/10000, policy loss 3.576265328320005e-07\tepoch 0/100 return: -84.0\n","epoch 10/100 return: -85.0\n","epoch 20/100 return: -92.0\n","epoch 30/100 return: -78.0\n","epoch 40/100 return: -85.0\n","epoch 50/100 return: -78.0\n","epoch 60/100 return: -79.0\n","epoch 70/100 return: -79.0\n","epoch 80/100 return: -80.0\n","epoch 90/100 return: -99.0\n","###############    Reward for test environment for run 10: -87.74.   ###############\n","\n","\n","Average reward for 10 repetitions: -86.77900000000001\n","ALL RESULTS TRAIL: [-86.77900000000001]\n","Config: {'ENV': 'Acrobot-v1', 'ALG': 'FINAL_Apr24_BCStudent_origindata_trajnum10', 'NUM_TRAJS_GIVEN': 10, 'NUM_TRAINING_ENVS': 2, 'NOISE_DIM': 4, 'REP_SIZE': 16, 'TRAJ_SHIFT': 10, 'SAMPLING_RATE': 5, 'NUM_STEPS_TRAIN': 10000, 'NUM_TRAJS_VALID': 100, 'NUM_REPETITIONS': 10, 'BATCH_SIZE': 64, 'MLP_WIDTHS': 64, 'ADAM_ALPHA': 0.001, 'SGLD_BUFFER_SIZE': 10000, 'SGLD_LEARN_RATE': 0.01, 'SGLD_NOISE_COEF': 0.01, 'SGLD_NUM_STEPS': 100, 'SGLD_REINIT_FREQ': 0.05, 'NUM_STEPS_TRAIN_ENERGY_MODEL': 1000, 'TRIAL': 0, 'METHOD': 'BC', 'EXPERT_ALG': 'dqn'}\n","Trial number 0\n","config method =  BC\n","config env =  Acrobot-v1\n","Run 1 out of 10\n","state_dim 10\n","epoch 9000/10000, policy loss 6.129514986241702e-06\tepoch 0/100 return: -89.0\n","epoch 10/100 return: -79.0\n","epoch 20/100 return: -90.0\n","epoch 30/100 return: -81.0\n","epoch 40/100 return: -75.0\n","epoch 50/100 return: -72.0\n","epoch 60/100 return: -89.0\n","epoch 70/100 return: -96.0\n","epoch 80/100 return: -71.0\n","epoch 90/100 return: -86.0\n","###############    Reward for test environment for run 1: -86.28.   ###############\n","\n","\n","Run 2 out of 10\n","state_dim 10\n","epoch 9000/10000, policy loss 0.0006786718149669468\tepoch 0/100 return: -74.0\n","epoch 10/100 return: -73.0\n","epoch 20/100 return: -73.0\n","epoch 30/100 return: -73.0\n","epoch 40/100 return: -74.0\n","epoch 50/100 return: -72.0\n","epoch 60/100 return: -102.0\n","epoch 70/100 return: -246.0\n","epoch 80/100 return: -91.0\n","epoch 90/100 return: -73.0\n","###############    Reward for test environment for run 2: -81.86.   ###############\n","\n","\n","Run 3 out of 10\n","state_dim 10\n","epoch 9000/10000, policy loss 6.381235289154574e-05\tepoch 0/100 return: -119.0\n","epoch 10/100 return: -86.0\n","epoch 20/100 return: -80.0\n","epoch 30/100 return: -65.0\n","epoch 40/100 return: -71.0\n","epoch 50/100 return: -64.0\n","epoch 60/100 return: -89.0\n","epoch 70/100 return: -64.0\n","epoch 80/100 return: -79.0\n","epoch 90/100 return: -87.0\n","###############    Reward for test environment for run 3: -80.66.   ###############\n","\n","\n","Run 4 out of 10\n","state_dim 10\n","epoch 9000/10000, policy loss 0.00042749528074637055\tepoch 0/100 return: -70.0\n","epoch 10/100 return: -85.0\n","epoch 20/100 return: -86.0\n","epoch 30/100 return: -100.0\n","epoch 40/100 return: -66.0\n","epoch 50/100 return: -96.0\n","epoch 60/100 return: -92.0\n","epoch 70/100 return: -72.0\n","epoch 80/100 return: -77.0\n","epoch 90/100 return: -78.0\n","###############    Reward for test environment for run 4: -82.78.   ###############\n","\n","\n","Run 5 out of 10\n","state_dim 10\n","epoch 9000/10000, policy loss 4.659995738620637e-06\tepoch 0/100 return: -74.0\n","epoch 10/100 return: -72.0\n","epoch 20/100 return: -97.0\n","epoch 30/100 return: -82.0\n","epoch 40/100 return: -87.0\n","epoch 50/100 return: -74.0\n","epoch 60/100 return: -78.0\n","epoch 70/100 return: -95.0\n","epoch 80/100 return: -70.0\n","epoch 90/100 return: -74.0\n","###############    Reward for test environment for run 5: -92.25.   ###############\n","\n","\n","Run 6 out of 10\n","state_dim 10\n","epoch 9000/10000, policy loss 3.1402323656948283e-06\tepoch 0/100 return: -69.0\n","epoch 10/100 return: -80.0\n","epoch 20/100 return: -72.0\n","epoch 30/100 return: -80.0\n","epoch 40/100 return: -90.0\n","epoch 50/100 return: -72.0\n","epoch 60/100 return: -77.0\n","epoch 70/100 return: -78.0\n","epoch 80/100 return: -69.0\n","epoch 90/100 return: -73.0\n","###############    Reward for test environment for run 6: -82.45.   ###############\n","\n","\n","Run 7 out of 10\n","state_dim 10\n","epoch 9000/10000, policy loss 3.587133323890157e-05\tepoch 0/100 return: -64.0\n","epoch 10/100 return: -86.0\n","epoch 20/100 return: -72.0\n","epoch 30/100 return: -87.0\n","epoch 40/100 return: -78.0\n","epoch 50/100 return: -70.0\n","epoch 60/100 return: -72.0\n","epoch 70/100 return: -77.0\n","epoch 80/100 return: -94.0\n","epoch 90/100 return: -78.0\n","###############    Reward for test environment for run 7: -85.82.   ###############\n","\n","\n","Run 8 out of 10\n","state_dim 10\n","epoch 9000/10000, policy loss 3.369076512171887e-05\tepoch 0/100 return: -95.0\n","epoch 10/100 return: -87.0\n","epoch 20/100 return: -87.0\n","epoch 30/100 return: -77.0\n","epoch 40/100 return: -119.0\n","epoch 50/100 return: -76.0\n","epoch 60/100 return: -74.0\n","epoch 70/100 return: -73.0\n","epoch 80/100 return: -74.0\n","epoch 90/100 return: -64.0\n","###############    Reward for test environment for run 8: -85.68.   ###############\n","\n","\n","Run 9 out of 10\n","state_dim 10\n","epoch 9000/10000, policy loss 9.741522717376938e-07\tepoch 0/100 return: -85.0\n","epoch 10/100 return: -65.0\n","epoch 20/100 return: -123.0\n","epoch 30/100 return: -79.0\n","epoch 40/100 return: -72.0\n","epoch 50/100 return: -73.0\n","epoch 60/100 return: -86.0\n","epoch 70/100 return: -96.0\n","epoch 80/100 return: -71.0\n","epoch 90/100 return: -72.0\n","###############    Reward for test environment for run 9: -80.69.   ###############\n","\n","\n","Run 10 out of 10\n","state_dim 10\n","epoch 9000/10000, policy loss 0.0003075145068578422\tepoch 0/100 return: -84.0\n","epoch 10/100 return: -93.0\n","epoch 20/100 return: -71.0\n","epoch 30/100 return: -72.0\n","epoch 40/100 return: -234.0\n","epoch 50/100 return: -73.0\n","epoch 60/100 return: -104.0\n","epoch 70/100 return: -78.0\n","epoch 80/100 return: -78.0\n","epoch 90/100 return: -84.0\n","###############    Reward for test environment for run 10: -89.04.   ###############\n","\n","\n","Average reward for 10 repetitions: -84.751\n","ALL RESULTS TRAIL: [-84.751]\n","Config: {'ENV': 'Acrobot-v1', 'ALG': 'FINAL_Apr24_BCStudent_origindata_trajnum20', 'NUM_TRAJS_GIVEN': 20, 'NUM_TRAINING_ENVS': 2, 'NOISE_DIM': 4, 'REP_SIZE': 16, 'TRAJ_SHIFT': 20, 'SAMPLING_RATE': 5, 'NUM_STEPS_TRAIN': 10000, 'NUM_TRAJS_VALID': 100, 'NUM_REPETITIONS': 10, 'BATCH_SIZE': 64, 'MLP_WIDTHS': 64, 'ADAM_ALPHA': 0.001, 'SGLD_BUFFER_SIZE': 10000, 'SGLD_LEARN_RATE': 0.01, 'SGLD_NOISE_COEF': 0.01, 'SGLD_NUM_STEPS': 100, 'SGLD_REINIT_FREQ': 0.05, 'NUM_STEPS_TRAIN_ENERGY_MODEL': 1000, 'TRIAL': 0, 'METHOD': 'BC', 'EXPERT_ALG': 'dqn'}\n","Trial number 0\n","config method =  BC\n","config env =  Acrobot-v1\n","Run 1 out of 10\n","state_dim 10\n","epoch 9000/10000, policy loss 0.03395640105009079\tepoch 0/100 return: -92.0\n","epoch 10/100 return: -74.0\n","epoch 20/100 return: -72.0\n","epoch 30/100 return: -91.0\n","epoch 40/100 return: -79.0\n","epoch 50/100 return: -89.0\n","epoch 60/100 return: -91.0\n","epoch 70/100 return: -71.0\n","epoch 80/100 return: -87.0\n","epoch 90/100 return: -88.0\n","###############    Reward for test environment for run 1: -83.72.   ###############\n","\n","\n","Run 2 out of 10\n","state_dim 10\n","epoch 9000/10000, policy loss 0.02213914319872856\tepoch 0/100 return: -85.0\n","epoch 10/100 return: -85.0\n","epoch 20/100 return: -72.0\n","epoch 30/100 return: -74.0\n","epoch 40/100 return: -85.0\n","epoch 50/100 return: -95.0\n","epoch 60/100 return: -79.0\n","epoch 70/100 return: -74.0\n","epoch 80/100 return: -87.0\n","epoch 90/100 return: -71.0\n","###############    Reward for test environment for run 2: -80.79.   ###############\n","\n","\n","Run 3 out of 10\n","state_dim 10\n","epoch 9000/10000, policy loss 0.004596162121742964\tepoch 0/100 return: -74.0\n","epoch 10/100 return: -64.0\n","epoch 20/100 return: -86.0\n","epoch 30/100 return: -77.0\n","epoch 40/100 return: -87.0\n","epoch 50/100 return: -98.0\n","epoch 60/100 return: -108.0\n","epoch 70/100 return: -77.0\n","epoch 80/100 return: -86.0\n","epoch 90/100 return: -73.0\n","###############    Reward for test environment for run 3: -82.26.   ###############\n","\n","\n","Run 4 out of 10\n","state_dim 10\n","epoch 9000/10000, policy loss 0.00106696633156389\tepoch 0/100 return: -77.0\n","epoch 10/100 return: -82.0\n","epoch 20/100 return: -74.0\n","epoch 30/100 return: -85.0\n","epoch 40/100 return: -65.0\n","epoch 50/100 return: -84.0\n","epoch 60/100 return: -86.0\n","epoch 70/100 return: -65.0\n","epoch 80/100 return: -92.0\n","epoch 90/100 return: -80.0\n","###############    Reward for test environment for run 4: -83.09.   ###############\n","\n","\n","Run 5 out of 10\n","state_dim 10\n","epoch 9000/10000, policy loss 0.004724100232124329\tepoch 0/100 return: -84.0\n","epoch 10/100 return: -73.0\n","epoch 20/100 return: -72.0\n","epoch 30/100 return: -81.0\n","epoch 40/100 return: -87.0\n","epoch 50/100 return: -78.0\n","epoch 60/100 return: -70.0\n","epoch 70/100 return: -99.0\n","epoch 80/100 return: -78.0\n","epoch 90/100 return: -79.0\n","###############    Reward for test environment for run 5: -85.69.   ###############\n","\n","\n","Run 6 out of 10\n","state_dim 10\n","epoch 9000/10000, policy loss 0.07854098826646805\tepoch 0/100 return: -73.0\n","epoch 10/100 return: -72.0\n","epoch 20/100 return: -65.0\n","epoch 30/100 return: -76.0\n","epoch 40/100 return: -83.0\n","epoch 50/100 return: -222.0\n","epoch 60/100 return: -89.0\n","epoch 70/100 return: -73.0\n","epoch 80/100 return: -94.0\n","epoch 90/100 return: -77.0\n","###############    Reward for test environment for run 6: -86.96.   ###############\n","\n","\n","Run 7 out of 10\n","state_dim 10\n","epoch 9000/10000, policy loss 0.044812411069869995\tepoch 0/100 return: -69.0\n","epoch 10/100 return: -76.0\n","epoch 20/100 return: -64.0\n","epoch 30/100 return: -64.0\n","epoch 40/100 return: -78.0\n","epoch 50/100 return: -65.0\n","epoch 60/100 return: -94.0\n","epoch 70/100 return: -73.0\n","epoch 80/100 return: -76.0\n","epoch 90/100 return: -78.0\n","###############    Reward for test environment for run 7: -85.22.   ###############\n","\n","\n","Run 8 out of 10\n","state_dim 10\n","epoch 9000/10000, policy loss 0.04141126573085785\tepoch 0/100 return: -71.0\n","epoch 10/100 return: -87.0\n","epoch 20/100 return: -76.0\n","epoch 30/100 return: -78.0\n","epoch 40/100 return: -77.0\n","epoch 50/100 return: -83.0\n","epoch 60/100 return: -84.0\n","epoch 70/100 return: -78.0\n","epoch 80/100 return: -86.0\n","epoch 90/100 return: -73.0\n","###############    Reward for test environment for run 8: -86.42.   ###############\n","\n","\n","Run 9 out of 10\n","state_dim 10\n","epoch 9000/10000, policy loss 0.0030457109678536654\tepoch 0/100 return: -84.0\n","epoch 10/100 return: -71.0\n","epoch 20/100 return: -78.0\n","epoch 30/100 return: -95.0\n","epoch 40/100 return: -91.0\n","epoch 50/100 return: -77.0\n","epoch 60/100 return: -89.0\n","epoch 70/100 return: -80.0\n","epoch 80/100 return: -71.0\n","epoch 90/100 return: -77.0\n","###############    Reward for test environment for run 9: -82.27.   ###############\n","\n","\n","Run 10 out of 10\n","state_dim 10\n","epoch 9000/10000, policy loss 0.027601301670074463\tepoch 0/100 return: -113.0\n","epoch 10/100 return: -72.0\n","epoch 20/100 return: -65.0\n","epoch 30/100 return: -76.0\n","epoch 40/100 return: -92.0\n","epoch 50/100 return: -120.0\n","epoch 60/100 return: -69.0\n","epoch 70/100 return: -72.0\n","epoch 80/100 return: -75.0\n","epoch 90/100 return: -65.0\n","###############    Reward for test environment for run 10: -82.36.   ###############\n","\n","\n","Average reward for 10 repetitions: -83.878\n","ALL RESULTS TRAIL: [-83.878]\n"]}]},{"cell_type":"code","source":[""],"metadata":{"id":"RZsZZEezony5"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["#10 Trails -- BC -- lunarlander"],"metadata":{"id":"rmZ8HxHvooip"}},{"cell_type":"code","source":["config['METHOD'] = \"BC\"\n","config['ENV'] = 'LunarLander-v2'\n","\n","for traj_num in [2, 4, 8, 16, 32, 64, 128]:\n","    config[\"NUM_TRAJS_GIVEN\"] = traj_num\n","    config[\"TRAJ_SHIFT\"] = traj_num\n","\n","\n","    config['ALG'] = \"FINAL_Apr24_BCStudent_origindata_trajnum\" + str(traj_num)\n","\n","\n","    ###############.  settings   ###############\n","    #config['ALG'] = \"BCStudent_Apr19_replicatedata\"\n","    #config['METHOD'] = \"BC\"\n","    #config['ENV'] == \"CartPole-v1\"\n","    #config['ENV'] == \"LunarLander-v2\"\n","    #config[\"NUM_TRAJS_GIVEN\"] = 20\n","    #config[\"TRAJ_SHIFT\"] = 20\n","    ###############.  settings   ###############\n","\n","\n","\n","    if config['METHOD'] == 'BCIRM':\n","        config['l2_regularizer_weight'] = 0.001\n","        config['penalty_weight'] = 10000\n","        config['penalty_anneal_iters'] = 100\n","\n","    all_results_trail = []\n","\n","    for trail in range(1): \n","        config['TRIAL'] = trail \n","\n","\n","        ###############.  start a trail   ###############\n","\n","        config[\"EXPERT_ALG\"] = yaml.load(open(\"testing/config.yml\"), Loader=yaml.FullLoader)[config[\"ENV\"]]\n","        print(\"Config: %s\" % config)\n","\n","        TRIAL = config[\"TRIAL\"] #args.trial\n","        print(\"Trial number %s\" % TRIAL)\n","\n","        results_dir_base = \"testing/results/\"\n","        results_dir = os.path.join(results_dir_base, config[\"ENV\"], str(config[\"NUM_TRAJS_GIVEN\"]), config[\"ALG\"])\n","\n","        if not os.path.exists(results_dir):\n","            os.makedirs(results_dir)\n","\n","        config_file = \"trial_\" + str(TRIAL) + \"_\" + \"config.pkl\"\n","\n","        results_file_name = \"trial_\" + str(TRIAL) + \"_\" + \"results.csv\"\n","        results_file_path = os.path.join(results_dir, results_file_name)\n","\n","        if os.path.exists(os.path.join(results_dir, config_file)):\n","            raise NameError(\"CONFIG file already exists %s. Choose a different trial number.\" % config_file)\n","        pickle.dump(config, open(os.path.join(results_dir, config_file), \"wb\"))\n","\n","\n","\n","\n","        ###############.  10 runs for each trail   ###############\n","\n","        print(\"config method = \", config['METHOD'])\n","        print(\"config env = \", config['ENV'])\n","\n","        for run_seed in range(config[\"NUM_REPETITIONS\"]):\n","            print(\"Run %s out of %s\" % (run_seed + 1, config[\"NUM_REPETITIONS\"]))\n","            student = make_student(run_seed, config)\n","            student.train(num_updates=config[\"NUM_STEPS_TRAIN\"])\n","\n","            env_wrapper_out_of_sample = EnvWrapper(\n","                env=gym.make(config[\"ENV\"]), mult_factor=get_test_mult_factors(config['NOISE_DIM'] - 1), idx=3, seed=1\n","            )\n","            action_match, return_mean, return_std = student.test(\n","                num_episodes=config[\"NUM_TRAJS_VALID\"], env_wrapper=env_wrapper_out_of_sample\n","            )\n","\n","            result = (action_match, return_mean, return_std)\n","            print(\"###############    Reward for test environment for run %s: %s.   ###############\\n\\n\" % (run_seed + 1, return_mean))\n","            save_results(results_file_path, run_seed, action_match, return_mean, return_std)\n","\n","        results_trial = pd.read_csv(\n","            \"testing/results/\"\n","            + config[\"ENV\"]\n","            + \"/\"\n","            + str(config[\"NUM_TRAJS_GIVEN\"])\n","            + \"/\"\n","            + config[\"ALG\"]\n","            + \"/trial_\"\n","            + str(TRIAL)\n","            + \"_results.csv\",\n","            header=None,\n","        )\n","\n","        print(\"Average reward for 10 repetitions: %s\" % np.mean(results_trial[2].values))\n","\n","        all_results_trail.append(np.mean(results_trial[2].values))\n","\n","    print(\"ALL RESULTS TRAIL:\" , all_results_trail)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"outputId":"f14e2630-bea6-40ab-d2cf-44c357dd4aa9","id":"4iIFxpDPooiq","executionInfo":{"status":"ok","timestamp":1650804292975,"user_tz":240,"elapsed":7209594,"user":{"displayName":"3 Lin","userId":"16848054003967173878"}}},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Config: {'ENV': 'LunarLander-v2', 'ALG': 'FINAL_Apr24_BCStudent_origindata_trajnum2', 'NUM_TRAJS_GIVEN': 2, 'NUM_TRAINING_ENVS': 2, 'NOISE_DIM': 4, 'REP_SIZE': 16, 'TRAJ_SHIFT': 2, 'SAMPLING_RATE': 5, 'NUM_STEPS_TRAIN': 10000, 'NUM_TRAJS_VALID': 100, 'NUM_REPETITIONS': 10, 'BATCH_SIZE': 64, 'MLP_WIDTHS': 64, 'ADAM_ALPHA': 0.001, 'SGLD_BUFFER_SIZE': 10000, 'SGLD_LEARN_RATE': 0.01, 'SGLD_NOISE_COEF': 0.01, 'SGLD_NUM_STEPS': 100, 'SGLD_REINIT_FREQ': 0.05, 'NUM_STEPS_TRAIN_ENERGY_MODEL': 1000, 'TRIAL': 0, 'METHOD': 'BC', 'EXPERT_ALG': 'dqn'}\n","Trial number 0\n","config method =  BC\n","config env =  LunarLander-v2\n","Run 1 out of 10\n","state_dim 12\n","epoch 9000/10000, policy loss 2.9391889256658033e-05\tepoch 0/100 return: -100.63826799578536\n","epoch 10/100 return: 6.576321586781091\n","epoch 20/100 return: -124.27246946565998\n","epoch 30/100 return: -188.90966991996498\n","epoch 40/100 return: 288.5499325090474\n","epoch 50/100 return: 9.947108318762979\n","epoch 60/100 return: -233.91186490567563\n","epoch 70/100 return: 44.81597924949767\n","epoch 80/100 return: 11.998979650910684\n","epoch 90/100 return: -64.86438328094754\n","###############    Reward for test environment for run 1: -33.952220197244834.   ###############\n","\n","\n","Run 2 out of 10\n","state_dim 12\n","epoch 9000/10000, policy loss 9.215948011842556e-06\tepoch 0/100 return: -269.7904530472845\n","epoch 10/100 return: -258.23344764980806\n","epoch 20/100 return: -162.8034931202012\n","epoch 30/100 return: -347.18161330199547\n","epoch 40/100 return: 230.731336585664\n","epoch 50/100 return: -187.5792913165267\n","epoch 60/100 return: -400.56628795783877\n","epoch 70/100 return: -619.6434025497676\n","epoch 80/100 return: -551.7064921817071\n","epoch 90/100 return: -375.31110580290425\n","###############    Reward for test environment for run 2: -342.7125458958373.   ###############\n","\n","\n","Run 3 out of 10\n","state_dim 12\n","epoch 9000/10000, policy loss 0.0004793615953531116\tepoch 0/100 return: -560.533657379112\n","epoch 10/100 return: -501.17179523291867\n","epoch 20/100 return: -445.098107670921\n","epoch 30/100 return: -472.5934759557854\n","epoch 40/100 return: -619.8482577169203\n","epoch 50/100 return: -624.0650259933967\n","epoch 60/100 return: -500.15381023739843\n","epoch 70/100 return: -518.7896389414271\n","epoch 80/100 return: -522.0538293130028\n","epoch 90/100 return: -529.03886834623\n","###############    Reward for test environment for run 3: -503.5762918805151.   ###############\n","\n","\n","Run 4 out of 10\n","state_dim 12\n","epoch 9000/10000, policy loss 0.00032820491469465196\tepoch 0/100 return: -580.4004069598209\n","epoch 10/100 return: -53.60851063398497\n","epoch 20/100 return: -734.1581360555151\n","epoch 30/100 return: -475.52618311892417\n","epoch 40/100 return: -67.28308235612133\n","epoch 50/100 return: 0.40081540431117446\n","epoch 60/100 return: -398.10225374630664\n","epoch 70/100 return: -541.0999109712893\n","epoch 80/100 return: -585.9402490063258\n","epoch 90/100 return: -363.5462956167161\n","###############    Reward for test environment for run 4: -320.40903247001194.   ###############\n","\n","\n","Run 5 out of 10\n","state_dim 12\n","epoch 9000/10000, policy loss 0.14786465466022491\tepoch 0/100 return: 40.57791007032844\n","epoch 10/100 return: 39.24295727021848\n","epoch 20/100 return: -0.37840539331961054\n","epoch 30/100 return: 14.282158834770158\n","epoch 40/100 return: -66.76200390588727\n","epoch 50/100 return: -35.27045240627652\n","epoch 60/100 return: 225.6373062481286\n","epoch 70/100 return: 230.01341020344896\n","epoch 80/100 return: -1.2945947543947938\n","epoch 90/100 return: 21.7642687901036\n","###############    Reward for test environment for run 5: 31.546664513390628.   ###############\n","\n","\n","Run 6 out of 10\n","state_dim 12\n","epoch 9000/10000, policy loss 5.9294419770594686e-05\tepoch 0/100 return: -67.33963178438522\n","epoch 10/100 return: 167.1885543905822\n","epoch 20/100 return: -83.05841997597598\n","epoch 30/100 return: 4.263783872915894\n","epoch 40/100 return: -36.15663682899957\n","epoch 50/100 return: -16.534748390372528\n","epoch 60/100 return: 189.15776331087952\n","epoch 70/100 return: 5.993730745521248\n","epoch 80/100 return: -287.17989342161957\n","epoch 90/100 return: -26.700052017428774\n","###############    Reward for test environment for run 6: -23.767319424375337.   ###############\n","\n","\n","Run 7 out of 10\n","state_dim 12\n","epoch 9000/10000, policy loss 1.0711297363741323e-05\tepoch 0/100 return: -17.254898099819215\n","epoch 10/100 return: -59.83290969239066\n","epoch 20/100 return: 17.201437594202957\n","epoch 30/100 return: 9.949332911515384\n","epoch 40/100 return: -47.88949427496943\n","epoch 50/100 return: -85.26742855852575\n","epoch 60/100 return: -39.60842253537287\n","epoch 70/100 return: -85.37241692401473\n","epoch 80/100 return: 29.300997706907793\n","epoch 90/100 return: -178.2999571079389\n","###############    Reward for test environment for run 7: -52.95062749166294.   ###############\n","\n","\n","Run 8 out of 10\n","state_dim 12\n","epoch 9000/10000, policy loss 1.227984012075467e-05\tepoch 0/100 return: -692.3562471742008\n","epoch 10/100 return: -346.34682978780415\n","epoch 20/100 return: -539.4979494470952\n","epoch 30/100 return: -297.50505405137994\n","epoch 40/100 return: -111.52046698670081\n","epoch 50/100 return: -221.56253076311052\n","epoch 60/100 return: -463.53709409354366\n","epoch 70/100 return: -163.8216158715632\n","epoch 80/100 return: -385.71247674251043\n","epoch 90/100 return: -512.0679635768522\n","###############    Reward for test environment for run 8: -289.0353102856927.   ###############\n","\n","\n","Run 9 out of 10\n","state_dim 12\n","epoch 9000/10000, policy loss 1.612543928786181e-05\tepoch 0/100 return: -174.8944288831704\n","epoch 10/100 return: -94.06045795213261\n","epoch 20/100 return: -115.63102176780977\n","epoch 30/100 return: -73.67031340839476\n","epoch 40/100 return: -8.703491926385084\n","epoch 50/100 return: -261.8472457298051\n","epoch 60/100 return: -367.7882467917549\n","epoch 70/100 return: -337.5011906922392\n","epoch 80/100 return: -130.92401483710634\n","epoch 90/100 return: -346.8869658117811\n","###############    Reward for test environment for run 9: -150.3077323954461.   ###############\n","\n","\n","Run 10 out of 10\n","state_dim 12\n","epoch 9000/10000, policy loss 0.00028966981335543096\tepoch 0/100 return: -572.5291726247362\n","epoch 10/100 return: -587.6519008031636\n","epoch 20/100 return: -587.2696014972331\n","epoch 30/100 return: 290.3706701669323\n","epoch 40/100 return: -418.8711565801987\n","epoch 50/100 return: -435.3056119284686\n","epoch 60/100 return: -507.78303280216426\n","epoch 70/100 return: -512.7572242915871\n","epoch 80/100 return: -362.0320823539151\n","epoch 90/100 return: -528.3895334762121\n","###############    Reward for test environment for run 10: -421.5806817472853.   ###############\n","\n","\n","Average reward for 10 repetitions: -210.6745097274681\n","ALL RESULTS TRAIL: [-210.6745097274681]\n","Config: {'ENV': 'LunarLander-v2', 'ALG': 'FINAL_Apr24_BCStudent_origindata_trajnum4', 'NUM_TRAJS_GIVEN': 4, 'NUM_TRAINING_ENVS': 2, 'NOISE_DIM': 4, 'REP_SIZE': 16, 'TRAJ_SHIFT': 4, 'SAMPLING_RATE': 5, 'NUM_STEPS_TRAIN': 10000, 'NUM_TRAJS_VALID': 100, 'NUM_REPETITIONS': 10, 'BATCH_SIZE': 64, 'MLP_WIDTHS': 64, 'ADAM_ALPHA': 0.001, 'SGLD_BUFFER_SIZE': 10000, 'SGLD_LEARN_RATE': 0.01, 'SGLD_NOISE_COEF': 0.01, 'SGLD_NUM_STEPS': 100, 'SGLD_REINIT_FREQ': 0.05, 'NUM_STEPS_TRAIN_ENERGY_MODEL': 1000, 'TRIAL': 0, 'METHOD': 'BC', 'EXPERT_ALG': 'dqn'}\n","Trial number 0\n","config method =  BC\n","config env =  LunarLander-v2\n","Run 1 out of 10\n","state_dim 12\n","epoch 9000/10000, policy loss 0.0010581937385722995\tepoch 0/100 return: -173.77692646869224\n","epoch 10/100 return: 64.68421336861853\n","epoch 20/100 return: 38.52224029865263\n","epoch 30/100 return: -68.11555577888262\n","epoch 40/100 return: 12.143812996276267\n","epoch 50/100 return: 227.0109930684378\n","epoch 60/100 return: 239.4633677710324\n","epoch 70/100 return: -57.998446282055944\n","epoch 80/100 return: 7.785798039509629\n","epoch 90/100 return: -98.15139399035289\n","###############    Reward for test environment for run 1: 25.424837571274484.   ###############\n","\n","\n","Run 2 out of 10\n","state_dim 12\n","epoch 9000/10000, policy loss 0.027917690575122833\tepoch 0/100 return: -56.303430539364676\n","epoch 10/100 return: -602.6860129433264\n","epoch 20/100 return: -564.0210495350918\n","epoch 30/100 return: -501.5262298132426\n","epoch 40/100 return: -201.20461208139878\n","epoch 50/100 return: -379.60811271557264\n","epoch 60/100 return: -597.734952103033\n","epoch 70/100 return: -449.63040430746764\n","epoch 80/100 return: -360.6333946056768\n","epoch 90/100 return: -17.64065851261603\n","###############    Reward for test environment for run 2: -403.7249216806296.   ###############\n","\n","\n","Run 3 out of 10\n","state_dim 12\n","epoch 9000/10000, policy loss 0.0934872254729271\tepoch 0/100 return: 50.498819348189244\n","epoch 10/100 return: 264.3115668834454\n","epoch 20/100 return: 6.8459849642599835\n","epoch 30/100 return: 83.267773589883\n","epoch 40/100 return: 59.35739103055934\n","epoch 50/100 return: 91.44435250105761\n","epoch 60/100 return: 249.21135972670217\n","epoch 70/100 return: 247.41391173332815\n","epoch 80/100 return: -278.5798194390551\n","epoch 90/100 return: 240.92275205215506\n","###############    Reward for test environment for run 3: 108.35124202817569.   ###############\n","\n","\n","Run 4 out of 10\n","state_dim 12\n","epoch 9000/10000, policy loss 0.0029593491926789284\tepoch 0/100 return: 0.11719014658287108\n","epoch 10/100 return: 22.151228855809066\n","epoch 20/100 return: 265.6623740711503\n","epoch 30/100 return: -29.2289159804031\n","epoch 40/100 return: -200.95560050727795\n","epoch 50/100 return: -3.4345526363669023\n","epoch 60/100 return: 213.25964220508683\n","epoch 70/100 return: -259.1968378261554\n","epoch 80/100 return: -113.39991562442373\n","epoch 90/100 return: 29.670321446545017\n","###############    Reward for test environment for run 4: -79.70725860371593.   ###############\n","\n","\n","Run 5 out of 10\n","state_dim 12\n","epoch 9000/10000, policy loss 0.036934737116098404\tepoch 0/100 return: -368.11870970233343\n","epoch 10/100 return: -364.83393498513414\n","epoch 20/100 return: -366.74981826648144\n","epoch 30/100 return: -514.0337172675315\n","epoch 40/100 return: -99.46118354945443\n","epoch 50/100 return: -601.991583019369\n","epoch 60/100 return: -408.96943722103225\n","epoch 70/100 return: -25.736189240105915\n","epoch 80/100 return: -629.645301871525\n","epoch 90/100 return: 2.116481046284264\n","###############    Reward for test environment for run 5: -261.82246205028673.   ###############\n","\n","\n","Run 6 out of 10\n","state_dim 12\n","epoch 9000/10000, policy loss 0.0006976326694712043\tepoch 0/100 return: 5.032375677658521\n","epoch 10/100 return: -230.13738683020108\n","epoch 20/100 return: -148.62945277851125\n","epoch 30/100 return: 182.21561960611328\n","epoch 40/100 return: -198.65812303342614\n","epoch 50/100 return: 252.51050104809684\n","epoch 60/100 return: -103.11261239140815\n","epoch 70/100 return: -114.09973523800417\n","epoch 80/100 return: -291.75616896029794\n","epoch 90/100 return: -24.719082461632738\n","###############    Reward for test environment for run 6: -60.3107536738177.   ###############\n","\n","\n","Run 7 out of 10\n","state_dim 12\n","epoch 9000/10000, policy loss 0.0048116957768797874\tepoch 0/100 return: 13.086555383110579\n","epoch 10/100 return: 29.52984235865145\n","epoch 20/100 return: 254.4160289185961\n","epoch 30/100 return: -289.8337297526273\n","epoch 40/100 return: -17.514775438688538\n","epoch 50/100 return: -252.0926594798432\n","epoch 60/100 return: 78.97292433430852\n","epoch 70/100 return: 288.23935469292473\n","epoch 80/100 return: -239.1330702485582\n","epoch 90/100 return: 23.68299126178796\n","###############    Reward for test environment for run 7: -5.923836597099758.   ###############\n","\n","\n","Run 8 out of 10\n","state_dim 12\n","epoch 9000/10000, policy loss 0.0032883496023714542\tepoch 0/100 return: -139.93479075821605\n","epoch 10/100 return: 31.22161544167915\n","epoch 20/100 return: -157.0951877681357\n","epoch 30/100 return: -85.0050319351688\n","epoch 40/100 return: -191.3286204041425\n","epoch 50/100 return: -36.35502375152222\n","epoch 60/100 return: -8.755491633802848\n","epoch 70/100 return: 4.179679801098118\n","epoch 80/100 return: -310.0868500005946\n","epoch 90/100 return: -274.94912782367385\n","###############    Reward for test environment for run 8: -96.97690684585974.   ###############\n","\n","\n","Run 9 out of 10\n","state_dim 12\n","epoch 9000/10000, policy loss 0.08128681778907776\tepoch 0/100 return: 254.629451929801\n","epoch 10/100 return: 65.05604581502283\n","epoch 20/100 return: -52.6914378101536\n","epoch 30/100 return: -79.65794028828863\n","epoch 40/100 return: 52.16631737535738\n","epoch 50/100 return: -37.456115301619334\n","epoch 60/100 return: -217.42402622899564\n","epoch 70/100 return: 260.80543226189627\n","epoch 80/100 return: 237.5595439850692\n","epoch 90/100 return: -28.81716790933136\n","###############    Reward for test environment for run 9: 15.898402615165455.   ###############\n","\n","\n","Run 10 out of 10\n","state_dim 12\n","epoch 9000/10000, policy loss 0.00429023802280426\tepoch 0/100 return: 204.4835807693023\n","epoch 10/100 return: 3.824450896760311\n","epoch 20/100 return: -82.96929654239057\n","epoch 30/100 return: 6.05923040279535\n","epoch 40/100 return: -558.2792016729511\n","epoch 50/100 return: -573.2442933447386\n","epoch 60/100 return: -706.0092107766694\n","epoch 70/100 return: 27.30763149915195\n","epoch 80/100 return: 38.445470139315006\n","epoch 90/100 return: -2.639574686228869\n","###############    Reward for test environment for run 10: -6.466563901230898.   ###############\n","\n","\n","Average reward for 10 repetitions: -76.52582211380249\n","ALL RESULTS TRAIL: [-76.52582211380249]\n","Config: {'ENV': 'LunarLander-v2', 'ALG': 'FINAL_Apr24_BCStudent_origindata_trajnum8', 'NUM_TRAJS_GIVEN': 8, 'NUM_TRAINING_ENVS': 2, 'NOISE_DIM': 4, 'REP_SIZE': 16, 'TRAJ_SHIFT': 8, 'SAMPLING_RATE': 5, 'NUM_STEPS_TRAIN': 10000, 'NUM_TRAJS_VALID': 100, 'NUM_REPETITIONS': 10, 'BATCH_SIZE': 64, 'MLP_WIDTHS': 64, 'ADAM_ALPHA': 0.001, 'SGLD_BUFFER_SIZE': 10000, 'SGLD_LEARN_RATE': 0.01, 'SGLD_NOISE_COEF': 0.01, 'SGLD_NUM_STEPS': 100, 'SGLD_REINIT_FREQ': 0.05, 'NUM_STEPS_TRAIN_ENERGY_MODEL': 1000, 'TRIAL': 0, 'METHOD': 'BC', 'EXPERT_ALG': 'dqn'}\n","Trial number 0\n","config method =  BC\n","config env =  LunarLander-v2\n","Run 1 out of 10\n","state_dim 12\n","epoch 9000/10000, policy loss 0.004823072347790003\tepoch 0/100 return: 244.27243368251516\n","epoch 10/100 return: 50.49875479552682\n","epoch 20/100 return: 242.087259242521\n","epoch 30/100 return: -22.441166841553127\n","epoch 40/100 return: -118.86506594797025\n","epoch 50/100 return: 6.645038475139496\n","epoch 60/100 return: -159.33262074129226\n","epoch 70/100 return: 285.69658311116007\n","epoch 80/100 return: -84.9757534646526\n","epoch 90/100 return: 218.98523159728717\n","###############    Reward for test environment for run 1: 36.16149626885311.   ###############\n","\n","\n","Run 2 out of 10\n","state_dim 12\n","epoch 9000/10000, policy loss 0.1142287403345108\tepoch 0/100 return: 73.47517598433558\n","epoch 10/100 return: -16.937851478689367\n","epoch 20/100 return: 47.290413470286495\n","epoch 30/100 return: 44.58792882853885\n","epoch 40/100 return: 55.10680270051586\n","epoch 50/100 return: -186.64209023502121\n","epoch 60/100 return: 21.848681100101302\n","epoch 70/100 return: 42.6453319916629\n","epoch 80/100 return: 23.13378445422623\n","epoch 90/100 return: 18.35291320939436\n","###############    Reward for test environment for run 2: -22.502580478179933.   ###############\n","\n","\n","Run 3 out of 10\n","state_dim 12\n","epoch 9000/10000, policy loss 0.013093450106680393\tepoch 0/100 return: -510.31237708418405\n","epoch 10/100 return: -473.4751911815392\n","epoch 20/100 return: -558.2782681892908\n","epoch 30/100 return: -333.5755843080482\n","epoch 40/100 return: 244.44415155470978\n","epoch 50/100 return: -488.2716600511989\n","epoch 60/100 return: -521.733416298372\n","epoch 70/100 return: -233.4318667362108\n","epoch 80/100 return: -487.23202953169766\n","epoch 90/100 return: 272.06916955537787\n","###############    Reward for test environment for run 3: -125.04097552367752.   ###############\n","\n","\n","Run 4 out of 10\n","state_dim 12\n","epoch 9000/10000, policy loss 0.014403964392840862\tepoch 0/100 return: -201.40859493887004\n","epoch 10/100 return: -178.2757918414781\n","epoch 20/100 return: -146.1129813691967\n","epoch 30/100 return: 258.07067185873825\n","epoch 40/100 return: 121.32652633584293\n","epoch 50/100 return: -311.10169180236585\n","epoch 60/100 return: 240.7041313495858\n","epoch 70/100 return: 183.31917943447698\n","epoch 80/100 return: -152.9259284432609\n","epoch 90/100 return: 131.7611100366949\n","###############    Reward for test environment for run 4: 8.53796353787389.   ###############\n","\n","\n","Run 5 out of 10\n","state_dim 12\n","epoch 9000/10000, policy loss 0.07142074406147003\tepoch 0/100 return: 278.61245842525085\n","epoch 10/100 return: 231.20173781008188\n","epoch 20/100 return: 288.13427235683713\n","epoch 30/100 return: 267.40644448153137\n","epoch 40/100 return: 268.30760273187974\n","epoch 50/100 return: 156.2978953800339\n","epoch 60/100 return: 238.8970096413204\n","epoch 70/100 return: 201.18109021169522\n","epoch 80/100 return: -9.724979112611058\n","epoch 90/100 return: 195.54514793183264\n","###############    Reward for test environment for run 5: 202.86977967849143.   ###############\n","\n","\n","Run 6 out of 10\n","state_dim 12\n","epoch 9000/10000, policy loss 0.003317583817988634\tepoch 0/100 return: -244.94117409012694\n","epoch 10/100 return: 223.21732060753834\n","epoch 20/100 return: 208.59342140113955\n","epoch 30/100 return: 244.8024199976807\n","epoch 40/100 return: 246.19405659040987\n","epoch 50/100 return: 218.00864027485045\n","epoch 60/100 return: -40.03227136537464\n","epoch 70/100 return: -286.85204494010014\n","epoch 80/100 return: 245.02011426062845\n","epoch 90/100 return: 101.60643350940721\n","###############    Reward for test environment for run 6: 93.5748020011995.   ###############\n","\n","\n","Run 7 out of 10\n","state_dim 12\n","epoch 9000/10000, policy loss 0.12631063163280487\tepoch 0/100 return: 229.42260152387252\n","epoch 10/100 return: -402.39161648535094\n","epoch 20/100 return: 261.7646630235651\n","epoch 30/100 return: 163.9740573017682\n","epoch 40/100 return: -69.50789610675406\n","epoch 50/100 return: 124.26651400917952\n","epoch 60/100 return: 154.7130118471105\n","epoch 70/100 return: 168.50211788157765\n","epoch 80/100 return: -144.37345898320055\n","epoch 90/100 return: 197.34257963076635\n","###############    Reward for test environment for run 7: 72.1140425087574.   ###############\n","\n","\n","Run 8 out of 10\n","state_dim 12\n","epoch 9000/10000, policy loss 0.07284507155418396\tepoch 0/100 return: 288.8439055113655\n","epoch 10/100 return: 252.42068093923402\n","epoch 20/100 return: 245.75250065308896\n","epoch 30/100 return: 270.30740155613967\n","epoch 40/100 return: 268.2042887263019\n","epoch 50/100 return: 199.29863228142128\n","epoch 60/100 return: 250.77333363587033\n","epoch 70/100 return: 270.68669091478455\n","epoch 80/100 return: 251.08786777189783\n","epoch 90/100 return: 258.9067411071495\n","###############    Reward for test environment for run 8: 238.8248322824569.   ###############\n","\n","\n","Run 9 out of 10\n","state_dim 12\n","epoch 9000/10000, policy loss 0.05431949347257614\tepoch 0/100 return: 260.5317311055636\n","epoch 10/100 return: 210.87411293789015\n","epoch 20/100 return: -300.2442807959676\n","epoch 30/100 return: -346.8739731474158\n","epoch 40/100 return: -222.18670299185672\n","epoch 50/100 return: 134.58963173044066\n","epoch 60/100 return: -233.74120216201516\n","epoch 70/100 return: -121.00497239264436\n","epoch 80/100 return: 8.155368929657413\n","epoch 90/100 return: -173.48304991906218\n","###############    Reward for test environment for run 9: -68.06038641457492.   ###############\n","\n","\n","Run 10 out of 10\n","state_dim 12\n","epoch 9000/10000, policy loss 0.011003994382917881\tepoch 0/100 return: 282.1218787795067\n","epoch 10/100 return: 242.23802341680505\n","epoch 20/100 return: 236.42861442693453\n","epoch 30/100 return: 286.0077689084676\n","epoch 40/100 return: 35.11624019353235\n","epoch 50/100 return: -118.86081838751034\n","epoch 60/100 return: 258.9800765713039\n","epoch 70/100 return: 230.69914819798473\n","epoch 80/100 return: 161.06632089518368\n","epoch 90/100 return: -26.387467603002303\n","###############    Reward for test environment for run 10: 152.93986511980694.   ###############\n","\n","\n","Average reward for 10 repetitions: 58.941883898100684\n","ALL RESULTS TRAIL: [58.941883898100684]\n","Config: {'ENV': 'LunarLander-v2', 'ALG': 'FINAL_Apr24_BCStudent_origindata_trajnum16', 'NUM_TRAJS_GIVEN': 16, 'NUM_TRAINING_ENVS': 2, 'NOISE_DIM': 4, 'REP_SIZE': 16, 'TRAJ_SHIFT': 16, 'SAMPLING_RATE': 5, 'NUM_STEPS_TRAIN': 10000, 'NUM_TRAJS_VALID': 100, 'NUM_REPETITIONS': 10, 'BATCH_SIZE': 64, 'MLP_WIDTHS': 64, 'ADAM_ALPHA': 0.001, 'SGLD_BUFFER_SIZE': 10000, 'SGLD_LEARN_RATE': 0.01, 'SGLD_NOISE_COEF': 0.01, 'SGLD_NUM_STEPS': 100, 'SGLD_REINIT_FREQ': 0.05, 'NUM_STEPS_TRAIN_ENERGY_MODEL': 1000, 'TRIAL': 0, 'METHOD': 'BC', 'EXPERT_ALG': 'dqn'}\n","Trial number 0\n","config method =  BC\n","config env =  LunarLander-v2\n","Run 1 out of 10\n","state_dim 12\n","epoch 9000/10000, policy loss 0.19944974780082703\tepoch 0/100 return: 290.5264594396632\n","epoch 10/100 return: 7.824359328145562\n","epoch 20/100 return: 273.53211597416015\n","epoch 30/100 return: 262.9550141263661\n","epoch 40/100 return: 256.78379827868366\n","epoch 50/100 return: -11.906896984829302\n","epoch 60/100 return: 263.71977808475214\n","epoch 70/100 return: 276.2822629297577\n","epoch 80/100 return: 254.20760910026723\n","epoch 90/100 return: 227.8769541343899\n","###############    Reward for test environment for run 1: 220.57926034221973.   ###############\n","\n","\n","Run 2 out of 10\n","state_dim 12\n","epoch 9000/10000, policy loss 0.13960516452789307\tepoch 0/100 return: -28.892126952942768\n","epoch 10/100 return: -165.9843799186379\n","epoch 20/100 return: -34.22319588850831\n","epoch 30/100 return: 59.539185617545286\n","epoch 40/100 return: 183.2847681549427\n","epoch 50/100 return: -4.502466445774657\n","epoch 60/100 return: 45.59633755157823\n","epoch 70/100 return: 51.21812909081481\n","epoch 80/100 return: 212.05839021250182\n","epoch 90/100 return: -3.960547358102221\n","###############    Reward for test environment for run 2: 7.609320892365203.   ###############\n","\n","\n","Run 3 out of 10\n","state_dim 12\n","epoch 9000/10000, policy loss 0.14299283921718597\tepoch 0/100 return: 138.71688000456197\n","epoch 10/100 return: 258.62628200561255\n","epoch 20/100 return: 31.397308904350297\n","epoch 30/100 return: 278.1479651102189\n","epoch 40/100 return: 241.6280055821979\n","epoch 50/100 return: 29.142183102376947\n","epoch 60/100 return: 293.40001973451604\n","epoch 70/100 return: 249.49680950872335\n","epoch 80/100 return: 258.89265038822845\n","epoch 90/100 return: 262.23270913623605\n","###############    Reward for test environment for run 3: 227.1719529772146.   ###############\n","\n","\n","Run 4 out of 10\n","state_dim 12\n","epoch 9000/10000, policy loss 0.1056821346282959\tepoch 0/100 return: 223.76043702005933\n","epoch 10/100 return: -2.2820409427174866\n","epoch 20/100 return: -264.1253841614252\n","epoch 30/100 return: 227.43913967480364\n","epoch 40/100 return: 251.93832681313742\n","epoch 50/100 return: 52.631491604121855\n","epoch 60/100 return: 34.122379160507684\n","epoch 70/100 return: 69.70185048685946\n","epoch 80/100 return: 21.409864870684643\n","epoch 90/100 return: 54.798967943009515\n","###############    Reward for test environment for run 4: 15.478220262450535.   ###############\n","\n","\n","Run 5 out of 10\n","state_dim 12\n","epoch 9000/10000, policy loss 0.0973404049873352\tepoch 0/100 return: 243.19769392961106\n","epoch 10/100 return: 249.51291756096873\n","epoch 20/100 return: 27.104167041827807\n","epoch 30/100 return: 264.2949720050036\n","epoch 40/100 return: 37.469246422650116\n","epoch 50/100 return: 270.25844333869935\n","epoch 60/100 return: 259.04988539534145\n","epoch 70/100 return: -54.35071634470079\n","epoch 80/100 return: 2.0979996974568422\n","epoch 90/100 return: 209.60773279814435\n","###############    Reward for test environment for run 5: 143.6835853469063.   ###############\n","\n","\n","Run 6 out of 10\n","state_dim 12\n","epoch 9000/10000, policy loss 0.184468075633049\tepoch 0/100 return: 253.1929985621254\n","epoch 10/100 return: 134.0740832491836\n","epoch 20/100 return: 219.22259950159327\n","epoch 30/100 return: 268.1955788555574\n","epoch 40/100 return: 283.73705903159487\n","epoch 50/100 return: 285.0560773332909\n","epoch 60/100 return: 260.46279659035315\n","epoch 70/100 return: 230.41333747447237\n","epoch 80/100 return: 272.4742288504657\n","epoch 90/100 return: 259.9780298897084\n","###############    Reward for test environment for run 6: 205.59486070274224.   ###############\n","\n","\n","Run 7 out of 10\n","state_dim 12\n","epoch 9000/10000, policy loss 0.18157660961151123\tepoch 0/100 return: 278.8706058850761\n","epoch 10/100 return: 307.436769139063\n","epoch 20/100 return: 41.45477642674277\n","epoch 30/100 return: 258.55496639105877\n","epoch 40/100 return: 280.8572759623818\n","epoch 50/100 return: 286.02001057001905\n","epoch 60/100 return: 261.046444512146\n","epoch 70/100 return: 249.83429307143768\n","epoch 80/100 return: 247.08938275542928\n","epoch 90/100 return: 297.69187936545984\n","###############    Reward for test environment for run 7: 221.77801498163416.   ###############\n","\n","\n","Run 8 out of 10\n","state_dim 12\n","epoch 9000/10000, policy loss 0.3194527328014374\tepoch 0/100 return: 259.7018142748685\n","epoch 10/100 return: -65.37958582273123\n","epoch 20/100 return: -67.24984703465609\n","epoch 30/100 return: 87.51291481603035\n","epoch 40/100 return: 268.8351665537085\n","epoch 50/100 return: 280.7116041184106\n","epoch 60/100 return: -60.75043075427452\n","epoch 70/100 return: 277.6618571244668\n","epoch 80/100 return: 242.42523792295782\n","epoch 90/100 return: 10.21970024996605\n","###############    Reward for test environment for run 8: 148.67918386176763.   ###############\n","\n","\n","Run 9 out of 10\n","state_dim 12\n","epoch 9000/10000, policy loss 0.10296608507633209\tepoch 0/100 return: 238.78498729866902\n","epoch 10/100 return: 255.20994515933742\n","epoch 20/100 return: 217.09663791257051\n","epoch 30/100 return: 261.587189994015\n","epoch 40/100 return: 145.20818442981061\n","epoch 50/100 return: 247.01662202109017\n","epoch 60/100 return: 280.40884553370216\n","epoch 70/100 return: 253.84229481115577\n","epoch 80/100 return: 271.7670778747645\n","epoch 90/100 return: 265.56138249427056\n","###############    Reward for test environment for run 9: 204.97476901659186.   ###############\n","\n","\n","Run 10 out of 10\n","state_dim 12\n","epoch 9000/10000, policy loss 0.12948644161224365\tepoch 0/100 return: 264.9923195494315\n","epoch 10/100 return: -485.9813647119707\n","epoch 20/100 return: 264.4182719918166\n","epoch 30/100 return: 239.80510747342757\n","epoch 40/100 return: -242.86740054560454\n","epoch 50/100 return: 34.241963406632046\n","epoch 60/100 return: 5.556704749039497\n","epoch 70/100 return: -842.1639059712552\n","epoch 80/100 return: 28.048418466480303\n","epoch 90/100 return: 39.2403719767999\n","###############    Reward for test environment for run 10: -87.60155832535656.   ###############\n","\n","\n","Average reward for 10 repetitions: 130.79476100585356\n","ALL RESULTS TRAIL: [130.79476100585356]\n","Config: {'ENV': 'LunarLander-v2', 'ALG': 'FINAL_Apr24_BCStudent_origindata_trajnum32', 'NUM_TRAJS_GIVEN': 32, 'NUM_TRAINING_ENVS': 2, 'NOISE_DIM': 4, 'REP_SIZE': 16, 'TRAJ_SHIFT': 32, 'SAMPLING_RATE': 5, 'NUM_STEPS_TRAIN': 10000, 'NUM_TRAJS_VALID': 100, 'NUM_REPETITIONS': 10, 'BATCH_SIZE': 64, 'MLP_WIDTHS': 64, 'ADAM_ALPHA': 0.001, 'SGLD_BUFFER_SIZE': 10000, 'SGLD_LEARN_RATE': 0.01, 'SGLD_NOISE_COEF': 0.01, 'SGLD_NUM_STEPS': 100, 'SGLD_REINIT_FREQ': 0.05, 'NUM_STEPS_TRAIN_ENERGY_MODEL': 1000, 'TRIAL': 0, 'METHOD': 'BC', 'EXPERT_ALG': 'dqn'}\n","Trial number 0\n","config method =  BC\n","config env =  LunarLander-v2\n","Run 1 out of 10\n","state_dim 12\n","epoch 9000/10000, policy loss 0.4222559928894043\tepoch 0/100 return: -155.76487170557786\n","epoch 10/100 return: 137.0847506310245\n","epoch 20/100 return: 149.6127963033374\n","epoch 30/100 return: 32.06627558041007\n","epoch 40/100 return: 198.58160097786018\n","epoch 50/100 return: -412.0558843842564\n","epoch 60/100 return: 166.50344472157144\n","epoch 70/100 return: 235.6134774840716\n","epoch 80/100 return: 227.04677755500313\n","epoch 90/100 return: 219.68465504983033\n","###############    Reward for test environment for run 1: 131.7143916359595.   ###############\n","\n","\n","Run 2 out of 10\n","state_dim 12\n","epoch 9000/10000, policy loss 0.2374640703201294\tepoch 0/100 return: 266.0349316136868\n","epoch 10/100 return: 273.3857395153259\n","epoch 20/100 return: 253.61763270504116\n","epoch 30/100 return: 283.13107693084464\n","epoch 40/100 return: 272.9940076063282\n","epoch 50/100 return: 230.29063725998517\n","epoch 60/100 return: 230.39449024621476\n","epoch 70/100 return: 271.6518406166646\n","epoch 80/100 return: 257.52679170057337\n","epoch 90/100 return: 260.2292332099729\n","###############    Reward for test environment for run 2: 250.21572711305893.   ###############\n","\n","\n","Run 3 out of 10\n","state_dim 12\n","epoch 9000/10000, policy loss 0.19092002511024475\tepoch 0/100 return: 252.3711764355605\n","epoch 10/100 return: -92.06010798615395\n","epoch 20/100 return: 270.41619606372086\n","epoch 30/100 return: 261.0685274796586\n","epoch 40/100 return: 249.76843577504474\n","epoch 50/100 return: 227.83310496470415\n","epoch 60/100 return: 239.53989632871884\n","epoch 70/100 return: 241.71185808116013\n","epoch 80/100 return: 242.3044884098946\n","epoch 90/100 return: 256.73149854837493\n","###############    Reward for test environment for run 3: 198.47642514028155.   ###############\n","\n","\n","Run 4 out of 10\n","state_dim 12\n","epoch 9000/10000, policy loss 0.37849435210227966\tepoch 0/100 return: 190.22909050570232\n","epoch 10/100 return: 261.11215997378025\n","epoch 20/100 return: 112.54214891481753\n","epoch 30/100 return: 262.414569762256\n","epoch 40/100 return: 83.09433658031067\n","epoch 50/100 return: 220.1393127617608\n","epoch 60/100 return: 276.52141687773474\n","epoch 70/100 return: 45.06592816511161\n","epoch 80/100 return: 231.58992286493873\n","epoch 90/100 return: -348.66995730312254\n","###############    Reward for test environment for run 4: 178.34237779033825.   ###############\n","\n","\n","Run 5 out of 10\n","state_dim 12\n","epoch 9000/10000, policy loss 0.12208851426839828\tepoch 0/100 return: 45.107922624983736\n","epoch 10/100 return: 133.39966396270478\n","epoch 20/100 return: 255.83135302036882\n","epoch 30/100 return: 183.31896352045163\n","epoch 40/100 return: 50.337155166783134\n","epoch 50/100 return: 7.743760691312553\n","epoch 60/100 return: 80.29612830303446\n","epoch 70/100 return: -182.9887635788443\n","epoch 80/100 return: -173.8760160249188\n","epoch 90/100 return: 248.8971405824014\n","###############    Reward for test environment for run 5: 85.68579910523712.   ###############\n","\n","\n","Run 6 out of 10\n","state_dim 12\n","epoch 9000/10000, policy loss 0.32676661014556885\tepoch 0/100 return: 29.89096037237522\n","epoch 10/100 return: 256.4590899449845\n","epoch 20/100 return: 293.6168475995496\n","epoch 30/100 return: -24.358460099692202\n","epoch 40/100 return: 267.0732452368137\n","epoch 50/100 return: 57.80226814837599\n","epoch 60/100 return: 252.74947492392636\n","epoch 70/100 return: 263.2789281175725\n","epoch 80/100 return: 261.7876587205041\n","epoch 90/100 return: 285.1267818709488\n","###############    Reward for test environment for run 6: 211.84484890779896.   ###############\n","\n","\n","Run 7 out of 10\n","state_dim 12\n","epoch 9000/10000, policy loss 0.18297359347343445\tepoch 0/100 return: 277.46891789004366\n","epoch 10/100 return: 239.1506987326902\n","epoch 20/100 return: 241.9195846209427\n","epoch 30/100 return: 236.65844001515734\n","epoch 40/100 return: 228.22244314149188\n","epoch 50/100 return: 267.38348986272524\n","epoch 60/100 return: 276.4933207439866\n","epoch 70/100 return: 8.671271423610264\n","epoch 80/100 return: 176.22895262062576\n","epoch 90/100 return: 253.67159155920095\n","###############    Reward for test environment for run 7: 179.92356600513136.   ###############\n","\n","\n","Run 8 out of 10\n","state_dim 12\n","epoch 9000/10000, policy loss 0.4301561117172241\tepoch 0/100 return: 208.55440936509083\n","epoch 10/100 return: 236.67292089984156\n","epoch 20/100 return: 250.5333512296485\n","epoch 30/100 return: 234.70487910761457\n","epoch 40/100 return: 281.5765548279365\n","epoch 50/100 return: 205.35299697371437\n","epoch 60/100 return: 233.1031378838901\n","epoch 70/100 return: 234.08082289330113\n","epoch 80/100 return: 196.19623884562498\n","epoch 90/100 return: 213.59033099570772\n","###############    Reward for test environment for run 8: 198.35034332925062.   ###############\n","\n","\n","Run 9 out of 10\n","state_dim 12\n","epoch 9000/10000, policy loss 0.32963040471076965\tepoch 0/100 return: 236.9929707463646\n","epoch 10/100 return: 243.42160885258025\n","epoch 20/100 return: 255.45888516098262\n","epoch 30/100 return: 253.95896599853367\n","epoch 40/100 return: 279.2866699182275\n","epoch 50/100 return: 273.3650901767321\n","epoch 60/100 return: 286.15507225809597\n","epoch 70/100 return: 252.98022636994284\n","epoch 80/100 return: 259.09123339610613\n","epoch 90/100 return: 279.8156987209721\n","###############    Reward for test environment for run 9: 240.98073033444908.   ###############\n","\n","\n","Run 10 out of 10\n","state_dim 12\n","epoch 9000/10000, policy loss 0.2503676414489746\tepoch 0/100 return: 287.6916392646812\n","epoch 10/100 return: 278.54533022410465\n","epoch 20/100 return: 273.3349484669608\n","epoch 30/100 return: 271.7291191089591\n","epoch 40/100 return: 225.27372267164256\n","epoch 50/100 return: 281.2293434082361\n","epoch 60/100 return: 268.5864601294078\n","epoch 70/100 return: 275.96208417935145\n","epoch 80/100 return: -382.3662430919472\n","epoch 90/100 return: 255.5058580834317\n","###############    Reward for test environment for run 10: 212.6191297957568.   ###############\n","\n","\n","Average reward for 10 repetitions: 188.81533391572623\n","ALL RESULTS TRAIL: [188.81533391572623]\n","Config: {'ENV': 'LunarLander-v2', 'ALG': 'FINAL_Apr24_BCStudent_origindata_trajnum64', 'NUM_TRAJS_GIVEN': 64, 'NUM_TRAINING_ENVS': 2, 'NOISE_DIM': 4, 'REP_SIZE': 16, 'TRAJ_SHIFT': 64, 'SAMPLING_RATE': 5, 'NUM_STEPS_TRAIN': 10000, 'NUM_TRAJS_VALID': 100, 'NUM_REPETITIONS': 10, 'BATCH_SIZE': 64, 'MLP_WIDTHS': 64, 'ADAM_ALPHA': 0.001, 'SGLD_BUFFER_SIZE': 10000, 'SGLD_LEARN_RATE': 0.01, 'SGLD_NOISE_COEF': 0.01, 'SGLD_NUM_STEPS': 100, 'SGLD_REINIT_FREQ': 0.05, 'NUM_STEPS_TRAIN_ENERGY_MODEL': 1000, 'TRIAL': 0, 'METHOD': 'BC', 'EXPERT_ALG': 'dqn'}\n","Trial number 0\n","config method =  BC\n","config env =  LunarLander-v2\n","Run 1 out of 10\n","state_dim 12\n","epoch 9000/10000, policy loss 0.42239031195640564\tepoch 0/100 return: 249.0774503377151\n","epoch 10/100 return: 277.1291044366557\n","epoch 20/100 return: 293.5915661660875\n","epoch 30/100 return: 224.71598371326976\n","epoch 40/100 return: 272.06949745172733\n","epoch 50/100 return: 276.44327662235435\n","epoch 60/100 return: 266.528422681334\n","epoch 70/100 return: 293.07785495368245\n","epoch 80/100 return: 259.51236958685286\n","epoch 90/100 return: 245.07809353218062\n","###############    Reward for test environment for run 1: 239.51550157219413.   ###############\n","\n","\n","Run 2 out of 10\n","state_dim 12\n","epoch 9000/10000, policy loss 0.37126630544662476\tepoch 0/100 return: 280.69302887401034\n","epoch 10/100 return: 206.03507679882404\n","epoch 20/100 return: 205.71669978255892\n","epoch 30/100 return: 230.20252028444196\n","epoch 40/100 return: 267.7465390563184\n","epoch 50/100 return: 250.2286551071411\n","epoch 60/100 return: 269.4424711721672\n","epoch 70/100 return: 276.90717349636714\n","epoch 80/100 return: 230.83340546899083\n","epoch 90/100 return: 240.3132413967807\n","###############    Reward for test environment for run 2: 253.76703652715187.   ###############\n","\n","\n","Run 3 out of 10\n","state_dim 12\n","epoch 9000/10000, policy loss 0.2892029583454132\tepoch 0/100 return: 238.21857204007125\n","epoch 10/100 return: -49.124144800707796\n","epoch 20/100 return: 282.84989196898215\n","epoch 30/100 return: 283.48936226401224\n","epoch 40/100 return: 279.96922640012076\n","epoch 50/100 return: 255.86535241299904\n","epoch 60/100 return: 289.19550173288997\n","epoch 70/100 return: 283.50811449375556\n","epoch 80/100 return: 253.40415399935975\n","epoch 90/100 return: 213.3851892405559\n","###############    Reward for test environment for run 3: 233.72083705868732.   ###############\n","\n","\n","Run 4 out of 10\n","state_dim 12\n","epoch 9000/10000, policy loss 0.504173755645752\tepoch 0/100 return: 248.45808406308143\n","epoch 10/100 return: 268.9090036042833\n","epoch 20/100 return: 249.4156552716077\n","epoch 30/100 return: 230.45854128500486\n","epoch 40/100 return: 277.7304501527583\n","epoch 50/100 return: 287.84681233391365\n","epoch 60/100 return: 242.6725473874157\n","epoch 70/100 return: 222.58184406704157\n","epoch 80/100 return: 282.61133432537997\n","epoch 90/100 return: 257.8319712333497\n","###############    Reward for test environment for run 4: 230.1650917575699.   ###############\n","\n","\n","Run 5 out of 10\n","state_dim 12\n","epoch 9000/10000, policy loss 0.41490304470062256\tepoch 0/100 return: 253.01994797365603\n","epoch 10/100 return: 291.40302643562825\n","epoch 20/100 return: -360.67414537806746\n","epoch 30/100 return: -20.559646037747363\n","epoch 40/100 return: 73.02922685213484\n","epoch 50/100 return: 262.72099318275923\n","epoch 60/100 return: 183.43987566811109\n","epoch 70/100 return: -11.18376834380644\n","epoch 80/100 return: 130.71356112057595\n","epoch 90/100 return: 9.365744911512866\n","###############    Reward for test environment for run 5: 75.75222365372043.   ###############\n","\n","\n","Run 6 out of 10\n","state_dim 12\n","epoch 9000/10000, policy loss 0.30921322107315063\tepoch 0/100 return: 237.17258939077837\n","epoch 10/100 return: 265.6929121238625\n","epoch 20/100 return: 279.8019331723476\n","epoch 30/100 return: 260.8181635378614\n","epoch 40/100 return: 248.76953358726706\n","epoch 50/100 return: 265.4634459925675\n","epoch 60/100 return: 264.37955999697317\n","epoch 70/100 return: 273.5017266960946\n","epoch 80/100 return: 238.3637395401275\n","epoch 90/100 return: 28.59571164573549\n","###############    Reward for test environment for run 6: 216.05873812209546.   ###############\n","\n","\n","Run 7 out of 10\n","state_dim 12\n","epoch 9000/10000, policy loss 0.271788626909256\tepoch 0/100 return: 251.13614412617306\n","epoch 10/100 return: 281.78969591237455\n","epoch 20/100 return: 254.0873790051149\n","epoch 30/100 return: 259.4068873385741\n","epoch 40/100 return: 28.730059885620364\n","epoch 50/100 return: 262.33886482322487\n","epoch 60/100 return: 243.12075054798447\n","epoch 70/100 return: 129.89952800644562\n","epoch 80/100 return: 272.2314003628563\n","epoch 90/100 return: 275.7036361547441\n","###############    Reward for test environment for run 7: 230.21577687757363.   ###############\n","\n","\n","Run 8 out of 10\n","state_dim 12\n","epoch 9000/10000, policy loss 0.33917272090911865\tepoch 0/100 return: 270.9129904420166\n","epoch 10/100 return: 104.19076701829867\n","epoch 20/100 return: 237.2619843481946\n","epoch 30/100 return: 241.72039922705133\n","epoch 40/100 return: 243.25235555736563\n","epoch 50/100 return: 244.6866137212403\n","epoch 60/100 return: 302.9029711259685\n","epoch 70/100 return: 298.85392147594496\n","epoch 80/100 return: 1.0566960703935564\n","epoch 90/100 return: 262.35647880291384\n","###############    Reward for test environment for run 8: 233.1810558362118.   ###############\n","\n","\n","Run 9 out of 10\n","state_dim 12\n","epoch 9000/10000, policy loss 0.40389302372932434\tepoch 0/100 return: 40.28167069388389\n","epoch 10/100 return: 131.6777192551157\n","epoch 20/100 return: 38.41789937518349\n","epoch 30/100 return: 3.193736690681783\n","epoch 40/100 return: 71.39768519400425\n","epoch 50/100 return: 52.29168959239741\n","epoch 60/100 return: 196.73912942754805\n","epoch 70/100 return: 112.2096447265866\n","epoch 80/100 return: 118.4535439296651\n","epoch 90/100 return: 41.14705628678807\n","###############    Reward for test environment for run 9: 84.92631737353673.   ###############\n","\n","\n","Run 10 out of 10\n","state_dim 12\n","epoch 9000/10000, policy loss 0.4691040515899658\tepoch 0/100 return: -48.614166447309785\n","epoch 10/100 return: 20.399071936708243\n","epoch 20/100 return: 41.2146206155635\n","epoch 30/100 return: -11.704334641306573\n","epoch 40/100 return: 21.274303376178704\n","epoch 50/100 return: -11.961815757726102\n","epoch 60/100 return: 236.80456944075007\n","epoch 70/100 return: -28.75724100738507\n","epoch 80/100 return: -2.0614081602365495\n","epoch 90/100 return: -627.1738919106209\n","###############    Reward for test environment for run 10: -51.180845010514695.   ###############\n","\n","\n","Average reward for 10 repetitions: 174.61217337682265\n","ALL RESULTS TRAIL: [174.61217337682265]\n","Config: {'ENV': 'LunarLander-v2', 'ALG': 'FINAL_Apr24_BCStudent_origindata_trajnum128', 'NUM_TRAJS_GIVEN': 128, 'NUM_TRAINING_ENVS': 2, 'NOISE_DIM': 4, 'REP_SIZE': 16, 'TRAJ_SHIFT': 128, 'SAMPLING_RATE': 5, 'NUM_STEPS_TRAIN': 10000, 'NUM_TRAJS_VALID': 100, 'NUM_REPETITIONS': 10, 'BATCH_SIZE': 64, 'MLP_WIDTHS': 64, 'ADAM_ALPHA': 0.001, 'SGLD_BUFFER_SIZE': 10000, 'SGLD_LEARN_RATE': 0.01, 'SGLD_NOISE_COEF': 0.01, 'SGLD_NUM_STEPS': 100, 'SGLD_REINIT_FREQ': 0.05, 'NUM_STEPS_TRAIN_ENERGY_MODEL': 1000, 'TRIAL': 0, 'METHOD': 'BC', 'EXPERT_ALG': 'dqn'}\n","Trial number 0\n","config method =  BC\n","config env =  LunarLander-v2\n","Run 1 out of 10\n","state_dim 12\n","epoch 9000/10000, policy loss 0.549355685710907\tepoch 0/100 return: 136.15321083175633\n","epoch 10/100 return: 66.04244110527537\n","epoch 20/100 return: 249.12333822763082\n","epoch 30/100 return: 207.13308577205612\n","epoch 40/100 return: 34.849508512878316\n","epoch 50/100 return: 172.01996767095474\n","epoch 60/100 return: 66.63617843098743\n","epoch 70/100 return: 154.74571478748825\n","epoch 80/100 return: 57.349768786597245\n","epoch 90/100 return: 219.19873750009936\n","###############    Reward for test environment for run 1: 145.12937683006714.   ###############\n","\n","\n","Run 2 out of 10\n","state_dim 12\n","epoch 9000/10000, policy loss 0.4310382008552551\tepoch 0/100 return: 151.0878469594328\n","epoch 10/100 return: 156.60721310984832\n","epoch 20/100 return: 286.73059908460925\n","epoch 30/100 return: 286.0941692205222\n","epoch 40/100 return: 233.61894358062358\n","epoch 50/100 return: 234.5827075860774\n","epoch 60/100 return: 233.50525875677397\n","epoch 70/100 return: 260.4925262368175\n","epoch 80/100 return: 290.0961364670844\n","epoch 90/100 return: 235.57218741910822\n","###############    Reward for test environment for run 2: 235.50367612596494.   ###############\n","\n","\n","Run 3 out of 10\n","state_dim 12\n","epoch 9000/10000, policy loss 0.5446106791496277\tepoch 0/100 return: 153.61308891190487\n","epoch 10/100 return: 26.351013318343163\n","epoch 20/100 return: -37.99963525301222\n","epoch 30/100 return: 42.522579395930364\n","epoch 40/100 return: -1.1635560951386044\n","epoch 50/100 return: -5.1683635019208\n","epoch 60/100 return: 51.30409719373071\n","epoch 70/100 return: 276.06355732158863\n","epoch 80/100 return: 0.5789615690159629\n","epoch 90/100 return: 3.434737893968415\n","###############    Reward for test environment for run 3: 40.70920481443519.   ###############\n","\n","\n","Run 4 out of 10\n","state_dim 12\n","epoch 9000/10000, policy loss 0.5764119625091553\tepoch 0/100 return: 245.15320593335946\n","epoch 10/100 return: 306.2863839875712\n","epoch 20/100 return: 293.8206083150744\n","epoch 30/100 return: 280.11756880059306\n","epoch 40/100 return: 268.3560354951767\n","epoch 50/100 return: 262.8374968660536\n","epoch 60/100 return: 160.97538815966226\n","epoch 70/100 return: 288.732689912812\n","epoch 80/100 return: 256.80860514505355\n","epoch 90/100 return: 242.38842371529597\n","###############    Reward for test environment for run 4: 256.83013774741096.   ###############\n","\n","\n","Run 5 out of 10\n","state_dim 12\n","epoch 9000/10000, policy loss 0.3440484404563904\tepoch 0/100 return: 277.7962701927304\n","epoch 10/100 return: 289.4792994206804\n","epoch 20/100 return: 247.45566088248458\n","epoch 30/100 return: -208.81286166627848\n","epoch 40/100 return: 269.6336426408693\n","epoch 50/100 return: 259.0500762481114\n","epoch 60/100 return: 268.91947107132734\n","epoch 70/100 return: 260.827585595141\n","epoch 80/100 return: 200.898160745784\n","epoch 90/100 return: 234.8464883738604\n","###############    Reward for test environment for run 5: 213.58230136448665.   ###############\n","\n","\n","Run 6 out of 10\n","state_dim 12\n","epoch 9000/10000, policy loss 0.3483850955963135\tepoch 0/100 return: 62.23014198160444\n","epoch 10/100 return: 55.53737068945525\n","epoch 20/100 return: 75.43705727235645\n","epoch 30/100 return: -222.8604532318408\n","epoch 40/100 return: 66.9159477552617\n","epoch 50/100 return: 52.355870714388054\n","epoch 60/100 return: 82.8795493918432\n","epoch 70/100 return: -50.14521491513409\n","epoch 80/100 return: 24.171257198448174\n","epoch 90/100 return: 81.93863771490636\n","###############    Reward for test environment for run 6: 27.553278922388444.   ###############\n","\n","\n","Run 7 out of 10\n","state_dim 12\n","epoch 9000/10000, policy loss 0.31249770522117615\tepoch 0/100 return: -258.60431529523987\n","epoch 10/100 return: 300.7754386207307\n","epoch 20/100 return: 273.6653497068834\n","epoch 30/100 return: 301.76224172943546\n","epoch 40/100 return: 273.5213207197759\n","epoch 50/100 return: 263.0764969392874\n","epoch 60/100 return: 277.718145864544\n","epoch 70/100 return: 187.04145708055012\n","epoch 80/100 return: 272.7930792049778\n","epoch 90/100 return: 266.0266876488457\n","###############    Reward for test environment for run 7: 191.1651829576539.   ###############\n","\n","\n","Run 8 out of 10\n","state_dim 12\n","epoch 9000/10000, policy loss 0.5403038263320923\tepoch 0/100 return: 273.5684583243792\n","epoch 10/100 return: 294.91290159929827\n","epoch 20/100 return: 278.4615654312835\n","epoch 30/100 return: 232.17424655611086\n","epoch 40/100 return: -175.99485182924076\n","epoch 50/100 return: 256.1493947741236\n","epoch 60/100 return: 203.1681704787498\n","epoch 70/100 return: 273.48775272874775\n","epoch 80/100 return: 279.5103775085497\n","epoch 90/100 return: 270.22207729812106\n","###############    Reward for test environment for run 8: 209.7491853261268.   ###############\n","\n","\n","Run 9 out of 10\n","state_dim 12\n","epoch 9000/10000, policy loss 0.45766106247901917\tepoch 0/100 return: 275.5752961281088\n","epoch 10/100 return: 211.49692748478586\n","epoch 20/100 return: 231.9847321342411\n","epoch 30/100 return: 284.13236908065477\n","epoch 40/100 return: 283.83428205482255\n","epoch 50/100 return: 280.62948041741697\n","epoch 60/100 return: 224.66209773112362\n","epoch 70/100 return: 286.2556950359108\n","epoch 80/100 return: 51.466734928353446\n","epoch 90/100 return: 248.5098230169045\n","###############    Reward for test environment for run 9: 237.18152216870186.   ###############\n","\n","\n","Run 10 out of 10\n","state_dim 12\n","epoch 9000/10000, policy loss 0.43060386180877686\tepoch 0/100 return: 243.76972080560893\n","epoch 10/100 return: 238.034850395121\n","epoch 20/100 return: 294.86425458955705\n","epoch 30/100 return: 275.5407306761996\n","epoch 40/100 return: 229.12862324742355\n","epoch 50/100 return: 238.86105471484197\n","epoch 60/100 return: 253.02741211363096\n","epoch 70/100 return: 162.4470047142579\n","epoch 80/100 return: 270.5217889649221\n","epoch 90/100 return: 243.07361877717867\n","###############    Reward for test environment for run 10: 226.38038799604016.   ###############\n","\n","\n","Average reward for 10 repetitions: 178.3784254253276\n","ALL RESULTS TRAIL: [178.3784254253276]\n"]}]},{"cell_type":"code","source":[""],"metadata":{"id":"K1KNb0-Dooiq"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["#10 Trails -- ICIL"],"metadata":{"id":"IfNC7HRN2v10"}},{"cell_type":"code","source":["config['METHOD'] = \"ICIL\"\n","\n","for traj_num in [16, 32, 64, 128]:\n","    config[\"NUM_TRAJS_GIVEN\"] = traj_num\n","    config[\"TRAJ_SHIFT\"] = traj_num\n","\n","\n","    config['ALG'] = \"FINAL_ICILStudent_replicatedata_trajnum\" + str(traj_num)\n","\n","\n","    ###############.  settings   ###############\n","    #config['ALG'] = \"ICILStudent_Apr19_replicatedata\"\n","    #config['METHOD'] = \"ICIL\"\n","    #config['ENV'] == \"CartPole-v1\"\n","    #config['ENV'] == \"LunarLander-v2\"\n","\n","    ###############.  settings   ###############\n","\n","\n","\n","    if config['METHOD'] == 'BCIRM':\n","        config['l2_regularizer_weight'] = 0.001\n","        config['penalty_weight'] = 10000\n","        config['penalty_anneal_iters'] = 100\n","\n","    all_results_trail = []\n","\n","    for trail in range(1): \n","        config['TRIAL'] = trail \n","\n","\n","        ###############.  start a trail   ###############\n","\n","        config[\"EXPERT_ALG\"] = yaml.load(open(\"testing/config.yml\"), Loader=yaml.FullLoader)[config[\"ENV\"]]\n","        print(\"Config: %s\" % config)\n","\n","        TRIAL = config[\"TRIAL\"] #args.trial\n","        print(\"Trial number %s\" % TRIAL)\n","\n","        results_dir_base = \"testing/results/\"\n","        results_dir = os.path.join(results_dir_base, config[\"ENV\"], str(config[\"NUM_TRAJS_GIVEN\"]), config[\"ALG\"])\n","\n","        if not os.path.exists(results_dir):\n","            os.makedirs(results_dir)\n","\n","        config_file = \"trial_\" + str(TRIAL) + \"_\" + \"config.pkl\"\n","\n","        results_file_name = \"trial_\" + str(TRIAL) + \"_\" + \"results.csv\"\n","        results_file_path = os.path.join(results_dir, results_file_name)\n","\n","        if os.path.exists(os.path.join(results_dir, config_file)):\n","            raise NameError(\"CONFIG file already exists %s. Choose a different trial number.\" % config_file)\n","        pickle.dump(config, open(os.path.join(results_dir, config_file), \"wb\"))\n","\n","\n","\n","\n","        ###############.  10 runs for each trail   ###############\n","\n","        print(\"config method = \", config['METHOD'])\n","        print(\"config env = \", config['ENV'])\n","\n","        for run_seed in range(config[\"NUM_REPETITIONS\"]):\n","            print(\"Run %s out of %s\" % (run_seed + 1, config[\"NUM_REPETITIONS\"]))\n","            student = make_student(run_seed, config)\n","            student.train(num_updates=config[\"NUM_STEPS_TRAIN\"])\n","\n","            env_wrapper_out_of_sample = EnvWrapper(\n","                env=gym.make(config[\"ENV\"]), mult_factor=get_test_mult_factors(config['NOISE_DIM'] - 1), idx=3, seed=1\n","            )\n","            action_match, return_mean, return_std = student.test(\n","                num_episodes=config[\"NUM_TRAJS_VALID\"], env_wrapper=env_wrapper_out_of_sample\n","            )\n","\n","            result = (action_match, return_mean, return_std)\n","            print(\"###############    Reward for test environment for run %s: %s.   ###############\\n\\n\" % (run_seed + 1, return_mean))\n","            save_results(results_file_path, run_seed, action_match, return_mean, return_std)\n","\n","        results_trial = pd.read_csv(\n","            \"testing/results/\"\n","            + config[\"ENV\"]\n","            + \"/\"\n","            + str(config[\"NUM_TRAJS_GIVEN\"])\n","            + \"/\"\n","            + config[\"ALG\"]\n","            + \"/trial_\"\n","            + str(TRIAL)\n","            + \"_results.csv\",\n","            header=None,\n","        )\n","\n","        print(\"Average reward for 10 repetitions: %s\" % np.mean(results_trial[2].values))\n","\n","        all_results_trail.append(np.mean(results_trial[2].values))\n","\n","    print(\"ALL RESULTS TRAIL:\" , all_results_trail)"],"metadata":{"id":"Ll-j1IJX2v10"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["#line by line"],"metadata":{"id":"HYrmsDz4fppf"}},{"cell_type":"code","source":["\n","config[\"EXPERT_ALG\"] = yaml.load(open(\"testing/config.yml\"), Loader=yaml.FullLoader)[config[\"ENV\"]]\n","print(\"Config: %s\" % config)\n","\n","\n","TRIAL = config[\"TRIAL\"] #args.trial\n","print(\"Trial number %s\" % TRIAL)\n","\n","\n","results_dir_base = \"testing/results/\"\n","results_dir = os.path.join(results_dir_base, config[\"ENV\"], str(config[\"NUM_TRAJS_GIVEN\"]), config[\"ALG\"])\n","\n","\n","if not os.path.exists(results_dir):\n","    os.makedirs(results_dir)\n","\n","\n","\n","config_file = \"trial_\" + str(TRIAL) + \"_\" + \"config.pkl\"\n","\n","results_file_name = \"trial_\" + str(TRIAL) + \"_\" + \"results.csv\"\n","results_file_path = os.path.join(results_dir, results_file_name)\n","\n","if os.path.exists(os.path.join(results_dir, config_file)):\n","    raise NameError(\"CONFIG file already exists %s. Choose a different trial number.\" % config_file)\n","pickle.dump(config, open(os.path.join(results_dir, config_file), \"wb\"))\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"oNBpmZwuK37S","executionInfo":{"status":"ok","timestamp":1650186548391,"user_tz":240,"elapsed":7,"user":{"displayName":"3 Lin","userId":"16848054003967173878"}},"outputId":"fa68db12-273b-44f3-91c2-19a1eeb07e90"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Config: {'ENV': 'CartPole-v1', 'ALG': 'ICILStudent', 'NUM_TRAJS_GIVEN': 10, 'NUM_TRAINING_ENVS': 2, 'NOISE_DIM': 4, 'REP_SIZE': 16, 'TRAJ_SHIFT': 20, 'SAMPLING_RATE': 5, 'NUM_STEPS_TRAIN': 10000, 'NUM_TRAJS_VALID': 100, 'NUM_REPETITIONS': 10, 'BATCH_SIZE': 64, 'MLP_WIDTHS': 64, 'ADAM_ALPHA': 0.001, 'SGLD_BUFFER_SIZE': 10000, 'SGLD_LEARN_RATE': 0.01, 'SGLD_NOISE_COEF': 0.01, 'SGLD_NUM_STEPS': 100, 'SGLD_REINIT_FREQ': 0.05, 'NUM_STEPS_TRAIN_ENERGY_MODEL': 1000, 'TRIAL': 8, 'METHOD': 'BC', 'EXPERT_ALG': 'dqn'}\n","Trial number 8\n"]}]},{"cell_type":"code","source":[""],"metadata":{"id":"XQWBUA1Auay0"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["\n","#\"\"\"\n","if __name__ == \"__main__\":\n","   \n","    print(\"config method = \", config['METHOD'])\n","    print(\"config env = \", config['ENV'])\n","\n","    for run_seed in range(config[\"NUM_REPETITIONS\"]):\n","        print(\"Run %s out of %s\" % (run_seed + 1, config[\"NUM_REPETITIONS\"]))\n","        student = make_student(run_seed, config)\n","        student.train(num_updates=config[\"NUM_STEPS_TRAIN\"])\n","\n","        env_wrapper_out_of_sample = EnvWrapper(\n","            env=gym.make(config[\"ENV\"]), mult_factor=get_test_mult_factors(config['NOISE_DIM'] - 1), idx=3, seed=1\n","        )\n","        action_match, return_mean, return_std = student.test(\n","            num_episodes=config[\"NUM_TRAJS_VALID\"], env_wrapper=env_wrapper_out_of_sample\n","        )\n","\n","        result = (action_match, return_mean, return_std)\n","        print(\"###############    Reward for test environment for run %s: %s.   ###############\\n\\n\" % (run_seed + 1, return_mean))\n","        save_results(results_file_path, run_seed, action_match, return_mean, return_std)\n","\n","    results_trial = pd.read_csv(\n","        \"testing/results/\"\n","        + config[\"ENV\"]\n","        + \"/\"\n","        + str(config[\"NUM_TRAJS_GIVEN\"])\n","        + \"/\"\n","        + config[\"ALG\"]\n","        + \"/trial_\"\n","        + str(TRIAL)\n","        + \"_results.csv\",\n","        header=None,\n","    )\n","\n","    print(\"Average reward for 10 repetitions: %s\" % np.mean(results_trial[2].values))\n","#\"\"\""],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"f6cyeiGzmJoC","executionInfo":{"status":"ok","timestamp":1650187197981,"user_tz":240,"elapsed":649594,"user":{"displayName":"3 Lin","userId":"16848054003967173878"}},"outputId":"247cc407-4314-4d64-aa55-f1b729788ad3"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["config method =  BC\n","config env =  CartPole-v1\n","Run 1 out of 10\n","0\n","1000\n","2000\n","3000\n","4000\n","5000\n","6000\n","7000\n","8000\n","9000\n","###############    Reward for test environment for run 1: 166.1.   ###############\n","\n","\n","Run 2 out of 10\n","0\n","1000\n","2000\n","3000\n","4000\n","5000\n","6000\n","7000\n","8000\n","9000\n","###############    Reward for test environment for run 2: 155.74.   ###############\n","\n","\n","Run 3 out of 10\n","0\n","1000\n","2000\n","3000\n","4000\n","5000\n","6000\n","7000\n","8000\n","9000\n","###############    Reward for test environment for run 3: 496.46.   ###############\n","\n","\n","Run 4 out of 10\n","0\n","1000\n","2000\n","3000\n","4000\n","5000\n","6000\n","7000\n","8000\n","9000\n","###############    Reward for test environment for run 4: 500.0.   ###############\n","\n","\n","Run 5 out of 10\n","0\n","1000\n","2000\n","3000\n","4000\n","5000\n","6000\n","7000\n","8000\n","9000\n","###############    Reward for test environment for run 5: 500.0.   ###############\n","\n","\n","Run 6 out of 10\n","0\n","1000\n","2000\n","3000\n","4000\n","5000\n","6000\n","7000\n","8000\n","9000\n","###############    Reward for test environment for run 6: 274.88.   ###############\n","\n","\n","Run 7 out of 10\n","0\n","1000\n","2000\n","3000\n","4000\n","5000\n","6000\n","7000\n","8000\n","9000\n","###############    Reward for test environment for run 7: 144.6.   ###############\n","\n","\n","Run 8 out of 10\n","0\n","1000\n","2000\n","3000\n","4000\n","5000\n","6000\n","7000\n","8000\n","9000\n","###############    Reward for test environment for run 8: 250.22.   ###############\n","\n","\n","Run 9 out of 10\n","0\n","1000\n","2000\n","3000\n","4000\n","5000\n","6000\n","7000\n","8000\n","9000\n","###############    Reward for test environment for run 9: 355.71.   ###############\n","\n","\n","Run 10 out of 10\n","0\n","1000\n","2000\n","3000\n","4000\n","5000\n","6000\n","7000\n","8000\n","9000\n","###############    Reward for test environment for run 10: 500.0.   ###############\n","\n","\n","Average reward for 10 repetitions: 334.371\n"]}]},{"cell_type":"code","source":["\n","#\"\"\"\n","if __name__ == \"__main__\":\n","   \n","    print(\"config method = \", config['METHOD'])\n","    print(\"config env = \", config['ENV'])\n","\n","    for run_seed in range(config[\"NUM_REPETITIONS\"]):\n","        print(\"Run %s out of %s\" % (run_seed + 1, config[\"NUM_REPETITIONS\"]))\n","        student = make_student(run_seed, config)\n","        student.train(num_updates=config[\"NUM_STEPS_TRAIN\"])\n","\n","        env_wrapper_out_of_sample = EnvWrapper(\n","            env=gym.make(config[\"ENV\"]), mult_factor=get_test_mult_factors(config['NOISE_DIM'] - 1), idx=3, seed=1\n","        )\n","        action_match, return_mean, return_std = student.test(\n","            num_episodes=config[\"NUM_TRAJS_VALID\"], env_wrapper=env_wrapper_out_of_sample\n","        )\n","\n","        result = (action_match, return_mean, return_std)\n","        print(\"###############    Reward for test environment for run %s: %s.   ###############\\n\\n\" % (run_seed + 1, return_mean))\n","        save_results(results_file_path, run_seed, action_match, return_mean, return_std)\n","\n","    results_trial = pd.read_csv(\n","        \"testing/results/\"\n","        + config[\"ENV\"]\n","        + \"/\"\n","        + str(config[\"NUM_TRAJS_GIVEN\"])\n","        + \"/\"\n","        + config[\"ALG\"]\n","        + \"/trial_\"\n","        + str(TRIAL)\n","        + \"_results.csv\",\n","        header=None,\n","    )\n","\n","    print(\"Average reward for 10 repetitions: %s\" % np.mean(results_trial[2].values))\n","#\"\"\""],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"B1BnY-X9Csx_","outputId":"c34d8215-be58-4fa6-9629-a3d1e0d97477","executionInfo":{"status":"ok","timestamp":1650186161762,"user_tz":240,"elapsed":1260902,"user":{"displayName":"3 Lin","userId":"16848054003967173878"}}},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["config method =  BCIRM\n","config env =  CartPole-v1\n","Run 1 out of 10\n","200 tensor(0.4755, device='cuda:0', grad_fn=<AddBackward0>)\n","400 tensor(0.4851, device='cuda:0', grad_fn=<AddBackward0>)\n","600 tensor(0.4670, device='cuda:0', grad_fn=<AddBackward0>)\n","800 tensor(0.4648, device='cuda:0', grad_fn=<AddBackward0>)\n","1000 tensor(0.4254, device='cuda:0', grad_fn=<AddBackward0>)\n","1200 tensor(0.5061, device='cuda:0', grad_fn=<AddBackward0>)\n","1400 tensor(0.5847, device='cuda:0', grad_fn=<AddBackward0>)\n","1600 tensor(0.4992, device='cuda:0', grad_fn=<AddBackward0>)\n","1800 tensor(0.3855, device='cuda:0', grad_fn=<AddBackward0>)\n","2000 tensor(0.4569, device='cuda:0', grad_fn=<AddBackward0>)\n","2200 tensor(0.5494, device='cuda:0', grad_fn=<AddBackward0>)\n","2400 tensor(0.3457, device='cuda:0', grad_fn=<AddBackward0>)\n","2600 tensor(0.4519, device='cuda:0', grad_fn=<AddBackward0>)\n","2800 tensor(0.3749, device='cuda:0', grad_fn=<AddBackward0>)\n","3000 tensor(0.5179, device='cuda:0', grad_fn=<AddBackward0>)\n","3200 tensor(0.6392, device='cuda:0', grad_fn=<AddBackward0>)\n","3400 tensor(0.5729, device='cuda:0', grad_fn=<AddBackward0>)\n","3600 tensor(0.3628, device='cuda:0', grad_fn=<AddBackward0>)\n","3800 tensor(0.4296, device='cuda:0', grad_fn=<AddBackward0>)\n","4000 tensor(0.4332, device='cuda:0', grad_fn=<AddBackward0>)\n","4200 tensor(0.3463, device='cuda:0', grad_fn=<AddBackward0>)\n","4400 tensor(0.5041, device='cuda:0', grad_fn=<AddBackward0>)\n","4600 tensor(0.4767, device='cuda:0', grad_fn=<AddBackward0>)\n","4800 tensor(0.4100, device='cuda:0', grad_fn=<AddBackward0>)\n","5000 tensor(0.3299, device='cuda:0', grad_fn=<AddBackward0>)\n","5200 tensor(0.3975, device='cuda:0', grad_fn=<AddBackward0>)\n","5400 tensor(0.3623, device='cuda:0', grad_fn=<AddBackward0>)\n","5600 tensor(0.4292, device='cuda:0', grad_fn=<AddBackward0>)\n","5800 tensor(0.5904, device='cuda:0', grad_fn=<AddBackward0>)\n","6000 tensor(0.4407, device='cuda:0', grad_fn=<AddBackward0>)\n","6200 tensor(0.3574, device='cuda:0', grad_fn=<AddBackward0>)\n","6400 tensor(0.6357, device='cuda:0', grad_fn=<AddBackward0>)\n","6600 tensor(0.5464, device='cuda:0', grad_fn=<AddBackward0>)\n","6800 tensor(0.4772, device='cuda:0', grad_fn=<AddBackward0>)\n","7000 tensor(0.3592, device='cuda:0', grad_fn=<AddBackward0>)\n","7200 tensor(0.4764, device='cuda:0', grad_fn=<AddBackward0>)\n","7400 tensor(0.4159, device='cuda:0', grad_fn=<AddBackward0>)\n","7600 tensor(0.4034, device='cuda:0', grad_fn=<AddBackward0>)\n","7800 tensor(0.5359, device='cuda:0', grad_fn=<AddBackward0>)\n","8000 tensor(0.4650, device='cuda:0', grad_fn=<AddBackward0>)\n","8200 tensor(0.3551, device='cuda:0', grad_fn=<AddBackward0>)\n","8400 tensor(0.5305, device='cuda:0', grad_fn=<AddBackward0>)\n","8600 tensor(0.4171, device='cuda:0', grad_fn=<AddBackward0>)\n","8800 tensor(0.5143, device='cuda:0', grad_fn=<AddBackward0>)\n","9000 tensor(0.4813, device='cuda:0', grad_fn=<AddBackward0>)\n","9200 tensor(0.4739, device='cuda:0', grad_fn=<AddBackward0>)\n","9400 tensor(0.4078, device='cuda:0', grad_fn=<AddBackward0>)\n","9600 tensor(0.4660, device='cuda:0', grad_fn=<AddBackward0>)\n","9800 tensor(0.4660, device='cuda:0', grad_fn=<AddBackward0>)\n","10000 tensor(0.4968, device='cuda:0', grad_fn=<AddBackward0>)\n","###############    Reward for test environment for run 1: 500.0.   ###############\n","\n","\n","Run 2 out of 10\n","200 tensor(0.4633, device='cuda:0', grad_fn=<AddBackward0>)\n","400 tensor(0.4356, device='cuda:0', grad_fn=<AddBackward0>)\n","600 tensor(0.4789, device='cuda:0', grad_fn=<AddBackward0>)\n","800 tensor(0.4329, device='cuda:0', grad_fn=<AddBackward0>)\n","1000 tensor(0.5328, device='cuda:0', grad_fn=<AddBackward0>)\n","1200 tensor(0.4042, device='cuda:0', grad_fn=<AddBackward0>)\n","1400 tensor(0.5114, device='cuda:0', grad_fn=<AddBackward0>)\n","1600 tensor(0.4908, device='cuda:0', grad_fn=<AddBackward0>)\n","1800 tensor(0.4130, device='cuda:0', grad_fn=<AddBackward0>)\n","2000 tensor(0.4741, device='cuda:0', grad_fn=<AddBackward0>)\n","2200 tensor(0.4827, device='cuda:0', grad_fn=<AddBackward0>)\n","2400 tensor(0.5630, device='cuda:0', grad_fn=<AddBackward0>)\n","2600 tensor(0.4678, device='cuda:0', grad_fn=<AddBackward0>)\n","2800 tensor(0.4623, device='cuda:0', grad_fn=<AddBackward0>)\n","3000 tensor(0.4577, device='cuda:0', grad_fn=<AddBackward0>)\n","3200 tensor(0.4845, device='cuda:0', grad_fn=<AddBackward0>)\n","3400 tensor(0.4523, device='cuda:0', grad_fn=<AddBackward0>)\n","3600 tensor(0.6399, device='cuda:0', grad_fn=<AddBackward0>)\n","3800 tensor(0.4786, device='cuda:0', grad_fn=<AddBackward0>)\n","4000 tensor(0.4973, device='cuda:0', grad_fn=<AddBackward0>)\n","4200 tensor(0.4877, device='cuda:0', grad_fn=<AddBackward0>)\n","4400 tensor(0.4190, device='cuda:0', grad_fn=<AddBackward0>)\n","4600 tensor(0.4808, device='cuda:0', grad_fn=<AddBackward0>)\n","4800 tensor(0.4588, device='cuda:0', grad_fn=<AddBackward0>)\n","5000 tensor(0.4338, device='cuda:0', grad_fn=<AddBackward0>)\n","5200 tensor(0.3918, device='cuda:0', grad_fn=<AddBackward0>)\n","5400 tensor(0.4403, device='cuda:0', grad_fn=<AddBackward0>)\n","5600 tensor(0.5173, device='cuda:0', grad_fn=<AddBackward0>)\n","5800 tensor(0.5245, device='cuda:0', grad_fn=<AddBackward0>)\n","6000 tensor(0.5274, device='cuda:0', grad_fn=<AddBackward0>)\n","6200 tensor(0.6150, device='cuda:0', grad_fn=<AddBackward0>)\n","6400 tensor(0.3348, device='cuda:0', grad_fn=<AddBackward0>)\n","6600 tensor(0.5432, device='cuda:0', grad_fn=<AddBackward0>)\n","6800 tensor(0.4832, device='cuda:0', grad_fn=<AddBackward0>)\n","7000 tensor(0.4507, device='cuda:0', grad_fn=<AddBackward0>)\n","7200 tensor(0.4204, device='cuda:0', grad_fn=<AddBackward0>)\n","7400 tensor(0.3584, device='cuda:0', grad_fn=<AddBackward0>)\n","7600 tensor(0.3819, device='cuda:0', grad_fn=<AddBackward0>)\n","7800 tensor(0.4049, device='cuda:0', grad_fn=<AddBackward0>)\n","8000 tensor(0.4604, device='cuda:0', grad_fn=<AddBackward0>)\n","8200 tensor(0.6376, device='cuda:0', grad_fn=<AddBackward0>)\n","8400 tensor(0.3473, device='cuda:0', grad_fn=<AddBackward0>)\n","8600 tensor(0.5275, device='cuda:0', grad_fn=<AddBackward0>)\n","8800 tensor(0.4467, device='cuda:0', grad_fn=<AddBackward0>)\n","9000 tensor(0.3926, device='cuda:0', grad_fn=<AddBackward0>)\n","9200 tensor(0.4104, device='cuda:0', grad_fn=<AddBackward0>)\n","9400 tensor(0.4646, device='cuda:0', grad_fn=<AddBackward0>)\n","9600 tensor(0.3669, device='cuda:0', grad_fn=<AddBackward0>)\n","9800 tensor(0.3467, device='cuda:0', grad_fn=<AddBackward0>)\n","10000 tensor(0.4640, device='cuda:0', grad_fn=<AddBackward0>)\n","###############    Reward for test environment for run 2: 500.0.   ###############\n","\n","\n","Run 3 out of 10\n","200 tensor(0.4777, device='cuda:0', grad_fn=<AddBackward0>)\n","400 tensor(0.3941, device='cuda:0', grad_fn=<AddBackward0>)\n","600 tensor(0.4967, device='cuda:0', grad_fn=<AddBackward0>)\n","800 tensor(0.4313, device='cuda:0', grad_fn=<AddBackward0>)\n","1000 tensor(0.4369, device='cuda:0', grad_fn=<AddBackward0>)\n","1200 tensor(0.5547, device='cuda:0', grad_fn=<AddBackward0>)\n","1400 tensor(0.3303, device='cuda:0', grad_fn=<AddBackward0>)\n","1600 tensor(0.3486, device='cuda:0', grad_fn=<AddBackward0>)\n","1800 tensor(0.4865, device='cuda:0', grad_fn=<AddBackward0>)\n","2000 tensor(0.3228, device='cuda:0', grad_fn=<AddBackward0>)\n","2200 tensor(0.5105, device='cuda:0', grad_fn=<AddBackward0>)\n","2400 tensor(0.4862, device='cuda:0', grad_fn=<AddBackward0>)\n","2600 tensor(0.3432, device='cuda:0', grad_fn=<AddBackward0>)\n","2800 tensor(0.4114, device='cuda:0', grad_fn=<AddBackward0>)\n","3000 tensor(0.4160, device='cuda:0', grad_fn=<AddBackward0>)\n","3200 tensor(0.3437, device='cuda:0', grad_fn=<AddBackward0>)\n","3400 tensor(0.3623, device='cuda:0', grad_fn=<AddBackward0>)\n","3600 tensor(0.3858, device='cuda:0', grad_fn=<AddBackward0>)\n","3800 tensor(0.5717, device='cuda:0', grad_fn=<AddBackward0>)\n","4000 tensor(0.4072, device='cuda:0', grad_fn=<AddBackward0>)\n","4200 tensor(0.3834, device='cuda:0', grad_fn=<AddBackward0>)\n","4400 tensor(0.4159, device='cuda:0', grad_fn=<AddBackward0>)\n","4600 tensor(0.3779, device='cuda:0', grad_fn=<AddBackward0>)\n","4800 tensor(0.4350, device='cuda:0', grad_fn=<AddBackward0>)\n","5000 tensor(0.3989, device='cuda:0', grad_fn=<AddBackward0>)\n","5200 tensor(0.4276, device='cuda:0', grad_fn=<AddBackward0>)\n","5400 tensor(0.3470, device='cuda:0', grad_fn=<AddBackward0>)\n","5600 tensor(0.4989, device='cuda:0', grad_fn=<AddBackward0>)\n","5800 tensor(0.4314, device='cuda:0', grad_fn=<AddBackward0>)\n","6000 tensor(0.4915, device='cuda:0', grad_fn=<AddBackward0>)\n","6200 tensor(0.3629, device='cuda:0', grad_fn=<AddBackward0>)\n","6400 tensor(0.4534, device='cuda:0', grad_fn=<AddBackward0>)\n","6600 tensor(0.3940, device='cuda:0', grad_fn=<AddBackward0>)\n","6800 tensor(0.4173, device='cuda:0', grad_fn=<AddBackward0>)\n","7000 tensor(0.3554, device='cuda:0', grad_fn=<AddBackward0>)\n","7200 tensor(0.3645, device='cuda:0', grad_fn=<AddBackward0>)\n","7400 tensor(0.5141, device='cuda:0', grad_fn=<AddBackward0>)\n","7600 tensor(0.3534, device='cuda:0', grad_fn=<AddBackward0>)\n","7800 tensor(0.3700, device='cuda:0', grad_fn=<AddBackward0>)\n","8000 tensor(0.4379, device='cuda:0', grad_fn=<AddBackward0>)\n","8200 tensor(0.4646, device='cuda:0', grad_fn=<AddBackward0>)\n","8400 tensor(0.5127, device='cuda:0', grad_fn=<AddBackward0>)\n","8600 tensor(0.3827, device='cuda:0', grad_fn=<AddBackward0>)\n","8800 tensor(0.4894, device='cuda:0', grad_fn=<AddBackward0>)\n","9000 tensor(0.3838, device='cuda:0', grad_fn=<AddBackward0>)\n","9200 tensor(0.4982, device='cuda:0', grad_fn=<AddBackward0>)\n","9400 tensor(0.4258, device='cuda:0', grad_fn=<AddBackward0>)\n","9600 tensor(0.4980, device='cuda:0', grad_fn=<AddBackward0>)\n","9800 tensor(0.4156, device='cuda:0', grad_fn=<AddBackward0>)\n","10000 tensor(0.3742, device='cuda:0', grad_fn=<AddBackward0>)\n","###############    Reward for test environment for run 3: 500.0.   ###############\n","\n","\n","Run 4 out of 10\n","200 tensor(0.4055, device='cuda:0', grad_fn=<AddBackward0>)\n","400 tensor(0.3889, device='cuda:0', grad_fn=<AddBackward0>)\n","600 tensor(0.4606, device='cuda:0', grad_fn=<AddBackward0>)\n","800 tensor(0.3499, device='cuda:0', grad_fn=<AddBackward0>)\n","1000 tensor(0.5976, device='cuda:0', grad_fn=<AddBackward0>)\n","1200 tensor(0.4434, device='cuda:0', grad_fn=<AddBackward0>)\n","1400 tensor(0.4671, device='cuda:0', grad_fn=<AddBackward0>)\n","1600 tensor(0.3102, device='cuda:0', grad_fn=<AddBackward0>)\n","1800 tensor(0.3908, device='cuda:0', grad_fn=<AddBackward0>)\n","2000 tensor(0.5072, device='cuda:0', grad_fn=<AddBackward0>)\n","2200 tensor(0.5751, device='cuda:0', grad_fn=<AddBackward0>)\n","2400 tensor(0.3628, device='cuda:0', grad_fn=<AddBackward0>)\n","2600 tensor(0.4372, device='cuda:0', grad_fn=<AddBackward0>)\n","2800 tensor(0.5374, device='cuda:0', grad_fn=<AddBackward0>)\n","3000 tensor(0.3915, device='cuda:0', grad_fn=<AddBackward0>)\n","3200 tensor(0.2895, device='cuda:0', grad_fn=<AddBackward0>)\n","3400 tensor(0.4264, device='cuda:0', grad_fn=<AddBackward0>)\n","3600 tensor(0.3917, device='cuda:0', grad_fn=<AddBackward0>)\n","3800 tensor(0.4354, device='cuda:0', grad_fn=<AddBackward0>)\n","4000 tensor(0.3937, device='cuda:0', grad_fn=<AddBackward0>)\n","4200 tensor(0.3979, device='cuda:0', grad_fn=<AddBackward0>)\n","4400 tensor(0.4320, device='cuda:0', grad_fn=<AddBackward0>)\n","4600 tensor(0.3254, device='cuda:0', grad_fn=<AddBackward0>)\n","4800 tensor(0.4601, device='cuda:0', grad_fn=<AddBackward0>)\n","5000 tensor(0.4886, device='cuda:0', grad_fn=<AddBackward0>)\n","5200 tensor(0.4213, device='cuda:0', grad_fn=<AddBackward0>)\n","5400 tensor(0.3349, device='cuda:0', grad_fn=<AddBackward0>)\n","5600 tensor(0.3538, device='cuda:0', grad_fn=<AddBackward0>)\n","5800 tensor(0.3542, device='cuda:0', grad_fn=<AddBackward0>)\n","6000 tensor(0.3519, device='cuda:0', grad_fn=<AddBackward0>)\n","6200 tensor(0.5079, device='cuda:0', grad_fn=<AddBackward0>)\n","6400 tensor(0.4002, device='cuda:0', grad_fn=<AddBackward0>)\n","6600 tensor(0.3187, device='cuda:0', grad_fn=<AddBackward0>)\n","6800 tensor(0.3188, device='cuda:0', grad_fn=<AddBackward0>)\n","7000 tensor(0.4645, device='cuda:0', grad_fn=<AddBackward0>)\n","7200 tensor(0.5227, device='cuda:0', grad_fn=<AddBackward0>)\n","7400 tensor(0.4124, device='cuda:0', grad_fn=<AddBackward0>)\n","7600 tensor(0.3441, device='cuda:0', grad_fn=<AddBackward0>)\n","7800 tensor(0.4814, device='cuda:0', grad_fn=<AddBackward0>)\n","8000 tensor(0.3557, device='cuda:0', grad_fn=<AddBackward0>)\n","8200 tensor(0.3794, device='cuda:0', grad_fn=<AddBackward0>)\n","8400 tensor(0.5175, device='cuda:0', grad_fn=<AddBackward0>)\n","8600 tensor(0.4351, device='cuda:0', grad_fn=<AddBackward0>)\n","8800 tensor(0.3927, device='cuda:0', grad_fn=<AddBackward0>)\n","9000 tensor(0.3817, device='cuda:0', grad_fn=<AddBackward0>)\n","9200 tensor(0.3335, device='cuda:0', grad_fn=<AddBackward0>)\n","9400 tensor(0.3244, device='cuda:0', grad_fn=<AddBackward0>)\n","9600 tensor(0.3683, device='cuda:0', grad_fn=<AddBackward0>)\n","9800 tensor(0.4535, device='cuda:0', grad_fn=<AddBackward0>)\n","10000 tensor(0.3502, device='cuda:0', grad_fn=<AddBackward0>)\n","###############    Reward for test environment for run 4: 500.0.   ###############\n","\n","\n","Run 5 out of 10\n","200 tensor(0.5101, device='cuda:0', grad_fn=<AddBackward0>)\n","400 tensor(0.5661, device='cuda:0', grad_fn=<AddBackward0>)\n","600 tensor(0.4944, device='cuda:0', grad_fn=<AddBackward0>)\n","800 tensor(0.5068, device='cuda:0', grad_fn=<AddBackward0>)\n","1000 tensor(0.4905, device='cuda:0', grad_fn=<AddBackward0>)\n","1200 tensor(0.4803, device='cuda:0', grad_fn=<AddBackward0>)\n","1400 tensor(0.4310, device='cuda:0', grad_fn=<AddBackward0>)\n","1600 tensor(0.5100, device='cuda:0', grad_fn=<AddBackward0>)\n","1800 tensor(0.5218, device='cuda:0', grad_fn=<AddBackward0>)\n","2000 tensor(0.4900, device='cuda:0', grad_fn=<AddBackward0>)\n","2200 tensor(0.5024, device='cuda:0', grad_fn=<AddBackward0>)\n","2400 tensor(0.4781, device='cuda:0', grad_fn=<AddBackward0>)\n","2600 tensor(0.5175, device='cuda:0', grad_fn=<AddBackward0>)\n","2800 tensor(0.4335, device='cuda:0', grad_fn=<AddBackward0>)\n","3000 tensor(0.4572, device='cuda:0', grad_fn=<AddBackward0>)\n","3200 tensor(0.4792, device='cuda:0', grad_fn=<AddBackward0>)\n","3400 tensor(0.4236, device='cuda:0', grad_fn=<AddBackward0>)\n","3600 tensor(0.5360, device='cuda:0', grad_fn=<AddBackward0>)\n","3800 tensor(0.4744, device='cuda:0', grad_fn=<AddBackward0>)\n","4000 tensor(0.4448, device='cuda:0', grad_fn=<AddBackward0>)\n","4200 tensor(0.5708, device='cuda:0', grad_fn=<AddBackward0>)\n","4400 tensor(0.4116, device='cuda:0', grad_fn=<AddBackward0>)\n","4600 tensor(0.3588, device='cuda:0', grad_fn=<AddBackward0>)\n","4800 tensor(0.3968, device='cuda:0', grad_fn=<AddBackward0>)\n","5000 tensor(0.4405, device='cuda:0', grad_fn=<AddBackward0>)\n","5200 tensor(0.3654, device='cuda:0', grad_fn=<AddBackward0>)\n","5400 tensor(0.5062, device='cuda:0', grad_fn=<AddBackward0>)\n","5600 tensor(0.4260, device='cuda:0', grad_fn=<AddBackward0>)\n","5800 tensor(0.4213, device='cuda:0', grad_fn=<AddBackward0>)\n","6000 tensor(0.3095, device='cuda:0', grad_fn=<AddBackward0>)\n","6200 tensor(0.3962, device='cuda:0', grad_fn=<AddBackward0>)\n","6400 tensor(0.4030, device='cuda:0', grad_fn=<AddBackward0>)\n","6600 tensor(0.4026, device='cuda:0', grad_fn=<AddBackward0>)\n","6800 tensor(0.4156, device='cuda:0', grad_fn=<AddBackward0>)\n","7000 tensor(0.5200, device='cuda:0', grad_fn=<AddBackward0>)\n","7200 tensor(0.4061, device='cuda:0', grad_fn=<AddBackward0>)\n","7400 tensor(0.5446, device='cuda:0', grad_fn=<AddBackward0>)\n","7600 tensor(0.4513, device='cuda:0', grad_fn=<AddBackward0>)\n","7800 tensor(0.4303, device='cuda:0', grad_fn=<AddBackward0>)\n","8000 tensor(0.4575, device='cuda:0', grad_fn=<AddBackward0>)\n","8200 tensor(0.5071, device='cuda:0', grad_fn=<AddBackward0>)\n","8400 tensor(0.5015, device='cuda:0', grad_fn=<AddBackward0>)\n","8600 tensor(0.5341, device='cuda:0', grad_fn=<AddBackward0>)\n","8800 tensor(0.4583, device='cuda:0', grad_fn=<AddBackward0>)\n","9000 tensor(0.4521, device='cuda:0', grad_fn=<AddBackward0>)\n","9200 tensor(0.4509, device='cuda:0', grad_fn=<AddBackward0>)\n","9400 tensor(0.4463, device='cuda:0', grad_fn=<AddBackward0>)\n","9600 tensor(0.4751, device='cuda:0', grad_fn=<AddBackward0>)\n","9800 tensor(0.3832, device='cuda:0', grad_fn=<AddBackward0>)\n","10000 tensor(0.4775, device='cuda:0', grad_fn=<AddBackward0>)\n","###############    Reward for test environment for run 5: 500.0.   ###############\n","\n","\n","Run 6 out of 10\n","200 tensor(0.5242, device='cuda:0', grad_fn=<AddBackward0>)\n","400 tensor(0.4283, device='cuda:0', grad_fn=<AddBackward0>)\n","600 tensor(0.4671, device='cuda:0', grad_fn=<AddBackward0>)\n","800 tensor(0.3733, device='cuda:0', grad_fn=<AddBackward0>)\n","1000 tensor(0.4743, device='cuda:0', grad_fn=<AddBackward0>)\n","1200 tensor(0.4334, device='cuda:0', grad_fn=<AddBackward0>)\n","1400 tensor(0.3921, device='cuda:0', grad_fn=<AddBackward0>)\n","1600 tensor(0.4838, device='cuda:0', grad_fn=<AddBackward0>)\n","1800 tensor(0.4149, device='cuda:0', grad_fn=<AddBackward0>)\n","2000 tensor(0.5421, device='cuda:0', grad_fn=<AddBackward0>)\n","2200 tensor(0.4384, device='cuda:0', grad_fn=<AddBackward0>)\n","2400 tensor(0.4349, device='cuda:0', grad_fn=<AddBackward0>)\n","2600 tensor(0.4971, device='cuda:0', grad_fn=<AddBackward0>)\n","2800 tensor(0.5017, device='cuda:0', grad_fn=<AddBackward0>)\n","3000 tensor(0.3965, device='cuda:0', grad_fn=<AddBackward0>)\n","3200 tensor(0.4337, device='cuda:0', grad_fn=<AddBackward0>)\n","3400 tensor(0.4950, device='cuda:0', grad_fn=<AddBackward0>)\n","3600 tensor(0.3649, device='cuda:0', grad_fn=<AddBackward0>)\n","3800 tensor(0.4068, device='cuda:0', grad_fn=<AddBackward0>)\n","4000 tensor(0.4000, device='cuda:0', grad_fn=<AddBackward0>)\n","4200 tensor(0.3948, device='cuda:0', grad_fn=<AddBackward0>)\n","4400 tensor(0.4424, device='cuda:0', grad_fn=<AddBackward0>)\n","4600 tensor(0.4057, device='cuda:0', grad_fn=<AddBackward0>)\n","4800 tensor(0.4762, device='cuda:0', grad_fn=<AddBackward0>)\n","5000 tensor(0.5721, device='cuda:0', grad_fn=<AddBackward0>)\n","5200 tensor(0.4365, device='cuda:0', grad_fn=<AddBackward0>)\n","5400 tensor(0.4298, device='cuda:0', grad_fn=<AddBackward0>)\n","5600 tensor(0.3593, device='cuda:0', grad_fn=<AddBackward0>)\n","5800 tensor(0.5259, device='cuda:0', grad_fn=<AddBackward0>)\n","6000 tensor(0.6846, device='cuda:0', grad_fn=<AddBackward0>)\n","6200 tensor(0.3737, device='cuda:0', grad_fn=<AddBackward0>)\n","6400 tensor(0.4221, device='cuda:0', grad_fn=<AddBackward0>)\n","6600 tensor(0.4174, device='cuda:0', grad_fn=<AddBackward0>)\n","6800 tensor(0.3716, device='cuda:0', grad_fn=<AddBackward0>)\n","7000 tensor(0.4892, device='cuda:0', grad_fn=<AddBackward0>)\n","7200 tensor(0.6121, device='cuda:0', grad_fn=<AddBackward0>)\n","7400 tensor(0.5533, device='cuda:0', grad_fn=<AddBackward0>)\n","7600 tensor(0.3168, device='cuda:0', grad_fn=<AddBackward0>)\n","7800 tensor(0.3936, device='cuda:0', grad_fn=<AddBackward0>)\n","8000 tensor(0.4069, device='cuda:0', grad_fn=<AddBackward0>)\n","8200 tensor(0.4179, device='cuda:0', grad_fn=<AddBackward0>)\n","8400 tensor(0.5282, device='cuda:0', grad_fn=<AddBackward0>)\n","8600 tensor(0.3875, device='cuda:0', grad_fn=<AddBackward0>)\n","8800 tensor(0.3617, device='cuda:0', grad_fn=<AddBackward0>)\n","9000 tensor(0.4657, device='cuda:0', grad_fn=<AddBackward0>)\n","9200 tensor(0.3939, device='cuda:0', grad_fn=<AddBackward0>)\n","9400 tensor(0.4217, device='cuda:0', grad_fn=<AddBackward0>)\n","9600 tensor(0.4849, device='cuda:0', grad_fn=<AddBackward0>)\n","9800 tensor(0.5560, device='cuda:0', grad_fn=<AddBackward0>)\n","10000 tensor(0.6484, device='cuda:0', grad_fn=<AddBackward0>)\n","###############    Reward for test environment for run 6: 500.0.   ###############\n","\n","\n","Run 7 out of 10\n","200 tensor(0.5311, device='cuda:0', grad_fn=<AddBackward0>)\n","400 tensor(0.5440, device='cuda:0', grad_fn=<AddBackward0>)\n","600 tensor(0.5003, device='cuda:0', grad_fn=<AddBackward0>)\n","800 tensor(0.3614, device='cuda:0', grad_fn=<AddBackward0>)\n","1000 tensor(0.4028, device='cuda:0', grad_fn=<AddBackward0>)\n","1200 tensor(0.5195, device='cuda:0', grad_fn=<AddBackward0>)\n","1400 tensor(0.5241, device='cuda:0', grad_fn=<AddBackward0>)\n","1600 tensor(0.4075, device='cuda:0', grad_fn=<AddBackward0>)\n","1800 tensor(0.4812, device='cuda:0', grad_fn=<AddBackward0>)\n","2000 tensor(0.4508, device='cuda:0', grad_fn=<AddBackward0>)\n","2200 tensor(0.4922, device='cuda:0', grad_fn=<AddBackward0>)\n","2400 tensor(0.5194, device='cuda:0', grad_fn=<AddBackward0>)\n","2600 tensor(0.4981, device='cuda:0', grad_fn=<AddBackward0>)\n","2800 tensor(0.3745, device='cuda:0', grad_fn=<AddBackward0>)\n","3000 tensor(0.4308, device='cuda:0', grad_fn=<AddBackward0>)\n","3200 tensor(0.4742, device='cuda:0', grad_fn=<AddBackward0>)\n","3400 tensor(0.4162, device='cuda:0', grad_fn=<AddBackward0>)\n","3600 tensor(0.4566, device='cuda:0', grad_fn=<AddBackward0>)\n","3800 tensor(0.5212, device='cuda:0', grad_fn=<AddBackward0>)\n","4000 tensor(0.4498, device='cuda:0', grad_fn=<AddBackward0>)\n","4200 tensor(0.5146, device='cuda:0', grad_fn=<AddBackward0>)\n","4400 tensor(0.4515, device='cuda:0', grad_fn=<AddBackward0>)\n","4600 tensor(0.3820, device='cuda:0', grad_fn=<AddBackward0>)\n","4800 tensor(0.5623, device='cuda:0', grad_fn=<AddBackward0>)\n","5000 tensor(0.4734, device='cuda:0', grad_fn=<AddBackward0>)\n","5200 tensor(0.4364, device='cuda:0', grad_fn=<AddBackward0>)\n","5400 tensor(0.4352, device='cuda:0', grad_fn=<AddBackward0>)\n","5600 tensor(0.4232, device='cuda:0', grad_fn=<AddBackward0>)\n","5800 tensor(0.6189, device='cuda:0', grad_fn=<AddBackward0>)\n","6000 tensor(0.4834, device='cuda:0', grad_fn=<AddBackward0>)\n","6200 tensor(0.3909, device='cuda:0', grad_fn=<AddBackward0>)\n","6400 tensor(0.4605, device='cuda:0', grad_fn=<AddBackward0>)\n","6600 tensor(0.4030, device='cuda:0', grad_fn=<AddBackward0>)\n","6800 tensor(0.5104, device='cuda:0', grad_fn=<AddBackward0>)\n","7000 tensor(0.3355, device='cuda:0', grad_fn=<AddBackward0>)\n","7200 tensor(0.4352, device='cuda:0', grad_fn=<AddBackward0>)\n","7400 tensor(0.3968, device='cuda:0', grad_fn=<AddBackward0>)\n","7600 tensor(0.3954, device='cuda:0', grad_fn=<AddBackward0>)\n","7800 tensor(0.4083, device='cuda:0', grad_fn=<AddBackward0>)\n","8000 tensor(0.4171, device='cuda:0', grad_fn=<AddBackward0>)\n","8200 tensor(0.5096, device='cuda:0', grad_fn=<AddBackward0>)\n","8400 tensor(0.4177, device='cuda:0', grad_fn=<AddBackward0>)\n","8600 tensor(0.4559, device='cuda:0', grad_fn=<AddBackward0>)\n","8800 tensor(0.4161, device='cuda:0', grad_fn=<AddBackward0>)\n","9000 tensor(0.3261, device='cuda:0', grad_fn=<AddBackward0>)\n","9200 tensor(0.4317, device='cuda:0', grad_fn=<AddBackward0>)\n","9400 tensor(0.4830, device='cuda:0', grad_fn=<AddBackward0>)\n","9600 tensor(0.3788, device='cuda:0', grad_fn=<AddBackward0>)\n","9800 tensor(0.5477, device='cuda:0', grad_fn=<AddBackward0>)\n","10000 tensor(0.4372, device='cuda:0', grad_fn=<AddBackward0>)\n","###############    Reward for test environment for run 7: 500.0.   ###############\n","\n","\n","Run 8 out of 10\n","200 tensor(0.4996, device='cuda:0', grad_fn=<AddBackward0>)\n","400 tensor(0.4832, device='cuda:0', grad_fn=<AddBackward0>)\n","600 tensor(0.4116, device='cuda:0', grad_fn=<AddBackward0>)\n","800 tensor(0.4379, device='cuda:0', grad_fn=<AddBackward0>)\n","1000 tensor(0.5477, device='cuda:0', grad_fn=<AddBackward0>)\n","1200 tensor(0.4906, device='cuda:0', grad_fn=<AddBackward0>)\n","1400 tensor(0.4750, device='cuda:0', grad_fn=<AddBackward0>)\n","1600 tensor(0.4457, device='cuda:0', grad_fn=<AddBackward0>)\n","1800 tensor(0.4254, device='cuda:0', grad_fn=<AddBackward0>)\n","2000 tensor(0.5194, device='cuda:0', grad_fn=<AddBackward0>)\n","2200 tensor(0.3465, device='cuda:0', grad_fn=<AddBackward0>)\n","2400 tensor(0.4050, device='cuda:0', grad_fn=<AddBackward0>)\n","2600 tensor(0.4509, device='cuda:0', grad_fn=<AddBackward0>)\n","2800 tensor(0.4819, device='cuda:0', grad_fn=<AddBackward0>)\n","3000 tensor(0.5030, device='cuda:0', grad_fn=<AddBackward0>)\n","3200 tensor(0.4792, device='cuda:0', grad_fn=<AddBackward0>)\n","3400 tensor(0.4285, device='cuda:0', grad_fn=<AddBackward0>)\n","3600 tensor(0.4628, device='cuda:0', grad_fn=<AddBackward0>)\n","3800 tensor(0.4397, device='cuda:0', grad_fn=<AddBackward0>)\n","4000 tensor(0.3735, device='cuda:0', grad_fn=<AddBackward0>)\n","4200 tensor(0.5094, device='cuda:0', grad_fn=<AddBackward0>)\n","4400 tensor(0.4456, device='cuda:0', grad_fn=<AddBackward0>)\n","4600 tensor(0.5018, device='cuda:0', grad_fn=<AddBackward0>)\n","4800 tensor(0.4479, device='cuda:0', grad_fn=<AddBackward0>)\n","5000 tensor(0.4028, device='cuda:0', grad_fn=<AddBackward0>)\n","5200 tensor(0.4805, device='cuda:0', grad_fn=<AddBackward0>)\n","5400 tensor(0.4057, device='cuda:0', grad_fn=<AddBackward0>)\n","5600 tensor(0.4615, device='cuda:0', grad_fn=<AddBackward0>)\n","5800 tensor(0.4811, device='cuda:0', grad_fn=<AddBackward0>)\n","6000 tensor(0.4588, device='cuda:0', grad_fn=<AddBackward0>)\n","6200 tensor(0.4159, device='cuda:0', grad_fn=<AddBackward0>)\n","6400 tensor(0.3645, device='cuda:0', grad_fn=<AddBackward0>)\n","6600 tensor(0.4662, device='cuda:0', grad_fn=<AddBackward0>)\n","6800 tensor(0.4364, device='cuda:0', grad_fn=<AddBackward0>)\n","7000 tensor(0.5097, device='cuda:0', grad_fn=<AddBackward0>)\n","7200 tensor(0.3663, device='cuda:0', grad_fn=<AddBackward0>)\n","7400 tensor(0.5068, device='cuda:0', grad_fn=<AddBackward0>)\n","7600 tensor(0.4684, device='cuda:0', grad_fn=<AddBackward0>)\n","7800 tensor(0.5493, device='cuda:0', grad_fn=<AddBackward0>)\n","8000 tensor(0.4219, device='cuda:0', grad_fn=<AddBackward0>)\n","8200 tensor(0.4671, device='cuda:0', grad_fn=<AddBackward0>)\n","8400 tensor(0.3976, device='cuda:0', grad_fn=<AddBackward0>)\n","8600 tensor(0.5095, device='cuda:0', grad_fn=<AddBackward0>)\n","8800 tensor(0.4081, device='cuda:0', grad_fn=<AddBackward0>)\n","9000 tensor(0.3286, device='cuda:0', grad_fn=<AddBackward0>)\n","9200 tensor(0.3659, device='cuda:0', grad_fn=<AddBackward0>)\n","9400 tensor(0.4363, device='cuda:0', grad_fn=<AddBackward0>)\n","9600 tensor(0.3921, device='cuda:0', grad_fn=<AddBackward0>)\n","9800 tensor(0.4081, device='cuda:0', grad_fn=<AddBackward0>)\n","10000 tensor(0.4465, device='cuda:0', grad_fn=<AddBackward0>)\n","###############    Reward for test environment for run 8: 500.0.   ###############\n","\n","\n","Run 9 out of 10\n","200 tensor(0.4446, device='cuda:0', grad_fn=<AddBackward0>)\n","400 tensor(0.4197, device='cuda:0', grad_fn=<AddBackward0>)\n","600 tensor(0.4441, device='cuda:0', grad_fn=<AddBackward0>)\n","800 tensor(0.4154, device='cuda:0', grad_fn=<AddBackward0>)\n","1000 tensor(0.5217, device='cuda:0', grad_fn=<AddBackward0>)\n","1200 tensor(0.3868, device='cuda:0', grad_fn=<AddBackward0>)\n","1400 tensor(0.4100, device='cuda:0', grad_fn=<AddBackward0>)\n","1600 tensor(0.4190, device='cuda:0', grad_fn=<AddBackward0>)\n","1800 tensor(0.5266, device='cuda:0', grad_fn=<AddBackward0>)\n","2000 tensor(0.4214, device='cuda:0', grad_fn=<AddBackward0>)\n","2200 tensor(0.3395, device='cuda:0', grad_fn=<AddBackward0>)\n","2400 tensor(0.5406, device='cuda:0', grad_fn=<AddBackward0>)\n","2600 tensor(0.3410, device='cuda:0', grad_fn=<AddBackward0>)\n","2800 tensor(0.3722, device='cuda:0', grad_fn=<AddBackward0>)\n","3000 tensor(0.3980, device='cuda:0', grad_fn=<AddBackward0>)\n","3200 tensor(0.4600, device='cuda:0', grad_fn=<AddBackward0>)\n","3400 tensor(0.3514, device='cuda:0', grad_fn=<AddBackward0>)\n","3600 tensor(0.4112, device='cuda:0', grad_fn=<AddBackward0>)\n","3800 tensor(0.3473, device='cuda:0', grad_fn=<AddBackward0>)\n","4000 tensor(0.4019, device='cuda:0', grad_fn=<AddBackward0>)\n","4200 tensor(0.4342, device='cuda:0', grad_fn=<AddBackward0>)\n","4400 tensor(0.4383, device='cuda:0', grad_fn=<AddBackward0>)\n","4600 tensor(0.3476, device='cuda:0', grad_fn=<AddBackward0>)\n","4800 tensor(0.4804, device='cuda:0', grad_fn=<AddBackward0>)\n","5000 tensor(0.3792, device='cuda:0', grad_fn=<AddBackward0>)\n","5200 tensor(0.3714, device='cuda:0', grad_fn=<AddBackward0>)\n","5400 tensor(0.4491, device='cuda:0', grad_fn=<AddBackward0>)\n","5600 tensor(0.3453, device='cuda:0', grad_fn=<AddBackward0>)\n","5800 tensor(0.3604, device='cuda:0', grad_fn=<AddBackward0>)\n","6000 tensor(0.3952, device='cuda:0', grad_fn=<AddBackward0>)\n","6200 tensor(0.3530, device='cuda:0', grad_fn=<AddBackward0>)\n","6400 tensor(0.3499, device='cuda:0', grad_fn=<AddBackward0>)\n","6600 tensor(0.4078, device='cuda:0', grad_fn=<AddBackward0>)\n","6800 tensor(0.4009, device='cuda:0', grad_fn=<AddBackward0>)\n","7000 tensor(0.5377, device='cuda:0', grad_fn=<AddBackward0>)\n","7200 tensor(0.3385, device='cuda:0', grad_fn=<AddBackward0>)\n","7400 tensor(0.3533, device='cuda:0', grad_fn=<AddBackward0>)\n","7600 tensor(0.2860, device='cuda:0', grad_fn=<AddBackward0>)\n","7800 tensor(0.3732, device='cuda:0', grad_fn=<AddBackward0>)\n","8000 tensor(0.4586, device='cuda:0', grad_fn=<AddBackward0>)\n","8200 tensor(0.4152, device='cuda:0', grad_fn=<AddBackward0>)\n","8400 tensor(0.3663, device='cuda:0', grad_fn=<AddBackward0>)\n","8600 tensor(0.3970, device='cuda:0', grad_fn=<AddBackward0>)\n","8800 tensor(0.4055, device='cuda:0', grad_fn=<AddBackward0>)\n","9000 tensor(0.3401, device='cuda:0', grad_fn=<AddBackward0>)\n","9200 tensor(0.3687, device='cuda:0', grad_fn=<AddBackward0>)\n","9400 tensor(0.4331, device='cuda:0', grad_fn=<AddBackward0>)\n","9600 tensor(0.4327, device='cuda:0', grad_fn=<AddBackward0>)\n","9800 tensor(0.3229, device='cuda:0', grad_fn=<AddBackward0>)\n","10000 tensor(0.4022, device='cuda:0', grad_fn=<AddBackward0>)\n","###############    Reward for test environment for run 9: 500.0.   ###############\n","\n","\n","Run 10 out of 10\n","200 tensor(0.5783, device='cuda:0', grad_fn=<AddBackward0>)\n","400 tensor(0.5127, device='cuda:0', grad_fn=<AddBackward0>)\n","600 tensor(0.6127, device='cuda:0', grad_fn=<AddBackward0>)\n","800 tensor(0.4535, device='cuda:0', grad_fn=<AddBackward0>)\n","1000 tensor(0.4916, device='cuda:0', grad_fn=<AddBackward0>)\n","1200 tensor(0.4756, device='cuda:0', grad_fn=<AddBackward0>)\n","1400 tensor(0.3963, device='cuda:0', grad_fn=<AddBackward0>)\n","1600 tensor(0.4793, device='cuda:0', grad_fn=<AddBackward0>)\n","1800 tensor(0.8120, device='cuda:0', grad_fn=<AddBackward0>)\n","2000 tensor(0.4294, device='cuda:0', grad_fn=<AddBackward0>)\n","2200 tensor(0.5630, device='cuda:0', grad_fn=<AddBackward0>)\n","2400 tensor(0.4126, device='cuda:0', grad_fn=<AddBackward0>)\n","2600 tensor(0.4683, device='cuda:0', grad_fn=<AddBackward0>)\n","2800 tensor(0.5448, device='cuda:0', grad_fn=<AddBackward0>)\n","3000 tensor(0.5140, device='cuda:0', grad_fn=<AddBackward0>)\n","3200 tensor(0.5784, device='cuda:0', grad_fn=<AddBackward0>)\n","3400 tensor(0.3458, device='cuda:0', grad_fn=<AddBackward0>)\n","3600 tensor(0.6900, device='cuda:0', grad_fn=<AddBackward0>)\n","3800 tensor(0.5784, device='cuda:0', grad_fn=<AddBackward0>)\n","4000 tensor(0.4420, device='cuda:0', grad_fn=<AddBackward0>)\n","4200 tensor(0.5287, device='cuda:0', grad_fn=<AddBackward0>)\n","4400 tensor(0.4385, device='cuda:0', grad_fn=<AddBackward0>)\n","4600 tensor(0.4884, device='cuda:0', grad_fn=<AddBackward0>)\n","4800 tensor(0.5089, device='cuda:0', grad_fn=<AddBackward0>)\n","5000 tensor(0.3094, device='cuda:0', grad_fn=<AddBackward0>)\n","5200 tensor(0.4566, device='cuda:0', grad_fn=<AddBackward0>)\n","5400 tensor(0.4584, device='cuda:0', grad_fn=<AddBackward0>)\n","5600 tensor(0.5061, device='cuda:0', grad_fn=<AddBackward0>)\n","5800 tensor(0.4770, device='cuda:0', grad_fn=<AddBackward0>)\n","6000 tensor(0.3909, device='cuda:0', grad_fn=<AddBackward0>)\n","6200 tensor(0.5923, device='cuda:0', grad_fn=<AddBackward0>)\n","6400 tensor(0.5265, device='cuda:0', grad_fn=<AddBackward0>)\n","6600 tensor(0.4626, device='cuda:0', grad_fn=<AddBackward0>)\n","6800 tensor(0.4599, device='cuda:0', grad_fn=<AddBackward0>)\n","7000 tensor(0.6071, device='cuda:0', grad_fn=<AddBackward0>)\n","7200 tensor(0.4436, device='cuda:0', grad_fn=<AddBackward0>)\n","7400 tensor(0.4251, device='cuda:0', grad_fn=<AddBackward0>)\n","7600 tensor(0.4227, device='cuda:0', grad_fn=<AddBackward0>)\n","7800 tensor(0.4851, device='cuda:0', grad_fn=<AddBackward0>)\n","8000 tensor(0.3918, device='cuda:0', grad_fn=<AddBackward0>)\n","8200 tensor(0.4012, device='cuda:0', grad_fn=<AddBackward0>)\n","8400 tensor(0.4658, device='cuda:0', grad_fn=<AddBackward0>)\n","8600 tensor(0.4172, device='cuda:0', grad_fn=<AddBackward0>)\n","8800 tensor(0.5353, device='cuda:0', grad_fn=<AddBackward0>)\n","9000 tensor(0.3854, device='cuda:0', grad_fn=<AddBackward0>)\n","9200 tensor(0.3747, device='cuda:0', grad_fn=<AddBackward0>)\n","9400 tensor(0.3918, device='cuda:0', grad_fn=<AddBackward0>)\n","9600 tensor(0.4321, device='cuda:0', grad_fn=<AddBackward0>)\n","9800 tensor(0.4465, device='cuda:0', grad_fn=<AddBackward0>)\n","10000 tensor(0.4670, device='cuda:0', grad_fn=<AddBackward0>)\n","###############    Reward for test environment for run 10: 500.0.   ###############\n","\n","\n","Average reward for 10 repetitions: 500.0\n"]}]},{"cell_type":"markdown","source":["# *** *core* *** generate expert_traj_i "],"metadata":{"id":"GZPXdm7LZW3I"}},{"cell_type":"code","source":["\n","\n","\n","def get_train_spurcorr_expert_trajs(datafile_name, env_name,  _noise = 0.001,  num_envs = 2):\n","\n","    from tqdm import tqdm \n","    import numpy as np\n","\n","    def generate_spurcorr_obs(observations, _mult_factor, _noise = 0.001, _idx = 0):\n","        noise_dims = len(_mult_factor)\n","        #obs_noise = np.zeros_like(observations)\n","        #obs_noise[-noise_dims:] = np.random.randn(noise_dims) * _noise\n","        spur_corr = np.matmul(observations[-noise_dims:], _mult_factor)\n","        #obs = np.concatenate([observations + obs_noise, spur_corr, [_idx]])\n","        obs = np.concatenate([observations , spur_corr + np.random.randn(noise_dims) * _noise, [_idx]])\n","        return obs\n","\n","    raw = np.load(datafile_name, allow_pickle = True)#[()]#[\"trajs\"]\n","\n","    obs_dim = len(raw['obs'][0])\n","    obs_num = len(raw['obs'])\n","\n","    for expert_num in range(num_envs):\n","        # print(_mult_factor_multipliers[expert_num])\n","            \n","        obs_new = np.zeros(shape = (obs_num, obs_dim *2))\n","\n","        #_mult_factor = np.diag(np.ones(obs_dim-1)) * _mult_factor_multipliers[expert_num]\n","        if expert_num ==0:\n","            _mult_factor = np.diag(np.ones(obs_dim-1))\n","        elif expert_num ==1:\n","            _mult_factor = np.ones((obs_dim-1, obs_dim-1)) + np.diag(np.ones(obs_dim-1))\n","\n","\n","        for i in tqdm(range(obs_num)):\n","            obs_new[i] = generate_spurcorr_obs(raw['obs'][i], _mult_factor, _noise = 0.001, _idx = expert_num)\n","\n","\n","        start_index = np.where(raw['episode_starts'] == 1)[0]\n","\n","        data_block = []\n","        for i in range(len(start_index)-1):\n","            slice_idx = np.arange(start_index[i], start_index[i+1])\n","\n","            obs = obs_new[slice_idx]\n","            actions = raw['actions'][slice_idx]\n","\n","            data_list = []\n","            for j in range(len(slice_idx)):\n","                data_list.append( (obs[j], actions[j][0], expert_num) )\n","\n","            data_block.append(data_list)\n","\n","        data_generated = {'trajs': data_block}  \n","\n","\n","        isExist = os.path.exists(\"./volume/\" + env_name)\n","        if not isExist:\n","            # Create a new directory because it does not exist \n","            os.makedirs(\"./volume/\" + env_name)\n","            print(\"The new directory for {} is created!\".format(env_name))\n","\n","\n","        np.save(\"./volume/\" + env_name + '/expert_trajs_' + str(expert_num) + '.npy', data_generated)\n","        print(\"\\n{} saved!\".format(\"./volume/\" + env_name + '/expert_trajs_' + str(expert_num) + '.npy'))\n","        \n","    #return data_generated"],"metadata":{"id":"MfsBdtuy-dnV"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["np.ones((obs_dim-1, obs_dim-1)) + np.diag(np.ones(obs_dim-1))"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"KP41mE0zr_QT","executionInfo":{"status":"ok","timestamp":1650235457872,"user_tz":240,"elapsed":107,"user":{"displayName":"3 Lin","userId":"16848054003967173878"}},"outputId":"03b5bd2d-9c61-4e1f-ab7e-bc2c7e872834"},"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["array([[2., 1., 1.],\n","       [1., 2., 1.],\n","       [1., 1., 2.]])"]},"metadata":{},"execution_count":34}]},{"cell_type":"code","source":["import os\n","os.getcwd()\n","\n","raw = np.load(datafile_name, allow_pickle = True)\n","obs_dim = len(raw['obs'][0])\n","obs_num = len(raw['obs'])\n","\n","(raw['episode_starts'] == 1).sum()"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"JZ-BkoWTqyex","executionInfo":{"status":"ok","timestamp":1650235350902,"user_tz":240,"elapsed":271,"user":{"displayName":"3 Lin","userId":"16848054003967173878"}},"outputId":"dc06d188-86a0-46b9-bd5f-de20c6c6fbe8"},"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["1000"]},"metadata":{},"execution_count":30}]},{"cell_type":"code","source":["#datafile_name = \"expert_lunarlander_timesteps2e5_episodes10.npz\"\n","#datafile_name = 'expert_cartpole_timesteps2e5_episodes10.npz'\n","#datafile_name = './contrib/expert_replicate/' + 'expert_cartpole_dqn_replicate_episodes1000.npz'\n","datafile_name = './contrib/expert_replicate/' + 'expert_lunarlander_ppo2_replicate_episodes1000.npz'\n","\n","\n","\n","env_name = 'LunarLander-v2'\n","#env_name = 'CartPole-v1'\n","\n","get_train_spurcorr_expert_trajs(datafile_name, env_name = env_name,  _noise = 0.001, num_envs = 2)\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":1000},"id":"u3wguhDuKf78","executionInfo":{"status":"error","timestamp":1650235472953,"user_tz":240,"elapsed":6063,"user":{"displayName":"3 Lin","userId":"16848054003967173878"}},"outputId":"8b1b7cd8-f1a0-4a64-9460-54a57440f177"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stderr","text":["\n","  0%|          | 0/902005 [00:00<?, ?it/s]\u001b[A\n","  0%|          | 3/902005 [00:00<11:15:03, 22.27it/s]\u001b[A\n","  0%|          | 5/902005 [00:00<11:38:04, 21.54it/s]\u001b[A\n","  0%|          | 8/902005 [00:00<11:35:07, 21.63it/s]\u001b[A\n","  0%|          | 11/902005 [00:00<11:29:04, 21.82it/s]\u001b[A\n","  0%|          | 14/902005 [00:00<11:27:21, 21.87it/s]\u001b[A\n","  0%|          | 17/902005 [00:00<11:26:43, 21.89it/s]\u001b[A\n","  0%|          | 19/902005 [00:00<11:52:15, 21.11it/s]\u001b[A\n","  0%|          | 22/902005 [00:01<11:48:50, 21.21it/s]\u001b[A\n","  0%|          | 25/902005 [00:01<11:44:49, 21.33it/s]\u001b[A\n","  0%|          | 28/902005 [00:01<11:48:11, 21.23it/s]\u001b[A\n","  0%|          | 31/902005 [00:01<11:55:39, 21.01it/s]\u001b[A\n","  0%|          | 34/902005 [00:01<11:56:12, 20.99it/s]\u001b[A\n","  0%|          | 37/902005 [00:01<11:51:06, 21.14it/s]\u001b[A\n","  0%|          | 40/902005 [00:01<11:56:15, 20.99it/s]\u001b[A\n","  0%|          | 43/902005 [00:02<11:47:26, 21.25it/s]\u001b[A\n","  0%|          | 46/902005 [00:02<11:44:04, 21.35it/s]\u001b[A\n","  0%|          | 49/902005 [00:02<11:39:50, 21.48it/s]\u001b[A\n","  0%|          | 52/902005 [00:02<11:37:20, 21.56it/s]\u001b[A\n","  0%|          | 55/902005 [00:02<11:34:24, 21.65it/s]\u001b[A\n","  0%|          | 58/902005 [00:02<11:31:46, 21.73it/s]\u001b[A\n","  0%|          | 61/902005 [00:02<11:29:16, 21.81it/s]\u001b[A\n","  0%|          | 64/902005 [00:02<11:38:21, 21.53it/s]\u001b[A\n","  0%|          | 67/902005 [00:03<11:38:58, 21.51it/s]\u001b[A\n","  0%|          | 70/902005 [00:03<11:38:11, 21.53it/s]\u001b[A\n","  0%|          | 73/902005 [00:03<11:33:09, 21.69it/s]\u001b[A\n","  0%|          | 76/902005 [00:03<11:29:16, 21.81it/s]\u001b[A\n","  0%|          | 79/902005 [00:03<11:29:37, 21.80it/s]\u001b[A\n","  0%|          | 82/902005 [00:03<11:26:30, 21.90it/s]\u001b[A\n","  0%|          | 85/902005 [00:03<11:36:50, 21.57it/s]\u001b[A\n","  0%|          | 88/902005 [00:04<11:37:28, 21.55it/s]\u001b[A\n","  0%|          | 91/902005 [00:04<11:36:01, 21.60it/s]\u001b[A\n","  0%|          | 94/902005 [00:04<11:30:36, 21.77it/s]\u001b[A\n","  0%|          | 97/902005 [00:04<11:25:33, 21.93it/s]\u001b[A\n","  0%|          | 100/902005 [00:04<11:26:36, 21.89it/s]\u001b[A\n","  0%|          | 103/902005 [00:04<11:32:59, 21.69it/s]\u001b[A\n","  0%|          | 106/902005 [00:04<11:33:27, 21.68it/s]\u001b[A\n","  0%|          | 109/902005 [00:05<11:27:17, 21.87it/s]\u001b[A\n","  0%|          | 112/902005 [00:05<11:27:04, 21.88it/s]\u001b[A\n","  0%|          | 115/902005 [00:05<11:26:43, 21.89it/s]\u001b[A\n","  0%|          | 118/902005 [00:05<11:22:23, 22.03it/s]\u001b[A\n","  0%|          | 121/902005 [00:05<11:19:36, 22.12it/s]\u001b[A\n","  0%|          | 124/902005 [00:05<11:20:06, 22.10it/s]\u001b[A"]},{"output_type":"error","ename":"KeyboardInterrupt","evalue":"ignored","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)","\u001b[0;32m<ipython-input-36-ecdccd885dfa>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0;31m#env_name = 'CartPole-v1'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 11\u001b[0;31m \u001b[0mget_train_spurcorr_expert_trajs\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdatafile_name\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0menv_name\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0menv_name\u001b[0m\u001b[0;34m,\u001b[0m  \u001b[0m_noise\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0.001\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnum_envs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m","\u001b[0;32m<ipython-input-35-10ee4cb98c15>\u001b[0m in \u001b[0;36mget_train_spurcorr_expert_trajs\u001b[0;34m(datafile_name, env_name, _noise, num_envs)\u001b[0m\n\u001b[1;32m     34\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     35\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtqdm\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobs_num\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 36\u001b[0;31m             \u001b[0mobs_new\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgenerate_spurcorr_obs\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mraw\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'obs'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_mult_factor\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_noise\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0.001\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_idx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mexpert_num\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     37\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     38\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.7/dist-packages/numpy/lib/npyio.py\u001b[0m in \u001b[0;36m__getitem__\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m    260\u001b[0m                 return format.read_array(bytes,\n\u001b[1;32m    261\u001b[0m                                          \u001b[0mallow_pickle\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mallow_pickle\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 262\u001b[0;31m                                          pickle_kwargs=self.pickle_kwargs)\n\u001b[0m\u001b[1;32m    263\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    264\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzip\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.7/dist-packages/numpy/lib/format.py\u001b[0m in \u001b[0;36mread_array\u001b[0;34m(fp, allow_pickle, pickle_kwargs)\u001b[0m\n\u001b[1;32m    775\u001b[0m                     \u001b[0mread_count\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmax_read_count\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcount\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    776\u001b[0m                     \u001b[0mread_size\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mread_count\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitemsize\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 777\u001b[0;31m                     \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_read_bytes\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfp\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mread_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"array data\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    778\u001b[0m                     array[i:i+read_count] = numpy.frombuffer(data, dtype=dtype,\n\u001b[1;32m    779\u001b[0m                                                              count=read_count)\n","\u001b[0;32m/usr/local/lib/python3.7/dist-packages/numpy/lib/format.py\u001b[0m in \u001b[0;36m_read_bytes\u001b[0;34m(fp, size, error_template)\u001b[0m\n\u001b[1;32m    904\u001b[0m         \u001b[0;31m# done about that.  note that regular files can't be non-blocking\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    905\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 906\u001b[0;31m             \u001b[0mr\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msize\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    907\u001b[0m             \u001b[0mdata\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mr\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    908\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mr\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m0\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0msize\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/lib/python3.7/zipfile.py\u001b[0m in \u001b[0;36mread\u001b[0;34m(self, n)\u001b[0m\n\u001b[1;32m    928\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_offset\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    929\u001b[0m         \u001b[0;32mwhile\u001b[0m \u001b[0mn\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m0\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_eof\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 930\u001b[0;31m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_read1\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mn\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    931\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mn\u001b[0m \u001b[0;34m<\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    932\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_readbuffer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/lib/python3.7/zipfile.py\u001b[0m in \u001b[0;36m_read1\u001b[0;34m(self, n)\u001b[0m\n\u001b[1;32m   1018\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_left\u001b[0m \u001b[0;34m<=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1019\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_eof\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1020\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_update_crc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1021\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1022\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/lib/python3.7/zipfile.py\u001b[0m in \u001b[0;36m_update_crc\u001b[0;34m(self, newdata)\u001b[0m\n\u001b[1;32m    943\u001b[0m             \u001b[0;31m# No need to compute the CRC if we don't have a reference value\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    944\u001b[0m             \u001b[0;32mreturn\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 945\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_running_crc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcrc32\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnewdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_running_crc\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    946\u001b[0m         \u001b[0;31m# Check the CRC if we're at the end of the file\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    947\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_eof\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_running_crc\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_expected_crc\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mKeyboardInterrupt\u001b[0m: "]}]},{"cell_type":"code","source":[""],"metadata":{"id":"ORPRPX6naoql"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["data1 = np.load(\"./volume/CartPole-v1/expert_trajs_1.npy\", allow_pickle = True)#[()]#[\"trajs\"]\n","for key in data1[()]:\n","  print(key)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"NYg6Et3uaoop","executionInfo":{"status":"ok","timestamp":1650168889275,"user_tz":240,"elapsed":388,"user":{"displayName":"3 Lin","userId":"16848054003967173878"}},"outputId":"a23c844c-9656-4437-9cef-ead6d912d4e6"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["trajs\n"]}]},{"cell_type":"code","source":[""],"metadata":{"id":"dq08yZ2C6EhZ"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[""],"metadata":{"id":"jT1UWt646AK9"},"execution_count":null,"outputs":[]}]}