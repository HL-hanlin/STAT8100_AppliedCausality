{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"CCIL_Apr22_cartpole.ipynb","provenance":[],"collapsed_sections":[]},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"},"accelerator":"GPU"},"cells":[{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"cXttaQe-507l","executionInfo":{"status":"ok","timestamp":1650815270432,"user_tz":240,"elapsed":22707,"user":{"displayName":"2 Lin","userId":"15119774230930098070"}},"outputId":"3446c0c9-432e-4180-c1e0-cd233f901c8a"},"outputs":[{"output_type":"stream","name":"stdout","text":["Mounted at /content/drive\n"]}],"source":["from google.colab import drive\n","drive.mount('/content/drive')\n","\n","\n","import os\n","\n","import torch\n","os.chdir('/content/drive/MyDrive/ImitationLearning/Invariant-Causal-Imitation-Learning-main/')\n"]},{"cell_type":"markdown","source":["# load"],"metadata":{"id":"CMy2nIGPEiAs"}},{"cell_type":"code","source":["!pip install mpi4py \n","!pip install box2d-py\n","!pip install box2d \n","!pip3 install gym[Box_2D] \n","!pip install gym==0.17.2 -qqq\n","!pip install numpy~=1.18.2 -qqq\n","!pip install pandas~=1.0.4 -qqq\n","!pip install PyYAML~=5.4.1 -qqq\n","!pip install scikit-learn~=0.22.2 -qqq\n","!pip install scipy~=1.1.0 -qqq\n","!pip install stable-baselines~=2.10.1 -qqq\n","!pip install tensorflow~=1.15.0 -qqq\n","!pip install torch>=1.6.0 -qqq\n","!pip install tqdm~=4.32.1 -qqq\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"AhOh_Fj16Eow","executionInfo":{"status":"ok","timestamp":1650815548282,"user_tz":240,"elapsed":138100,"user":{"displayName":"2 Lin","userId":"15119774230930098070"}},"outputId":"988b7cef-d668-43b9-c26d-5bbb12291040"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Collecting mpi4py\n","  Downloading mpi4py-3.1.3.tar.gz (2.5 MB)\n","\u001b[?25l\r\u001b[K     |▏                               | 10 kB 32.8 MB/s eta 0:00:01\r\u001b[K     |▎                               | 20 kB 40.2 MB/s eta 0:00:01\r\u001b[K     |▍                               | 30 kB 39.2 MB/s eta 0:00:01\r\u001b[K     |▌                               | 40 kB 26.4 MB/s eta 0:00:01\r\u001b[K     |▋                               | 51 kB 20.8 MB/s eta 0:00:01\r\u001b[K     |▉                               | 61 kB 23.8 MB/s eta 0:00:01\r\u001b[K     |█                               | 71 kB 23.9 MB/s eta 0:00:01\r\u001b[K     |█                               | 81 kB 25.8 MB/s eta 0:00:01\r\u001b[K     |█▏                              | 92 kB 26.5 MB/s eta 0:00:01\r\u001b[K     |█▎                              | 102 kB 28.1 MB/s eta 0:00:01\r\u001b[K     |█▌                              | 112 kB 28.1 MB/s eta 0:00:01\r\u001b[K     |█▋                              | 122 kB 28.1 MB/s eta 0:00:01\r\u001b[K     |█▊                              | 133 kB 28.1 MB/s eta 0:00:01\r\u001b[K     |█▉                              | 143 kB 28.1 MB/s eta 0:00:01\r\u001b[K     |██                              | 153 kB 28.1 MB/s eta 0:00:01\r\u001b[K     |██▏                             | 163 kB 28.1 MB/s eta 0:00:01\r\u001b[K     |██▎                             | 174 kB 28.1 MB/s eta 0:00:01\r\u001b[K     |██▍                             | 184 kB 28.1 MB/s eta 0:00:01\r\u001b[K     |██▌                             | 194 kB 28.1 MB/s eta 0:00:01\r\u001b[K     |██▋                             | 204 kB 28.1 MB/s eta 0:00:01\r\u001b[K     |██▉                             | 215 kB 28.1 MB/s eta 0:00:01\r\u001b[K     |███                             | 225 kB 28.1 MB/s eta 0:00:01\r\u001b[K     |███                             | 235 kB 28.1 MB/s eta 0:00:01\r\u001b[K     |███▏                            | 245 kB 28.1 MB/s eta 0:00:01\r\u001b[K     |███▎                            | 256 kB 28.1 MB/s eta 0:00:01\r\u001b[K     |███▌                            | 266 kB 28.1 MB/s eta 0:00:01\r\u001b[K     |███▋                            | 276 kB 28.1 MB/s eta 0:00:01\r\u001b[K     |███▊                            | 286 kB 28.1 MB/s eta 0:00:01\r\u001b[K     |███▉                            | 296 kB 28.1 MB/s eta 0:00:01\r\u001b[K     |████                            | 307 kB 28.1 MB/s eta 0:00:01\r\u001b[K     |████▏                           | 317 kB 28.1 MB/s eta 0:00:01\r\u001b[K     |████▎                           | 327 kB 28.1 MB/s eta 0:00:01\r\u001b[K     |████▍                           | 337 kB 28.1 MB/s eta 0:00:01\r\u001b[K     |████▌                           | 348 kB 28.1 MB/s eta 0:00:01\r\u001b[K     |████▋                           | 358 kB 28.1 MB/s eta 0:00:01\r\u001b[K     |████▉                           | 368 kB 28.1 MB/s eta 0:00:01\r\u001b[K     |█████                           | 378 kB 28.1 MB/s eta 0:00:01\r\u001b[K     |█████                           | 389 kB 28.1 MB/s eta 0:00:01\r\u001b[K     |█████▏                          | 399 kB 28.1 MB/s eta 0:00:01\r\u001b[K     |█████▎                          | 409 kB 28.1 MB/s eta 0:00:01\r\u001b[K     |█████▌                          | 419 kB 28.1 MB/s eta 0:00:01\r\u001b[K     |█████▋                          | 430 kB 28.1 MB/s eta 0:00:01\r\u001b[K     |█████▊                          | 440 kB 28.1 MB/s eta 0:00:01\r\u001b[K     |█████▉                          | 450 kB 28.1 MB/s eta 0:00:01\r\u001b[K     |██████                          | 460 kB 28.1 MB/s eta 0:00:01\r\u001b[K     |██████▏                         | 471 kB 28.1 MB/s eta 0:00:01\r\u001b[K     |██████▎                         | 481 kB 28.1 MB/s eta 0:00:01\r\u001b[K     |██████▍                         | 491 kB 28.1 MB/s eta 0:00:01\r\u001b[K     |██████▌                         | 501 kB 28.1 MB/s eta 0:00:01\r\u001b[K     |██████▋                         | 512 kB 28.1 MB/s eta 0:00:01\r\u001b[K     |██████▉                         | 522 kB 28.1 MB/s eta 0:00:01\r\u001b[K     |███████                         | 532 kB 28.1 MB/s eta 0:00:01\r\u001b[K     |███████                         | 542 kB 28.1 MB/s eta 0:00:01\r\u001b[K     |███████▏                        | 552 kB 28.1 MB/s eta 0:00:01\r\u001b[K     |███████▎                        | 563 kB 28.1 MB/s eta 0:00:01\r\u001b[K     |███████▌                        | 573 kB 28.1 MB/s eta 0:00:01\r\u001b[K     |███████▋                        | 583 kB 28.1 MB/s eta 0:00:01\r\u001b[K     |███████▊                        | 593 kB 28.1 MB/s eta 0:00:01\r\u001b[K     |███████▉                        | 604 kB 28.1 MB/s eta 0:00:01\r\u001b[K     |████████                        | 614 kB 28.1 MB/s eta 0:00:01\r\u001b[K     |████████▏                       | 624 kB 28.1 MB/s eta 0:00:01\r\u001b[K     |████████▎                       | 634 kB 28.1 MB/s eta 0:00:01\r\u001b[K     |████████▍                       | 645 kB 28.1 MB/s eta 0:00:01\r\u001b[K     |████████▌                       | 655 kB 28.1 MB/s eta 0:00:01\r\u001b[K     |████████▋                       | 665 kB 28.1 MB/s eta 0:00:01\r\u001b[K     |████████▉                       | 675 kB 28.1 MB/s eta 0:00:01\r\u001b[K     |█████████                       | 686 kB 28.1 MB/s eta 0:00:01\r\u001b[K     |█████████                       | 696 kB 28.1 MB/s eta 0:00:01\r\u001b[K     |█████████▏                      | 706 kB 28.1 MB/s eta 0:00:01\r\u001b[K     |█████████▎                      | 716 kB 28.1 MB/s eta 0:00:01\r\u001b[K     |█████████▌                      | 727 kB 28.1 MB/s eta 0:00:01\r\u001b[K     |█████████▋                      | 737 kB 28.1 MB/s eta 0:00:01\r\u001b[K     |█████████▊                      | 747 kB 28.1 MB/s eta 0:00:01\r\u001b[K     |█████████▉                      | 757 kB 28.1 MB/s eta 0:00:01\r\u001b[K     |██████████                      | 768 kB 28.1 MB/s eta 0:00:01\r\u001b[K     |██████████▏                     | 778 kB 28.1 MB/s eta 0:00:01\r\u001b[K     |██████████▎                     | 788 kB 28.1 MB/s eta 0:00:01\r\u001b[K     |██████████▍                     | 798 kB 28.1 MB/s eta 0:00:01\r\u001b[K     |██████████▌                     | 808 kB 28.1 MB/s eta 0:00:01\r\u001b[K     |██████████▋                     | 819 kB 28.1 MB/s eta 0:00:01\r\u001b[K     |██████████▉                     | 829 kB 28.1 MB/s eta 0:00:01\r\u001b[K     |███████████                     | 839 kB 28.1 MB/s eta 0:00:01\r\u001b[K     |███████████                     | 849 kB 28.1 MB/s eta 0:00:01\r\u001b[K     |███████████▏                    | 860 kB 28.1 MB/s eta 0:00:01\r\u001b[K     |███████████▎                    | 870 kB 28.1 MB/s eta 0:00:01\r\u001b[K     |███████████▌                    | 880 kB 28.1 MB/s eta 0:00:01\r\u001b[K     |███████████▋                    | 890 kB 28.1 MB/s eta 0:00:01\r\u001b[K     |███████████▊                    | 901 kB 28.1 MB/s eta 0:00:01\r\u001b[K     |███████████▉                    | 911 kB 28.1 MB/s eta 0:00:01\r\u001b[K     |████████████                    | 921 kB 28.1 MB/s eta 0:00:01\r\u001b[K     |████████████▏                   | 931 kB 28.1 MB/s eta 0:00:01\r\u001b[K     |████████████▎                   | 942 kB 28.1 MB/s eta 0:00:01\r\u001b[K     |████████████▍                   | 952 kB 28.1 MB/s eta 0:00:01\r\u001b[K     |████████████▌                   | 962 kB 28.1 MB/s eta 0:00:01\r\u001b[K     |████████████▋                   | 972 kB 28.1 MB/s eta 0:00:01\r\u001b[K     |████████████▉                   | 983 kB 28.1 MB/s eta 0:00:01\r\u001b[K     |█████████████                   | 993 kB 28.1 MB/s eta 0:00:01\r\u001b[K     |█████████████                   | 1.0 MB 28.1 MB/s eta 0:00:01\r\u001b[K     |█████████████▏                  | 1.0 MB 28.1 MB/s eta 0:00:01\r\u001b[K     |█████████████▎                  | 1.0 MB 28.1 MB/s eta 0:00:01\r\u001b[K     |█████████████▌                  | 1.0 MB 28.1 MB/s eta 0:00:01\r\u001b[K     |█████████████▋                  | 1.0 MB 28.1 MB/s eta 0:00:01\r\u001b[K     |█████████████▊                  | 1.1 MB 28.1 MB/s eta 0:00:01\r\u001b[K     |█████████████▉                  | 1.1 MB 28.1 MB/s eta 0:00:01\r\u001b[K     |██████████████                  | 1.1 MB 28.1 MB/s eta 0:00:01\r\u001b[K     |██████████████▏                 | 1.1 MB 28.1 MB/s eta 0:00:01\r\u001b[K     |██████████████▎                 | 1.1 MB 28.1 MB/s eta 0:00:01\r\u001b[K     |██████████████▍                 | 1.1 MB 28.1 MB/s eta 0:00:01\r\u001b[K     |██████████████▌                 | 1.1 MB 28.1 MB/s eta 0:00:01\r\u001b[K     |██████████████▋                 | 1.1 MB 28.1 MB/s eta 0:00:01\r\u001b[K     |██████████████▊                 | 1.1 MB 28.1 MB/s eta 0:00:01\r\u001b[K     |███████████████                 | 1.1 MB 28.1 MB/s eta 0:00:01\r\u001b[K     |███████████████                 | 1.2 MB 28.1 MB/s eta 0:00:01\r\u001b[K     |███████████████▏                | 1.2 MB 28.1 MB/s eta 0:00:01\r\u001b[K     |███████████████▎                | 1.2 MB 28.1 MB/s eta 0:00:01\r\u001b[K     |███████████████▍                | 1.2 MB 28.1 MB/s eta 0:00:01\r\u001b[K     |███████████████▋                | 1.2 MB 28.1 MB/s eta 0:00:01\r\u001b[K     |███████████████▊                | 1.2 MB 28.1 MB/s eta 0:00:01\r\u001b[K     |███████████████▉                | 1.2 MB 28.1 MB/s eta 0:00:01\r\u001b[K     |████████████████                | 1.2 MB 28.1 MB/s eta 0:00:01\r\u001b[K     |████████████████                | 1.2 MB 28.1 MB/s eta 0:00:01\r\u001b[K     |████████████████▎               | 1.2 MB 28.1 MB/s eta 0:00:01\r\u001b[K     |████████████████▍               | 1.3 MB 28.1 MB/s eta 0:00:01\r\u001b[K     |████████████████▌               | 1.3 MB 28.1 MB/s eta 0:00:01\r\u001b[K     |████████████████▋               | 1.3 MB 28.1 MB/s eta 0:00:01\r\u001b[K     |████████████████▊               | 1.3 MB 28.1 MB/s eta 0:00:01\r\u001b[K     |█████████████████               | 1.3 MB 28.1 MB/s eta 0:00:01\r\u001b[K     |█████████████████               | 1.3 MB 28.1 MB/s eta 0:00:01\r\u001b[K     |█████████████████▏              | 1.3 MB 28.1 MB/s eta 0:00:01\r\u001b[K     |█████████████████▎              | 1.3 MB 28.1 MB/s eta 0:00:01\r\u001b[K     |█████████████████▍              | 1.3 MB 28.1 MB/s eta 0:00:01\r\u001b[K     |█████████████████▋              | 1.4 MB 28.1 MB/s eta 0:00:01\r\u001b[K     |█████████████████▊              | 1.4 MB 28.1 MB/s eta 0:00:01\r\u001b[K     |█████████████████▉              | 1.4 MB 28.1 MB/s eta 0:00:01\r\u001b[K     |██████████████████              | 1.4 MB 28.1 MB/s eta 0:00:01\r\u001b[K     |██████████████████              | 1.4 MB 28.1 MB/s eta 0:00:01\r\u001b[K     |██████████████████▎             | 1.4 MB 28.1 MB/s eta 0:00:01\r\u001b[K     |██████████████████▍             | 1.4 MB 28.1 MB/s eta 0:00:01\r\u001b[K     |██████████████████▌             | 1.4 MB 28.1 MB/s eta 0:00:01\r\u001b[K     |██████████████████▋             | 1.4 MB 28.1 MB/s eta 0:00:01\r\u001b[K     |██████████████████▊             | 1.4 MB 28.1 MB/s eta 0:00:01\r\u001b[K     |███████████████████             | 1.5 MB 28.1 MB/s eta 0:00:01\r\u001b[K     |███████████████████             | 1.5 MB 28.1 MB/s eta 0:00:01\r\u001b[K     |███████████████████▏            | 1.5 MB 28.1 MB/s eta 0:00:01\r\u001b[K     |███████████████████▎            | 1.5 MB 28.1 MB/s eta 0:00:01\r\u001b[K     |███████████████████▍            | 1.5 MB 28.1 MB/s eta 0:00:01\r\u001b[K     |███████████████████▋            | 1.5 MB 28.1 MB/s eta 0:00:01\r\u001b[K     |███████████████████▊            | 1.5 MB 28.1 MB/s eta 0:00:01\r\u001b[K     |███████████████████▉            | 1.5 MB 28.1 MB/s eta 0:00:01\r\u001b[K     |████████████████████            | 1.5 MB 28.1 MB/s eta 0:00:01\r\u001b[K     |████████████████████            | 1.5 MB 28.1 MB/s eta 0:00:01\r\u001b[K     |████████████████████▎           | 1.6 MB 28.1 MB/s eta 0:00:01\r\u001b[K     |████████████████████▍           | 1.6 MB 28.1 MB/s eta 0:00:01\r\u001b[K     |████████████████████▌           | 1.6 MB 28.1 MB/s eta 0:00:01\r\u001b[K     |████████████████████▋           | 1.6 MB 28.1 MB/s eta 0:00:01\r\u001b[K     |████████████████████▊           | 1.6 MB 28.1 MB/s eta 0:00:01\r\u001b[K     |█████████████████████           | 1.6 MB 28.1 MB/s eta 0:00:01\r\u001b[K     |█████████████████████           | 1.6 MB 28.1 MB/s eta 0:00:01\r\u001b[K     |█████████████████████▏          | 1.6 MB 28.1 MB/s eta 0:00:01\r\u001b[K     |█████████████████████▎          | 1.6 MB 28.1 MB/s eta 0:00:01\r\u001b[K     |█████████████████████▍          | 1.6 MB 28.1 MB/s eta 0:00:01\r\u001b[K     |█████████████████████▋          | 1.7 MB 28.1 MB/s eta 0:00:01\r\u001b[K     |█████████████████████▊          | 1.7 MB 28.1 MB/s eta 0:00:01\r\u001b[K     |█████████████████████▉          | 1.7 MB 28.1 MB/s eta 0:00:01\r\u001b[K     |██████████████████████          | 1.7 MB 28.1 MB/s eta 0:00:01\r\u001b[K     |██████████████████████          | 1.7 MB 28.1 MB/s eta 0:00:01\r\u001b[K     |██████████████████████▎         | 1.7 MB 28.1 MB/s eta 0:00:01\r\u001b[K     |██████████████████████▍         | 1.7 MB 28.1 MB/s eta 0:00:01\r\u001b[K     |██████████████████████▌         | 1.7 MB 28.1 MB/s eta 0:00:01\r\u001b[K     |██████████████████████▋         | 1.7 MB 28.1 MB/s eta 0:00:01\r\u001b[K     |██████████████████████▊         | 1.8 MB 28.1 MB/s eta 0:00:01\r\u001b[K     |███████████████████████         | 1.8 MB 28.1 MB/s eta 0:00:01\r\u001b[K     |███████████████████████         | 1.8 MB 28.1 MB/s eta 0:00:01\r\u001b[K     |███████████████████████▏        | 1.8 MB 28.1 MB/s eta 0:00:01\r\u001b[K     |███████████████████████▎        | 1.8 MB 28.1 MB/s eta 0:00:01\r\u001b[K     |███████████████████████▍        | 1.8 MB 28.1 MB/s eta 0:00:01\r\u001b[K     |███████████████████████▋        | 1.8 MB 28.1 MB/s eta 0:00:01\r\u001b[K     |███████████████████████▊        | 1.8 MB 28.1 MB/s eta 0:00:01\r\u001b[K     |███████████████████████▉        | 1.8 MB 28.1 MB/s eta 0:00:01\r\u001b[K     |████████████████████████        | 1.8 MB 28.1 MB/s eta 0:00:01\r\u001b[K     |████████████████████████        | 1.9 MB 28.1 MB/s eta 0:00:01\r\u001b[K     |████████████████████████▎       | 1.9 MB 28.1 MB/s eta 0:00:01\r\u001b[K     |████████████████████████▍       | 1.9 MB 28.1 MB/s eta 0:00:01\r\u001b[K     |████████████████████████▌       | 1.9 MB 28.1 MB/s eta 0:00:01\r\u001b[K     |████████████████████████▋       | 1.9 MB 28.1 MB/s eta 0:00:01\r\u001b[K     |████████████████████████▊       | 1.9 MB 28.1 MB/s eta 0:00:01\r\u001b[K     |█████████████████████████       | 1.9 MB 28.1 MB/s eta 0:00:01\r\u001b[K     |█████████████████████████       | 1.9 MB 28.1 MB/s eta 0:00:01\r\u001b[K     |█████████████████████████▏      | 1.9 MB 28.1 MB/s eta 0:00:01\r\u001b[K     |█████████████████████████▎      | 1.9 MB 28.1 MB/s eta 0:00:01\r\u001b[K     |█████████████████████████▍      | 2.0 MB 28.1 MB/s eta 0:00:01\r\u001b[K     |█████████████████████████▋      | 2.0 MB 28.1 MB/s eta 0:00:01\r\u001b[K     |█████████████████████████▊      | 2.0 MB 28.1 MB/s eta 0:00:01\r\u001b[K     |█████████████████████████▉      | 2.0 MB 28.1 MB/s eta 0:00:01\r\u001b[K     |██████████████████████████      | 2.0 MB 28.1 MB/s eta 0:00:01\r\u001b[K     |██████████████████████████      | 2.0 MB 28.1 MB/s eta 0:00:01\r\u001b[K     |██████████████████████████▎     | 2.0 MB 28.1 MB/s eta 0:00:01\r\u001b[K     |██████████████████████████▍     | 2.0 MB 28.1 MB/s eta 0:00:01\r\u001b[K     |██████████████████████████▌     | 2.0 MB 28.1 MB/s eta 0:00:01\r\u001b[K     |██████████████████████████▋     | 2.0 MB 28.1 MB/s eta 0:00:01\r\u001b[K     |██████████████████████████▊     | 2.1 MB 28.1 MB/s eta 0:00:01\r\u001b[K     |███████████████████████████     | 2.1 MB 28.1 MB/s eta 0:00:01\r\u001b[K     |███████████████████████████     | 2.1 MB 28.1 MB/s eta 0:00:01\r\u001b[K     |███████████████████████████▏    | 2.1 MB 28.1 MB/s eta 0:00:01\r\u001b[K     |███████████████████████████▎    | 2.1 MB 28.1 MB/s eta 0:00:01\r\u001b[K     |███████████████████████████▍    | 2.1 MB 28.1 MB/s eta 0:00:01\r\u001b[K     |███████████████████████████▋    | 2.1 MB 28.1 MB/s eta 0:00:01\r\u001b[K     |███████████████████████████▊    | 2.1 MB 28.1 MB/s eta 0:00:01\r\u001b[K     |███████████████████████████▉    | 2.1 MB 28.1 MB/s eta 0:00:01\r\u001b[K     |████████████████████████████    | 2.2 MB 28.1 MB/s eta 0:00:01\r\u001b[K     |████████████████████████████    | 2.2 MB 28.1 MB/s eta 0:00:01\r\u001b[K     |████████████████████████████▎   | 2.2 MB 28.1 MB/s eta 0:00:01\r\u001b[K     |████████████████████████████▍   | 2.2 MB 28.1 MB/s eta 0:00:01\r\u001b[K     |████████████████████████████▌   | 2.2 MB 28.1 MB/s eta 0:00:01\r\u001b[K     |████████████████████████████▋   | 2.2 MB 28.1 MB/s eta 0:00:01\r\u001b[K     |████████████████████████████▊   | 2.2 MB 28.1 MB/s eta 0:00:01\r\u001b[K     |█████████████████████████████   | 2.2 MB 28.1 MB/s eta 0:00:01\r\u001b[K     |█████████████████████████████   | 2.2 MB 28.1 MB/s eta 0:00:01\r\u001b[K     |█████████████████████████████▏  | 2.2 MB 28.1 MB/s eta 0:00:01\r\u001b[K     |█████████████████████████████▎  | 2.3 MB 28.1 MB/s eta 0:00:01\r\u001b[K     |█████████████████████████████▍  | 2.3 MB 28.1 MB/s eta 0:00:01\r\u001b[K     |█████████████████████████████▌  | 2.3 MB 28.1 MB/s eta 0:00:01\r\u001b[K     |█████████████████████████████▊  | 2.3 MB 28.1 MB/s eta 0:00:01\r\u001b[K     |█████████████████████████████▉  | 2.3 MB 28.1 MB/s eta 0:00:01\r\u001b[K     |██████████████████████████████  | 2.3 MB 28.1 MB/s eta 0:00:01\r\u001b[K     |██████████████████████████████  | 2.3 MB 28.1 MB/s eta 0:00:01\r\u001b[K     |██████████████████████████████▏ | 2.3 MB 28.1 MB/s eta 0:00:01\r\u001b[K     |██████████████████████████████▍ | 2.3 MB 28.1 MB/s eta 0:00:01\r\u001b[K     |██████████████████████████████▌ | 2.3 MB 28.1 MB/s eta 0:00:01\r\u001b[K     |██████████████████████████████▋ | 2.4 MB 28.1 MB/s eta 0:00:01\r\u001b[K     |██████████████████████████████▊ | 2.4 MB 28.1 MB/s eta 0:00:01\r\u001b[K     |██████████████████████████████▉ | 2.4 MB 28.1 MB/s eta 0:00:01\r\u001b[K     |███████████████████████████████ | 2.4 MB 28.1 MB/s eta 0:00:01\r\u001b[K     |███████████████████████████████▏| 2.4 MB 28.1 MB/s eta 0:00:01\r\u001b[K     |███████████████████████████████▎| 2.4 MB 28.1 MB/s eta 0:00:01\r\u001b[K     |███████████████████████████████▍| 2.4 MB 28.1 MB/s eta 0:00:01\r\u001b[K     |███████████████████████████████▌| 2.4 MB 28.1 MB/s eta 0:00:01\r\u001b[K     |███████████████████████████████▊| 2.4 MB 28.1 MB/s eta 0:00:01\r\u001b[K     |███████████████████████████████▉| 2.4 MB 28.1 MB/s eta 0:00:01\r\u001b[K     |████████████████████████████████| 2.5 MB 28.1 MB/s eta 0:00:01\r\u001b[K     |████████████████████████████████| 2.5 MB 28.1 MB/s \n","\u001b[?25h  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n","  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n","    Preparing wheel metadata ... \u001b[?25l\u001b[?25hdone\n","Building wheels for collected packages: mpi4py\n","  Building wheel for mpi4py (PEP 517) ... \u001b[?25l\u001b[?25hdone\n","  Created wheel for mpi4py: filename=mpi4py-3.1.3-cp37-cp37m-linux_x86_64.whl size=2185298 sha256=80fd80e70ff0096b90ecda97cc2116cddf4172936912ac61ccd5824da6fc920f\n","  Stored in directory: /root/.cache/pip/wheels/7a/07/14/6a0c63fa2c6e473c6edc40985b7d89f05c61ff25ee7f0ad9ac\n","Successfully built mpi4py\n","Installing collected packages: mpi4py\n","Successfully installed mpi4py-3.1.3\n","Collecting box2d-py\n","  Downloading box2d_py-2.3.8-cp37-cp37m-manylinux1_x86_64.whl (448 kB)\n","\u001b[K     |████████████████████████████████| 448 kB 31.6 MB/s \n","\u001b[?25hInstalling collected packages: box2d-py\n","Successfully installed box2d-py-2.3.8\n","Collecting box2d\n","  Downloading Box2D-2.3.10-cp37-cp37m-manylinux1_x86_64.whl (1.3 MB)\n","\u001b[K     |████████████████████████████████| 1.3 MB 31.4 MB/s \n","\u001b[?25hInstalling collected packages: box2d\n","Successfully installed box2d-2.3.10\n","Requirement already satisfied: gym[Box_2D] in /usr/local/lib/python3.7/dist-packages (0.17.3)\n","\u001b[33mWARNING: gym 0.17.3 does not provide the extra 'box_2d'\u001b[0m\n","Requirement already satisfied: scipy in /usr/local/lib/python3.7/dist-packages (from gym[Box_2D]) (1.4.1)\n","Requirement already satisfied: cloudpickle<1.7.0,>=1.2.0 in /usr/local/lib/python3.7/dist-packages (from gym[Box_2D]) (1.3.0)\n","Requirement already satisfied: numpy>=1.10.4 in /usr/local/lib/python3.7/dist-packages (from gym[Box_2D]) (1.21.6)\n","Requirement already satisfied: pyglet<=1.5.0,>=1.4.0 in /usr/local/lib/python3.7/dist-packages (from gym[Box_2D]) (1.5.0)\n","Requirement already satisfied: future in /usr/local/lib/python3.7/dist-packages (from pyglet<=1.5.0,>=1.4.0->gym[Box_2D]) (0.16.0)\n","\u001b[K     |████████████████████████████████| 1.6 MB 23.5 MB/s \n","\u001b[?25h  Building wheel for gym (setup.py) ... \u001b[?25l\u001b[?25hdone\n","\u001b[K     |████████████████████████████████| 20.1 MB 1.1 MB/s \n","\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n","tensorflow 2.8.0 requires tf-estimator-nightly==2.8.0.dev2021122109, which is not installed.\n","tensorflow 2.8.0 requires numpy>=1.20, but you have numpy 1.18.5 which is incompatible.\n","tables 3.7.0 requires numpy>=1.19.0, but you have numpy 1.18.5 which is incompatible.\n","jaxlib 0.3.2+cuda11.cudnn805 requires numpy>=1.19, but you have numpy 1.18.5 which is incompatible.\n","jax 0.3.4 requires numpy>=1.19, but you have numpy 1.18.5 which is incompatible.\n","datascience 0.10.6 requires folium==0.2.1, but you have folium 0.8.3 which is incompatible.\n","albumentations 0.1.12 requires imgaug<0.2.7,>=0.2.5, but you have imgaug 0.2.9 which is incompatible.\u001b[0m\n","\u001b[K     |████████████████████████████████| 10.1 MB 29.8 MB/s \n","\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n","google-colab 1.0.0 requires pandas>=1.1.0; python_version >= \"3.0\", but you have pandas 1.0.5 which is incompatible.\u001b[0m\n","\u001b[K     |████████████████████████████████| 636 kB 29.3 MB/s \n","\u001b[K     |████████████████████████████████| 7.1 MB 27.4 MB/s \n","\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n","yellowbrick 1.4 requires scikit-learn>=1.0.0, but you have scikit-learn 0.22.2.post1 which is incompatible.\n","imbalanced-learn 0.8.1 requires scikit-learn>=0.24, but you have scikit-learn 0.22.2.post1 which is incompatible.\u001b[0m\n","\u001b[K     |████████████████████████████████| 31.2 MB 1.4 MB/s \n","\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n","yellowbrick 1.4 requires scikit-learn>=1.0.0, but you have scikit-learn 0.22.2.post1 which is incompatible.\n","pymc3 3.11.4 requires scipy>=1.2.0, but you have scipy 1.1.0 which is incompatible.\n","plotnine 0.6.0 requires scipy>=1.2.0, but you have scipy 1.1.0 which is incompatible.\n","jaxlib 0.3.2+cuda11.cudnn805 requires numpy>=1.19, but you have numpy 1.18.5 which is incompatible.\n","jax 0.3.4 requires numpy>=1.19, but you have numpy 1.18.5 which is incompatible.\n","jax 0.3.4 requires scipy>=1.2.1, but you have scipy 1.1.0 which is incompatible.\n","imbalanced-learn 0.8.1 requires scikit-learn>=0.24, but you have scikit-learn 0.22.2.post1 which is incompatible.\n","albumentations 0.1.12 requires imgaug<0.2.7,>=0.2.5, but you have imgaug 0.2.9 which is incompatible.\u001b[0m\n","\u001b[K     |████████████████████████████████| 240 kB 29.0 MB/s \n","\u001b[K     |████████████████████████████████| 110.5 MB 1.5 kB/s \n","\u001b[K     |████████████████████████████████| 50 kB 7.5 MB/s \n","\u001b[K     |████████████████████████████████| 2.9 MB 56.0 MB/s \n","\u001b[K     |████████████████████████████████| 503 kB 71.4 MB/s \n","\u001b[K     |████████████████████████████████| 3.8 MB 62.1 MB/s \n","\u001b[?25h  Building wheel for gast (setup.py) ... \u001b[?25l\u001b[?25hdone\n","\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n","tensorflow-probability 0.16.0 requires gast>=0.3.2, but you have gast 0.2.2 which is incompatible.\n","kapre 0.3.7 requires tensorflow>=2.0.0, but you have tensorflow 1.15.5 which is incompatible.\u001b[0m\n","\u001b[K     |████████████████████████████████| 50 kB 6.9 MB/s \n","\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n","spacy 2.2.4 requires tqdm<5.0.0,>=4.38.0, but you have tqdm 4.32.2 which is incompatible.\n","panel 0.12.1 requires tqdm>=4.48.0, but you have tqdm 4.32.2 which is incompatible.\n","fbprophet 0.7.1 requires tqdm>=4.36.1, but you have tqdm 4.32.2 which is incompatible.\u001b[0m\n","\u001b[?25h"]}]},{"cell_type":"markdown","source":["#config"],"metadata":{"id":"UoOvpQGcfM2h"}},{"cell_type":"code","source":["\n","config = {\n","    \"ENV\": \"CartPole-v1\",\n","    \"ALG\": \"BCIRMStudent_Apr17\",\n","    \"NUM_TRAJS_GIVEN\": 20, #\n","    \"NUM_TRAINING_ENVS\": 2,\n","    \"NOISE_DIM\": 4,\n","    \"REP_SIZE\": 16,\n","    \"TRAJ_SHIFT\": 20, # 20,\n","    \"SAMPLING_RATE\": 5,\n","    \"NUM_STEPS_TRAIN\": 10000,\n","    \"NUM_TRAJS_VALID\": 100,\n","    \"NUM_REPETITIONS\": 15,\n","    \"BATCH_SIZE\": 64,\n","    \"MLP_WIDTHS\": 64,\n","    \"ADAM_ALPHA\": 1e-3,\n","    \"SGLD_BUFFER_SIZE\": 10000,\n","    \"SGLD_LEARN_RATE\": 0.01,\n","    \"SGLD_NOISE_COEF\": 0.01,\n","    \"SGLD_NUM_STEPS\": 100,\n","    \"SGLD_REINIT_FREQ\": 0.05,\n","    \"NUM_STEPS_TRAIN_ENERGY_MODEL\": 1000,\n","    #\"NUM_STEPS_TRAIN_VAE_MODEL\": 20000,\n","    'TRIAL': 0\n","}\n","\n","\n","#config['ENV'] = \"LunarLander-v2\"\n","config['ENV'] = \"CartPole-v1\"\n","\n","#config['METHOD'] = \"BCIRM\"\n","config['METHOD'] = \"CCIL\"\n","\n","\n","\n","\n","if config['ENV'] == \"CartPole-v1\":\n","    config[\"REP_SIZE\"] = 16\n","\n","\n","\n","if config['METHOD'] == 'BCIRM':\n","    config['l2_regularizer_weight'] = 0.001\n","    config['penalty_weight'] = 10000\n","    config['penalty_anneal_iters'] = 2500\n","elif config['METHOD'] == \"iVAE_IRM\":\n","    config[\"NUM_STEPS_TRAIN_VAE_MODEL\"] = 10000\n","    config['PHASE2_SAMPLES'] = 50000\n","    #config['LATENT_DIM'] = config[\"REP_SIZE\"]\n","elif config['METHOD'] == \"CCIL\":\n","    #config['LATENT_DIM'] = config[\"REP_SIZE\"]\n","    config['MASK_PROB'] = 0.3\n","    #config['BATCH_SIZE'] = 5000\n"],"metadata":{"id":"4qkwWpjMfNzN"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["#testing/il"],"metadata":{"id":"tXRGSCjZfbnX"}},{"cell_type":"code","source":["import argparse\n","import os\n","import pickle\n","\n","import gym\n","import numpy as np\n","import pandas as pd\n","import yaml\n","import numpy as np\n","\n","from testing.paths import get_model_path, get_trajs_path  # pylint: disable=reimported\n","\n","from contrib.energy_model import EnergyModel\n","from contrib.env_wrapper import EnvWrapper, get_test_mult_factors\n","from network import EnvDiscriminator\n","from network import FeaturesDecoder\n","from network import FeaturesEncoder\n","from network import MineNetwork \n","from network import ObservationsDecoder\n","from network import StudentNetwork\n","\n","\n","from student import ICILStudent, BCStudent, BCIRMStudent, iVAE_IRMStudent, CCILStudent \n","from testing.train_utils import fill_buffer, make_agent, save_results\n","from vae.ivae_wrapper import VAE_wrapper\n","  \n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"GqnTIvivg1DT","executionInfo":{"status":"ok","timestamp":1650815579851,"user_tz":240,"elapsed":13595,"user":{"displayName":"2 Lin","userId":"15119774230930098070"}},"outputId":"6dad66d6-7e90-45d1-e5c4-5865b07fd026"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.7/dist-packages/stable_baselines/__init__.py:33: UserWarning: stable-baselines is in maintenance mode, please use [Stable-Baselines3 (SB3)](https://github.com/DLR-RM/stable-baselines3) for an up-to-date version. You can find a [migration guide](https://stable-baselines3.readthedocs.io/en/master/guide/migration.html) in SB3 documentation.\n","  \"stable-baselines is in maintenance mode, please use [Stable-Baselines3 (SB3)](https://github.com/DLR-RM/stable-baselines3) for an up-to-date version. You can find a [migration guide](https://stable-baselines3.readthedocs.io/en/master/guide/migration.html) in SB3 documentation.\"\n"]}]},{"cell_type":"markdown","source":["# make student"],"metadata":{"id":"hsMME-Bm3oMT"}},{"cell_type":"code","source":["\n","\n","# pylint: disable=redefined-outer-name\n","def make_student(run_seed, config):\n","    env = gym.make(config[\"ENV\"])\n","    trajs_path = get_trajs_path(config[\"ENV\"], \"student_\" + config[\"ALG\"], env_id=\"student\", run_seed=run_seed)\n","    model_path = get_model_path(config[\"ENV\"], \"student_\" + config[\"ALG\"], run_seed=run_seed)\n","\n","    state_dim = env.observation_space.shape[0] + config[\"NOISE_DIM\"]\n","    action_dim = env.action_space.n\n","    num_training_envs = config[\"NUM_TRAINING_ENVS\"]\n","\n","    # run_seed = run_seed\n","    batch_size = config[\"BATCH_SIZE\"]\n","    teacher = make_agent(config[\"ENV\"], config[\"EXPERT_ALG\"], config[\"NUM_TRAINING_ENVS\"])\n","    teacher.load_pretrained()\n","\n","    buffer = fill_buffer(\n","        trajs_path=teacher.trajs_paths,\n","        batch_size=batch_size,\n","        run_seed=run_seed,\n","        traj_shift=config[\"TRAJ_SHIFT\"],\n","        buffer_size_in_trajs=config[\"NUM_TRAJS_GIVEN\"],\n","        sampling_rate=config[\"SAMPLING_RATE\"],\n","        strictly_batch_data = False\n","    )\n","\n","    if buffer.total_size < batch_size:\n","        batch_size = buffer.total_size\n","\n","\n","\n","    ##########################      COMMON      ##########################\n","\n","    print(\"state_dim\", state_dim)\n","\n","    causal_features_encoder = FeaturesEncoder(\n","        input_size=state_dim, representation_size=config[\"REP_SIZE\"], width=config[\"MLP_WIDTHS\"]\n","    )\n","\n","    policy_network = StudentNetwork(in_dim=config[\"REP_SIZE\"], out_dim=action_dim, width=config[\"MLP_WIDTHS\"])\n","\n","    #print(\"config method = \", config['METHOD'])\n","\n","\n","    ##########################       BC       #######################\n","\n","    if config['METHOD'] == 'BC':\n","\n","        return BCStudent(\n","            env=env,\n","            trajs_paths=trajs_path,\n","            model_path=model_path,\n","            num_training_envs=num_training_envs,\n","            teacher=teacher,\n","            causal_features_encoder=causal_features_encoder,\n","            policy_network=policy_network,\n","            buffer=buffer,\n","            adam_alpha=config[\"ADAM_ALPHA\"],\n","            config = config\n","        )\n","\n","\n","    ##########################       BC IRM       #######################\n","\n","\n","    elif config['METHOD'] == 'BCIRM':\n","\n","        return BCIRMStudent(\n","            env=env,\n","            trajs_paths=trajs_path,\n","            model_path=model_path,\n","            num_training_envs=num_training_envs,\n","            teacher=teacher,\n","            causal_features_encoder=causal_features_encoder,\n","            policy_network=policy_network,\n","            buffer=buffer,\n","            adam_alpha=config[\"ADAM_ALPHA\"],\n","            config = config\n","        )\n","\n","\n","\n","    ##########################       CCIL       #######################\n","\n","\n","    elif config['METHOD'] == 'CCIL':\n","        \n","        causal_features_encoder = FeaturesEncoder(input_size=state_dim, representation_size=config[\"REP_SIZE\"], width=config[\"MLP_WIDTHS\"])\n","\n","        return CCILStudent(\n","            env=env,\n","            trajs_paths=trajs_path,\n","            model_path=model_path,\n","            num_training_envs=num_training_envs,\n","            teacher=teacher,\n","            causal_features_encoder=causal_features_encoder,\n","            policy_network=policy_network,\n","            buffer=buffer,\n","            adam_alpha=config[\"ADAM_ALPHA\"],\n","            config = config\n","        )\n","\n","    ##########################       iVAE IRM       #######################\n","\n","    elif config['METHOD'] == 'iVAE_IRM':\n","\n","        config['LATENT_DIM'] = state_dim \n","\n","        vae_wrapper =  VAE_wrapper(buffer, data_dim = state_dim, action_dim = action_dim, env_dim = config['NUM_TRAINING_ENVS'], latent_dim = config['LATENT_DIM'], use_e = True)\n","        vae_wrapper.train(num_updates=config[\"NUM_STEPS_TRAIN_VAE_MODEL\"])\n","        #vae_wrapper.start_phase2(n_samples = config['PHASE2_SAMPLES'])\n","        vae_wrapper.pa_list = [0,1,2,3,4,5,6,7]\n","        \n","\n","        causal_features_encoder = FeaturesEncoder(\n","            input_size=config[\"LATENT_DIM\"], representation_size=config[\"REP_SIZE\"], width=config[\"MLP_WIDTHS\"] )\n","\n","        policy_network = StudentNetwork(in_dim=config[\"REP_SIZE\"], out_dim=action_dim, width=config[\"MLP_WIDTHS\"])\n","\n","        phase3_obs_to_latent_encoder = FeaturesEncoder(\n","            input_size=state_dim, representation_size=config[\"LATENT_DIM\"], width=config[\"MLP_WIDTHS\"] )\n","        \n","        #causal_features_encoder = FeaturesEncoder(\n","        #            input_size=state_dim, representation_size = len(vae_wrapper.pa_list), width=config[\"MLP_WIDTHS\"])\n","\n","        #policy_network = StudentNetwork(in_dim = config['LATENT_DIM'], out_dim=action_dim, width=config[\"MLP_WIDTHS\"])\n","\n","        \n","        return iVAE_IRMStudent(\n","            env=env,\n","            vae_wrapper = vae_wrapper,\n","            trajs_paths=trajs_path,\n","            model_path=model_path,\n","            num_training_envs=num_training_envs,\n","            teacher=teacher,\n","            phase3_obs_to_latent_encoder = phase3_obs_to_latent_encoder,\n","            causal_features_encoder=causal_features_encoder,\n","            policy_network=policy_network,\n","            buffer=buffer,\n","            adam_alpha=config[\"ADAM_ALPHA\"],\n","            config = config\n","        )\n","        \n","\n","\n","    ##########################       ICIL        #######################\n","\n","    elif config['METHOD'] == 'ICIL':\n","        energy_model = EnergyModel(\n","            in_dim=state_dim,\n","            width=config[\"MLP_WIDTHS\"],\n","            batch_size=batch_size,\n","            adam_alpha=config[\"ADAM_ALPHA\"],\n","            buffer=buffer,\n","            sgld_buffer_size=config[\"SGLD_BUFFER_SIZE\"],\n","            sgld_learn_rate=config[\"SGLD_LEARN_RATE\"],\n","            sgld_noise_coef=config[\"SGLD_NOISE_COEF\"],\n","            sgld_num_steps=config[\"SGLD_NUM_STEPS\"],\n","            sgld_reinit_freq=config[\"SGLD_REINIT_FREQ\"],\n","        )\n","        energy_model.train(num_updates=config[\"NUM_STEPS_TRAIN_ENERGY_MODEL\"])\n","\n","        causal_features_decoder = FeaturesDecoder(action_size=action_dim, representation_size=config[\"REP_SIZE\"], width=config[\"MLP_WIDTHS\"])\n","\n","        observations_decoder = ObservationsDecoder(representation_size=config[\"REP_SIZE\"], out_size=state_dim, width=config[\"MLP_WIDTHS\"] )\n","\n","        env_discriminator = EnvDiscriminator(representation_size=config[\"REP_SIZE\"], num_envs=config[\"NUM_TRAINING_ENVS\"], width=config[\"MLP_WIDTHS\"])\n","\n","        noise_features_encoders = [FeaturesEncoder(input_size=state_dim, representation_size=config[\"REP_SIZE\"], width=config[\"MLP_WIDTHS\"])  \n","            for i in range(num_training_envs)]\n","        \n","        noise_features_decoders = [FeaturesDecoder(action_size=action_dim, representation_size=config[\"REP_SIZE\"], width=config[\"MLP_WIDTHS\"])\n","            for i in range(num_training_envs)]\n","\n","        mine_network = MineNetwork(x_dim=config[\"REP_SIZE\"], z_dim=config[\"REP_SIZE\"], width=config[\"MLP_WIDTHS\"])\n","\n","        return ICILStudent(\n","            env=env,\n","            trajs_paths=trajs_path,\n","            model_path=model_path,\n","            num_training_envs=num_training_envs,\n","            teacher=teacher,\n","            causal_features_encoder=causal_features_encoder,\n","            noise_features_encoders=noise_features_encoders,\n","            causal_features_decoder=causal_features_decoder,\n","            noise_features_decoders=noise_features_decoders,\n","            observations_decoder=observations_decoder,\n","            env_discriminator=env_discriminator,\n","            policy_network=policy_network,\n","            energy_model=energy_model,\n","            mine_network=mine_network,\n","            buffer=buffer,\n","            adam_alpha=config[\"ADAM_ALPHA\"],\n","            config = config\n","        )\n","\n","\n","def init_arg():\n","    parser = argparse.ArgumentParser()\n","    parser.add_argument(\"--env_name\", default=\"CartPole-v1\")\n","    parser.add_argument(\"--num_trajectories\", default=20, type=int)\n","    parser.add_argument(\"--trial\", default=0, type=int)\n","    return parser.parse_args()\n"],"metadata":{"id":"BYsHrHrlffKj"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[""],"metadata":{"id":"rKRfOUTUCciv"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["#10 Trails -- CCIL -- cartpole * * *"],"metadata":{"id":"tkcl8F5lCc0u"}},{"cell_type":"code","source":["config['MASK_PROB'] = 0.5\n","config['NUM_STEPS_TRAIN'] = 100000\n","config['ADAM_ALPHA'] = 0.001\n","config['NUM_REPETITIONS'] = 10\n","config['ENV'] = 'CartPole-v1'"],"metadata":{"id":"_yWZIYTOExhW"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["config['METHOD'] = \"CCIL\"\n","\n","for traj_num in [1, 2, 5, 10, 20, 30, 40, 50]:\n","    config[\"NUM_TRAJS_GIVEN\"] = traj_num\n","    config[\"TRAJ_SHIFT\"] = traj_num\n","\n","    config['ALG'] = \"FINAL_Apr24_CCILStudent_origindata_multiply_trajnum\" + str(traj_num)\n","\n","\n","    ###############.  settings   ###############\n","    #config['ALG'] = \"BCIRMStudent_Apr19_replicatedata\"\n","    #config['METHOD'] = \"BCIRM\"\n","    #config['METHOD'] = \"ICIL\"\n","    #config[\"NUM_TRAJS_GIVEN\"] = 50\n","    #config[\"TRAJ_SHIFT\"] = 50\n","    #config['ENV'] == \"CartPole-v1\"\n","    ###############.  settings   ###############\n","\n","    all_results_trail = []\n","\n","    for trail in range(1): \n","        config['TRIAL'] = trail \n","\n","\n","        ###############.  start a trail   ###############\n","\n","        config[\"EXPERT_ALG\"] = yaml.load(open(\"testing/config.yml\"), Loader=yaml.FullLoader)[config[\"ENV\"]]\n","        print(\"Config: %s\" % config)\n","\n","        TRIAL = config[\"TRIAL\"] #args.trial\n","        print(\"Trial number %s\" % TRIAL)\n","\n","        results_dir_base = \"testing/results/\"\n","        results_dir = os.path.join(results_dir_base, config[\"ENV\"], str(config[\"NUM_TRAJS_GIVEN\"]), config[\"ALG\"])\n","\n","        if not os.path.exists(results_dir):\n","            os.makedirs(results_dir)\n","\n","        config_file = \"trial_\" + str(TRIAL) + \"_\" + \"config.pkl\"\n","\n","        results_file_name = \"trial_\" + str(TRIAL) + \"_\" + \"results.csv\"\n","        results_file_path = os.path.join(results_dir, results_file_name)\n","\n","        if os.path.exists(os.path.join(results_dir, config_file)):\n","            raise NameError(\"CONFIG file already exists %s. Choose a different trial number.\" % config_file)\n","        pickle.dump(config, open(os.path.join(results_dir, config_file), \"wb\"))\n","\n","\n","\n","\n","        ###############.  10 runs for each trail   ###############\n","\n","        print(\"config method = \", config['METHOD'])\n","        print(\"config env = \", config['ENV'])\n","\n","        for run_seed in range(config[\"NUM_REPETITIONS\"]):\n","            print(\"Run %s out of %s\" % (run_seed + 1, config[\"NUM_REPETITIONS\"]))\n","            student = make_student(run_seed, config)\n","            student.train(num_updates=config[\"NUM_STEPS_TRAIN\"])\n","\n","            env_wrapper_out_of_sample = EnvWrapper(\n","                env=gym.make(config[\"ENV\"]), mult_factor=get_test_mult_factors(config['NOISE_DIM'] - 1), idx=3, seed=1\n","            )\n","\n","            env_wrapper_out_of_sample.noise = 0\n","\n","            action_match, return_mean, return_std = student.test(\n","                num_episodes=config[\"NUM_TRAJS_VALID\"], env_wrapper=env_wrapper_out_of_sample\n","            )\n","\n","            result = (action_match, return_mean, return_std)\n","            print(\"###############    Reward for test environment for run %s: %s.   ###############\\n\\n\" % (run_seed + 1, return_mean))\n","            save_results(results_file_path, run_seed, action_match, return_mean, return_std)\n","\n","        results_trial = pd.read_csv(\n","            \"testing/results/\"\n","            + config[\"ENV\"]\n","            + \"/\"\n","            + str(config[\"NUM_TRAJS_GIVEN\"])\n","            + \"/\"\n","            + config[\"ALG\"]\n","            + \"/trial_\"\n","            + str(TRIAL)\n","            + \"_results.csv\",\n","            header=None,\n","        )\n","\n","        print(\"Average reward for 10 repetitions: %s\" % np.mean(results_trial[2].values))\n","\n","        all_results_trail.append(np.mean(results_trial[2].values))\n","    print(\"ALL RESULTS TRAIL:\" , all_results_trail)\n","\n","\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"outputId":"e3c1378c-f384-466e-9ed0-59f7b9ec830f","id":"BJ20eYbwCc0v","executionInfo":{"status":"ok","timestamp":1650806425026,"user_tz":240,"elapsed":18614186,"user":{"displayName":"2 Lin","userId":"15119774230930098070"}}},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Config: {'ENV': 'CartPole-v1', 'ALG': 'FINAL_Apr24_CCILStudent_origindata_multiply_trajnum1', 'NUM_TRAJS_GIVEN': 1, 'NUM_TRAINING_ENVS': 2, 'NOISE_DIM': 4, 'REP_SIZE': 16, 'TRAJ_SHIFT': 1, 'SAMPLING_RATE': 5, 'NUM_STEPS_TRAIN': 100000, 'NUM_TRAJS_VALID': 100, 'NUM_REPETITIONS': 10, 'BATCH_SIZE': 64, 'MLP_WIDTHS': 64, 'ADAM_ALPHA': 0.001, 'SGLD_BUFFER_SIZE': 10000, 'SGLD_LEARN_RATE': 0.01, 'SGLD_NOISE_COEF': 0.01, 'SGLD_NUM_STEPS': 100, 'SGLD_REINIT_FREQ': 0.05, 'NUM_STEPS_TRAIN_ENERGY_MODEL': 1000, 'TRIAL': 0, 'METHOD': 'CCIL', 'MASK_PROB': 0.5, 'EXPERT_ALG': 'dqn'}\n","Trial number 0\n","config method =  CCIL\n","config env =  CartPole-v1\n","Run 1 out of 10\n","state_dim 8\n","epoch 99000/100000, policy loss 0.1817515641450882 \tepoch 0/100 return: 208.0\n","epoch 10/100 return: 277.0\n","epoch 20/100 return: 266.0\n","epoch 30/100 return: 127.0\n","epoch 40/100 return: 136.0\n","epoch 50/100 return: 164.0\n","epoch 60/100 return: 117.0\n","epoch 70/100 return: 166.0\n","epoch 80/100 return: 203.0\n","epoch 90/100 return: 170.0\n","###############    Reward for test environment for run 1: 152.99.   ###############\n","\n","\n","Run 2 out of 10\n","state_dim 8\n","epoch 99000/100000, policy loss 0.25219959020614624 \tepoch 0/100 return: 196.0\n","epoch 10/100 return: 115.0\n","epoch 20/100 return: 154.0\n","epoch 30/100 return: 67.0\n","epoch 40/100 return: 152.0\n","epoch 50/100 return: 227.0\n","epoch 60/100 return: 131.0\n","epoch 70/100 return: 131.0\n","epoch 80/100 return: 137.0\n","epoch 90/100 return: 145.0\n","###############    Reward for test environment for run 2: 132.67.   ###############\n","\n","\n","Run 3 out of 10\n","state_dim 8\n","epoch 99000/100000, policy loss 0.252396821975708 \tepoch 0/100 return: 66.0\n","epoch 10/100 return: 81.0\n","epoch 20/100 return: 72.0\n","epoch 30/100 return: 74.0\n","epoch 40/100 return: 86.0\n","epoch 50/100 return: 68.0\n","epoch 60/100 return: 50.0\n","epoch 70/100 return: 63.0\n","epoch 80/100 return: 48.0\n","epoch 90/100 return: 50.0\n","###############    Reward for test environment for run 3: 80.6.   ###############\n","\n","\n","Run 4 out of 10\n","state_dim 8\n","epoch 99000/100000, policy loss 0.3576701581478119 \tepoch 0/100 return: 59.0\n","epoch 10/100 return: 96.0\n","epoch 20/100 return: 52.0\n","epoch 30/100 return: 90.0\n","epoch 40/100 return: 61.0\n","epoch 50/100 return: 43.0\n","epoch 60/100 return: 87.0\n","epoch 70/100 return: 251.0\n","epoch 80/100 return: 60.0\n","epoch 90/100 return: 458.0\n","###############    Reward for test environment for run 4: 133.1.   ###############\n","\n","\n","Run 5 out of 10\n","state_dim 8\n","epoch 99000/100000, policy loss 0.36333876848220825 \tepoch 0/100 return: 189.0\n","epoch 10/100 return: 252.0\n","epoch 20/100 return: 234.0\n","epoch 30/100 return: 194.0\n","epoch 40/100 return: 162.0\n","epoch 50/100 return: 201.0\n","epoch 60/100 return: 165.0\n","epoch 70/100 return: 251.0\n","epoch 80/100 return: 159.0\n","epoch 90/100 return: 174.0\n","###############    Reward for test environment for run 5: 196.38.   ###############\n","\n","\n","Run 6 out of 10\n","state_dim 8\n","epoch 99000/100000, policy loss 0.2245921492576599 \tepoch 0/100 return: 84.0\n","epoch 10/100 return: 72.0\n","epoch 20/100 return: 84.0\n","epoch 30/100 return: 83.0\n","epoch 40/100 return: 88.0\n","epoch 50/100 return: 98.0\n","epoch 60/100 return: 72.0\n","epoch 70/100 return: 69.0\n","epoch 80/100 return: 64.0\n","epoch 90/100 return: 74.0\n","###############    Reward for test environment for run 6: 86.99.   ###############\n","\n","\n","Run 7 out of 10\n","state_dim 8\n","epoch 99000/100000, policy loss 0.39762890338897705 \tepoch 0/100 return: 85.0\n","epoch 10/100 return: 87.0\n","epoch 20/100 return: 84.0\n","epoch 30/100 return: 76.0\n","epoch 40/100 return: 88.0\n","epoch 50/100 return: 68.0\n","epoch 60/100 return: 89.0\n","epoch 70/100 return: 82.0\n","epoch 80/100 return: 82.0\n","epoch 90/100 return: 79.0\n","###############    Reward for test environment for run 7: 83.57.   ###############\n","\n","\n","Run 8 out of 10\n","state_dim 8\n","epoch 99000/100000, policy loss 0.25037312507629395 \tepoch 0/100 return: 128.0\n","epoch 10/100 return: 500.0\n","epoch 20/100 return: 75.0\n","epoch 30/100 return: 500.0\n","epoch 40/100 return: 137.0\n","epoch 50/100 return: 138.0\n","epoch 60/100 return: 133.0\n","epoch 70/100 return: 120.0\n","epoch 80/100 return: 60.0\n","epoch 90/100 return: 64.0\n","###############    Reward for test environment for run 8: 172.46.   ###############\n","\n","\n","Run 9 out of 10\n","state_dim 8\n","epoch 99000/100000, policy loss 0.2537316679954529 \tepoch 0/100 return: 49.0\n","epoch 10/100 return: 90.0\n","epoch 20/100 return: 48.0\n","epoch 30/100 return: 60.0\n","epoch 40/100 return: 80.0\n","epoch 50/100 return: 51.0\n","epoch 60/100 return: 56.0\n","epoch 70/100 return: 53.0\n","epoch 80/100 return: 51.0\n","epoch 90/100 return: 64.0\n","###############    Reward for test environment for run 9: 59.01.   ###############\n","\n","\n","Run 10 out of 10\n","state_dim 8\n","epoch 99000/100000, policy loss 0.3171532154083252 \tepoch 0/100 return: 500.0\n","epoch 10/100 return: 188.0\n","epoch 20/100 return: 185.0\n","epoch 30/100 return: 127.0\n","epoch 40/100 return: 105.0\n","epoch 50/100 return: 166.0\n","epoch 60/100 return: 89.0\n","epoch 70/100 return: 91.0\n","epoch 80/100 return: 166.0\n","epoch 90/100 return: 500.0\n","###############    Reward for test environment for run 10: 162.12.   ###############\n","\n","\n","Average reward for 10 repetitions: 125.98899999999999\n","ALL RESULTS TRAIL: [125.98899999999999]\n","Config: {'ENV': 'CartPole-v1', 'ALG': 'FINAL_Apr24_CCILStudent_origindata_multiply_trajnum2', 'NUM_TRAJS_GIVEN': 2, 'NUM_TRAINING_ENVS': 2, 'NOISE_DIM': 4, 'REP_SIZE': 16, 'TRAJ_SHIFT': 2, 'SAMPLING_RATE': 5, 'NUM_STEPS_TRAIN': 100000, 'NUM_TRAJS_VALID': 100, 'NUM_REPETITIONS': 10, 'BATCH_SIZE': 64, 'MLP_WIDTHS': 64, 'ADAM_ALPHA': 0.001, 'SGLD_BUFFER_SIZE': 10000, 'SGLD_LEARN_RATE': 0.01, 'SGLD_NOISE_COEF': 0.01, 'SGLD_NUM_STEPS': 100, 'SGLD_REINIT_FREQ': 0.05, 'NUM_STEPS_TRAIN_ENERGY_MODEL': 1000, 'TRIAL': 0, 'METHOD': 'CCIL', 'MASK_PROB': 0.5, 'EXPERT_ALG': 'dqn'}\n","Trial number 0\n","config method =  CCIL\n","config env =  CartPole-v1\n","Run 1 out of 10\n","state_dim 8\n","epoch 99000/100000, policy loss 0.41151106357574463 \tepoch 0/100 return: 150.0\n","epoch 10/100 return: 61.0\n","epoch 20/100 return: 145.0\n","epoch 30/100 return: 95.0\n","epoch 40/100 return: 113.0\n","epoch 50/100 return: 81.0\n","epoch 60/100 return: 140.0\n","epoch 70/100 return: 89.0\n","epoch 80/100 return: 83.0\n","epoch 90/100 return: 166.0\n","###############    Reward for test environment for run 1: 97.22.   ###############\n","\n","\n","Run 2 out of 10\n","state_dim 8\n","epoch 99000/100000, policy loss 0.24502496421337128 \tepoch 0/100 return: 64.0\n","epoch 10/100 return: 500.0\n","epoch 20/100 return: 100.0\n","epoch 30/100 return: 89.0\n","epoch 40/100 return: 500.0\n","epoch 50/100 return: 78.0\n","epoch 60/100 return: 81.0\n","epoch 70/100 return: 89.0\n","epoch 80/100 return: 60.0\n","epoch 90/100 return: 91.0\n","###############    Reward for test environment for run 2: 240.9.   ###############\n","\n","\n","Run 3 out of 10\n","state_dim 8\n","epoch 99000/100000, policy loss 0.24808044731616974 \tepoch 0/100 return: 500.0\n","epoch 10/100 return: 500.0\n","epoch 20/100 return: 500.0\n","epoch 30/100 return: 500.0\n","epoch 40/100 return: 500.0\n","epoch 50/100 return: 137.0\n","epoch 60/100 return: 255.0\n","epoch 70/100 return: 360.0\n","epoch 80/100 return: 500.0\n","epoch 90/100 return: 98.0\n","###############    Reward for test environment for run 3: 304.87.   ###############\n","\n","\n","Run 4 out of 10\n","state_dim 8\n","epoch 99000/100000, policy loss 0.3512818217277527 \tepoch 0/100 return: 58.0\n","epoch 10/100 return: 185.0\n","epoch 20/100 return: 82.0\n","epoch 30/100 return: 70.0\n","epoch 40/100 return: 180.0\n","epoch 50/100 return: 63.0\n","epoch 60/100 return: 174.0\n","epoch 70/100 return: 182.0\n","epoch 80/100 return: 58.0\n","epoch 90/100 return: 52.0\n","###############    Reward for test environment for run 4: 120.31.   ###############\n","\n","\n","Run 5 out of 10\n","state_dim 8\n","epoch 99000/100000, policy loss 0.3018755316734314 \tepoch 0/100 return: 123.0\n","epoch 10/100 return: 168.0\n","epoch 20/100 return: 170.0\n","epoch 30/100 return: 163.0\n","epoch 40/100 return: 95.0\n","epoch 50/100 return: 154.0\n","epoch 60/100 return: 114.0\n","epoch 70/100 return: 86.0\n","epoch 80/100 return: 169.0\n","epoch 90/100 return: 180.0\n","###############    Reward for test environment for run 5: 152.44.   ###############\n","\n","\n","Run 6 out of 10\n","state_dim 8\n","epoch 99000/100000, policy loss 0.24616114795207977 \tepoch 0/100 return: 59.0\n","epoch 10/100 return: 67.0\n","epoch 20/100 return: 73.0\n","epoch 30/100 return: 82.0\n","epoch 40/100 return: 205.0\n","epoch 50/100 return: 65.0\n","epoch 60/100 return: 70.0\n","epoch 70/100 return: 87.0\n","epoch 80/100 return: 71.0\n","epoch 90/100 return: 85.0\n","###############    Reward for test environment for run 6: 160.13.   ###############\n","\n","\n","Run 7 out of 10\n","state_dim 8\n","epoch 99000/100000, policy loss 0.25969529151916504 \tepoch 0/100 return: 39.0\n","epoch 10/100 return: 58.0\n","epoch 20/100 return: 149.0\n","epoch 30/100 return: 28.0\n","epoch 40/100 return: 101.0\n","epoch 50/100 return: 45.0\n","epoch 60/100 return: 44.0\n","epoch 70/100 return: 135.0\n","epoch 80/100 return: 53.0\n","epoch 90/100 return: 43.0\n","###############    Reward for test environment for run 7: 64.69.   ###############\n","\n","\n","Run 8 out of 10\n","state_dim 8\n","epoch 99000/100000, policy loss 0.3493945300579071 \tepoch 0/100 return: 85.0\n","epoch 10/100 return: 90.0\n","epoch 20/100 return: 103.0\n","epoch 30/100 return: 90.0\n","epoch 40/100 return: 78.0\n","epoch 50/100 return: 98.0\n","epoch 60/100 return: 86.0\n","epoch 70/100 return: 142.0\n","epoch 80/100 return: 111.0\n","epoch 90/100 return: 70.0\n","###############    Reward for test environment for run 8: 89.05.   ###############\n","\n","\n","Run 9 out of 10\n","state_dim 8\n","epoch 99000/100000, policy loss 0.3552049398422241 \tepoch 0/100 return: 13.0\n","epoch 10/100 return: 10.0\n","epoch 20/100 return: 121.0\n","epoch 30/100 return: 72.0\n","epoch 40/100 return: 28.0\n","epoch 50/100 return: 26.0\n","epoch 60/100 return: 84.0\n","epoch 70/100 return: 35.0\n","epoch 80/100 return: 22.0\n","epoch 90/100 return: 18.0\n","###############    Reward for test environment for run 9: 36.82.   ###############\n","\n","\n","Run 10 out of 10\n","state_dim 8\n","epoch 99000/100000, policy loss 0.38779863715171814 \tepoch 0/100 return: 129.0\n","epoch 10/100 return: 317.0\n","epoch 20/100 return: 449.0\n","epoch 30/100 return: 221.0\n","epoch 40/100 return: 407.0\n","epoch 50/100 return: 270.0\n","epoch 60/100 return: 272.0\n","epoch 70/100 return: 275.0\n","epoch 80/100 return: 422.0\n","epoch 90/100 return: 453.0\n","###############    Reward for test environment for run 10: 333.75.   ###############\n","\n","\n","Average reward for 10 repetitions: 160.01799999999997\n","ALL RESULTS TRAIL: [160.01799999999997]\n","Config: {'ENV': 'CartPole-v1', 'ALG': 'FINAL_Apr24_CCILStudent_origindata_multiply_trajnum5', 'NUM_TRAJS_GIVEN': 5, 'NUM_TRAINING_ENVS': 2, 'NOISE_DIM': 4, 'REP_SIZE': 16, 'TRAJ_SHIFT': 5, 'SAMPLING_RATE': 5, 'NUM_STEPS_TRAIN': 100000, 'NUM_TRAJS_VALID': 100, 'NUM_REPETITIONS': 10, 'BATCH_SIZE': 64, 'MLP_WIDTHS': 64, 'ADAM_ALPHA': 0.001, 'SGLD_BUFFER_SIZE': 10000, 'SGLD_LEARN_RATE': 0.01, 'SGLD_NOISE_COEF': 0.01, 'SGLD_NUM_STEPS': 100, 'SGLD_REINIT_FREQ': 0.05, 'NUM_STEPS_TRAIN_ENERGY_MODEL': 1000, 'TRIAL': 0, 'METHOD': 'CCIL', 'MASK_PROB': 0.5, 'EXPERT_ALG': 'dqn'}\n","Trial number 0\n","config method =  CCIL\n","config env =  CartPole-v1\n","Run 1 out of 10\n","state_dim 8\n","epoch 99000/100000, policy loss 0.5114606618881226 \tepoch 0/100 return: 178.0\n","epoch 10/100 return: 120.0\n","epoch 20/100 return: 187.0\n","epoch 30/100 return: 164.0\n","epoch 40/100 return: 182.0\n","epoch 50/100 return: 500.0\n","epoch 60/100 return: 148.0\n","epoch 70/100 return: 150.0\n","epoch 80/100 return: 500.0\n","epoch 90/100 return: 136.0\n","###############    Reward for test environment for run 1: 254.38.   ###############\n","\n","\n","Run 2 out of 10\n","state_dim 8\n","epoch 99000/100000, policy loss 0.5323002934455872 \tepoch 0/100 return: 62.0\n","epoch 10/100 return: 69.0\n","epoch 20/100 return: 72.0\n","epoch 30/100 return: 70.0\n","epoch 40/100 return: 63.0\n","epoch 50/100 return: 57.0\n","epoch 60/100 return: 67.0\n","epoch 70/100 return: 69.0\n","epoch 80/100 return: 66.0\n","epoch 90/100 return: 64.0\n","###############    Reward for test environment for run 2: 77.31.   ###############\n","\n","\n","Run 3 out of 10\n","state_dim 8\n","epoch 99000/100000, policy loss 0.47374147176742554 \tepoch 0/100 return: 9.0\n","epoch 10/100 return: 10.0\n","epoch 20/100 return: 12.0\n","epoch 30/100 return: 13.0\n","epoch 40/100 return: 10.0\n","epoch 50/100 return: 24.0\n","epoch 60/100 return: 13.0\n","epoch 70/100 return: 9.0\n","epoch 80/100 return: 10.0\n","epoch 90/100 return: 34.0\n","###############    Reward for test environment for run 3: 19.38.   ###############\n","\n","\n","Run 4 out of 10\n","state_dim 8\n","epoch 99000/100000, policy loss 0.43064039945602417 \tepoch 0/100 return: 500.0\n","epoch 10/100 return: 500.0\n","epoch 20/100 return: 500.0\n","epoch 30/100 return: 500.0\n","epoch 40/100 return: 500.0\n","epoch 50/100 return: 500.0\n","epoch 60/100 return: 500.0\n","epoch 70/100 return: 500.0\n","epoch 80/100 return: 500.0\n","epoch 90/100 return: 500.0\n","###############    Reward for test environment for run 4: 500.0.   ###############\n","\n","\n","Run 5 out of 10\n","state_dim 8\n","epoch 99000/100000, policy loss 0.40202537178993225 \tepoch 0/100 return: 9.0\n","epoch 10/100 return: 9.0\n","epoch 20/100 return: 9.0\n","epoch 30/100 return: 28.0\n","epoch 40/100 return: 11.0\n","epoch 50/100 return: 9.0\n","epoch 60/100 return: 8.0\n","epoch 70/100 return: 9.0\n","epoch 80/100 return: 13.0\n","epoch 90/100 return: 15.0\n","###############    Reward for test environment for run 5: 10.55.   ###############\n","\n","\n","Run 6 out of 10\n","state_dim 8\n","epoch 99000/100000, policy loss 0.4362345039844513 \tepoch 0/100 return: 118.0\n","epoch 10/100 return: 113.0\n","epoch 20/100 return: 110.0\n","epoch 30/100 return: 106.0\n","epoch 40/100 return: 118.0\n","epoch 50/100 return: 311.0\n","epoch 60/100 return: 233.0\n","epoch 70/100 return: 308.0\n","epoch 80/100 return: 93.0\n","epoch 90/100 return: 444.0\n","###############    Reward for test environment for run 6: 204.61.   ###############\n","\n","\n","Run 7 out of 10\n","state_dim 8\n","epoch 99000/100000, policy loss 0.4709279239177704 \tepoch 0/100 return: 140.0\n","epoch 10/100 return: 233.0\n","epoch 20/100 return: 146.0\n","epoch 30/100 return: 178.0\n","epoch 40/100 return: 158.0\n","epoch 50/100 return: 132.0\n","epoch 60/100 return: 184.0\n","epoch 70/100 return: 132.0\n","epoch 80/100 return: 228.0\n","epoch 90/100 return: 168.0\n","###############    Reward for test environment for run 7: 190.63.   ###############\n","\n","\n","Run 8 out of 10\n","state_dim 8\n","epoch 99000/100000, policy loss 0.3802410662174225 \tepoch 0/100 return: 154.0\n","epoch 10/100 return: 152.0\n","epoch 20/100 return: 158.0\n","epoch 30/100 return: 99.0\n","epoch 40/100 return: 198.0\n","epoch 50/100 return: 198.0\n","epoch 60/100 return: 154.0\n","epoch 70/100 return: 148.0\n","epoch 80/100 return: 160.0\n","epoch 90/100 return: 77.0\n","###############    Reward for test environment for run 8: 156.0.   ###############\n","\n","\n","Run 9 out of 10\n","state_dim 8\n","epoch 99000/100000, policy loss 0.5044702291488647 \tepoch 0/100 return: 117.0\n","epoch 10/100 return: 124.0\n","epoch 20/100 return: 116.0\n","epoch 30/100 return: 76.0\n","epoch 40/100 return: 338.0\n","epoch 50/100 return: 288.0\n","epoch 60/100 return: 199.0\n","epoch 70/100 return: 139.0\n","epoch 80/100 return: 114.0\n","epoch 90/100 return: 254.0\n","###############    Reward for test environment for run 9: 133.59.   ###############\n","\n","\n","Run 10 out of 10\n","state_dim 8\n","epoch 99000/100000, policy loss 0.3408207297325134 \tepoch 0/100 return: 142.0\n","epoch 10/100 return: 158.0\n","epoch 20/100 return: 168.0\n","epoch 30/100 return: 160.0\n","epoch 40/100 return: 151.0\n","epoch 50/100 return: 462.0\n","epoch 60/100 return: 153.0\n","epoch 70/100 return: 153.0\n","epoch 80/100 return: 159.0\n","epoch 90/100 return: 157.0\n","###############    Reward for test environment for run 10: 199.29.   ###############\n","\n","\n","Average reward for 10 repetitions: 174.57399999999998\n","ALL RESULTS TRAIL: [174.57399999999998]\n","Config: {'ENV': 'CartPole-v1', 'ALG': 'FINAL_Apr24_CCILStudent_origindata_multiply_trajnum10', 'NUM_TRAJS_GIVEN': 10, 'NUM_TRAINING_ENVS': 2, 'NOISE_DIM': 4, 'REP_SIZE': 16, 'TRAJ_SHIFT': 10, 'SAMPLING_RATE': 5, 'NUM_STEPS_TRAIN': 100000, 'NUM_TRAJS_VALID': 100, 'NUM_REPETITIONS': 10, 'BATCH_SIZE': 64, 'MLP_WIDTHS': 64, 'ADAM_ALPHA': 0.001, 'SGLD_BUFFER_SIZE': 10000, 'SGLD_LEARN_RATE': 0.01, 'SGLD_NOISE_COEF': 0.01, 'SGLD_NUM_STEPS': 100, 'SGLD_REINIT_FREQ': 0.05, 'NUM_STEPS_TRAIN_ENERGY_MODEL': 1000, 'TRIAL': 0, 'METHOD': 'CCIL', 'MASK_PROB': 0.5, 'EXPERT_ALG': 'dqn'}\n","Trial number 0\n","config method =  CCIL\n","config env =  CartPole-v1\n","Run 1 out of 10\n","state_dim 8\n","epoch 99000/100000, policy loss 0.4326176345348358 \tepoch 0/100 return: 160.0\n","epoch 10/100 return: 271.0\n","epoch 20/100 return: 113.0\n","epoch 30/100 return: 141.0\n","epoch 40/100 return: 221.0\n","epoch 50/100 return: 167.0\n","epoch 60/100 return: 134.0\n","epoch 70/100 return: 202.0\n","epoch 80/100 return: 188.0\n","epoch 90/100 return: 161.0\n","###############    Reward for test environment for run 1: 193.05.   ###############\n","\n","\n","Run 2 out of 10\n","state_dim 8\n","epoch 99000/100000, policy loss 0.49478575587272644 \tepoch 0/100 return: 112.0\n","epoch 10/100 return: 77.0\n","epoch 20/100 return: 92.0\n","epoch 30/100 return: 146.0\n","epoch 40/100 return: 282.0\n","epoch 50/100 return: 162.0\n","epoch 60/100 return: 79.0\n","epoch 70/100 return: 109.0\n","epoch 80/100 return: 185.0\n","epoch 90/100 return: 170.0\n","###############    Reward for test environment for run 2: 143.5.   ###############\n","\n","\n","Run 3 out of 10\n","state_dim 8\n","epoch 99000/100000, policy loss 0.4793322682380676 \tepoch 0/100 return: 67.0\n","epoch 10/100 return: 500.0\n","epoch 20/100 return: 78.0\n","epoch 30/100 return: 56.0\n","epoch 40/100 return: 88.0\n","epoch 50/100 return: 91.0\n","epoch 60/100 return: 91.0\n","epoch 70/100 return: 66.0\n","epoch 80/100 return: 60.0\n","epoch 90/100 return: 90.0\n","###############    Reward for test environment for run 3: 98.97.   ###############\n","\n","\n","Run 4 out of 10\n","state_dim 8\n","epoch 99000/100000, policy loss 0.4899749755859375 \tepoch 0/100 return: 500.0\n","epoch 10/100 return: 500.0\n","epoch 20/100 return: 500.0\n","epoch 30/100 return: 117.0\n","epoch 40/100 return: 500.0\n","epoch 50/100 return: 96.0\n","epoch 60/100 return: 96.0\n","epoch 70/100 return: 500.0\n","epoch 80/100 return: 112.0\n","epoch 90/100 return: 500.0\n","###############    Reward for test environment for run 4: 383.33.   ###############\n","\n","\n","Run 5 out of 10\n","state_dim 8\n","epoch 99000/100000, policy loss 0.6361445188522339 \tepoch 0/100 return: 101.0\n","epoch 10/100 return: 170.0\n","epoch 20/100 return: 153.0\n","epoch 30/100 return: 68.0\n","epoch 40/100 return: 149.0\n","epoch 50/100 return: 119.0\n","epoch 60/100 return: 92.0\n","epoch 70/100 return: 201.0\n","epoch 80/100 return: 152.0\n","epoch 90/100 return: 160.0\n","###############    Reward for test environment for run 5: 151.77.   ###############\n","\n","\n","Run 6 out of 10\n","state_dim 8\n","epoch 99000/100000, policy loss 0.48258355259895325 \tepoch 0/100 return: 223.0\n","epoch 10/100 return: 92.0\n","epoch 20/100 return: 500.0\n","epoch 30/100 return: 72.0\n","epoch 40/100 return: 500.0\n","epoch 50/100 return: 108.0\n","epoch 60/100 return: 500.0\n","epoch 70/100 return: 169.0\n","epoch 80/100 return: 50.0\n","epoch 90/100 return: 275.0\n","###############    Reward for test environment for run 6: 226.57.   ###############\n","\n","\n","Run 7 out of 10\n","state_dim 8\n","epoch 99000/100000, policy loss 0.4954015612602234 \tepoch 0/100 return: 320.0\n","epoch 10/100 return: 235.0\n","epoch 20/100 return: 120.0\n","epoch 30/100 return: 204.0\n","epoch 40/100 return: 500.0\n","epoch 50/100 return: 137.0\n","epoch 60/100 return: 500.0\n","epoch 70/100 return: 500.0\n","epoch 80/100 return: 207.0\n","epoch 90/100 return: 500.0\n","###############    Reward for test environment for run 7: 272.81.   ###############\n","\n","\n","Run 8 out of 10\n","state_dim 8\n","epoch 99000/100000, policy loss 0.45767149329185486 \tepoch 0/100 return: 92.0\n","epoch 10/100 return: 66.0\n","epoch 20/100 return: 68.0\n","epoch 30/100 return: 124.0\n","epoch 40/100 return: 66.0\n","epoch 50/100 return: 66.0\n","epoch 60/100 return: 62.0\n","epoch 70/100 return: 64.0\n","epoch 80/100 return: 56.0\n","epoch 90/100 return: 103.0\n","###############    Reward for test environment for run 8: 73.24.   ###############\n","\n","\n","Run 9 out of 10\n","state_dim 8\n","epoch 99000/100000, policy loss 0.4960086941719055 \tepoch 0/100 return: 394.0\n","epoch 10/100 return: 331.0\n","epoch 20/100 return: 296.0\n","epoch 30/100 return: 316.0\n","epoch 40/100 return: 295.0\n","epoch 50/100 return: 358.0\n","epoch 60/100 return: 304.0\n","epoch 70/100 return: 296.0\n","epoch 80/100 return: 300.0\n","epoch 90/100 return: 375.0\n","###############    Reward for test environment for run 9: 328.36.   ###############\n","\n","\n","Run 10 out of 10\n","state_dim 8\n","epoch 99000/100000, policy loss 0.5673857927322388 \tepoch 0/100 return: 500.0\n","epoch 10/100 return: 318.0\n","epoch 20/100 return: 254.0\n","epoch 30/100 return: 500.0\n","epoch 40/100 return: 111.0\n","epoch 50/100 return: 500.0\n","epoch 60/100 return: 112.0\n","epoch 70/100 return: 109.0\n","epoch 80/100 return: 227.0\n","epoch 90/100 return: 243.0\n","###############    Reward for test environment for run 10: 313.31.   ###############\n","\n","\n","Average reward for 10 repetitions: 218.49099999999999\n","ALL RESULTS TRAIL: [218.49099999999999]\n","Config: {'ENV': 'CartPole-v1', 'ALG': 'FINAL_Apr24_CCILStudent_origindata_multiply_trajnum20', 'NUM_TRAJS_GIVEN': 20, 'NUM_TRAINING_ENVS': 2, 'NOISE_DIM': 4, 'REP_SIZE': 16, 'TRAJ_SHIFT': 20, 'SAMPLING_RATE': 5, 'NUM_STEPS_TRAIN': 100000, 'NUM_TRAJS_VALID': 100, 'NUM_REPETITIONS': 10, 'BATCH_SIZE': 64, 'MLP_WIDTHS': 64, 'ADAM_ALPHA': 0.001, 'SGLD_BUFFER_SIZE': 10000, 'SGLD_LEARN_RATE': 0.01, 'SGLD_NOISE_COEF': 0.01, 'SGLD_NUM_STEPS': 100, 'SGLD_REINIT_FREQ': 0.05, 'NUM_STEPS_TRAIN_ENERGY_MODEL': 1000, 'TRIAL': 0, 'METHOD': 'CCIL', 'MASK_PROB': 0.5, 'EXPERT_ALG': 'dqn'}\n","Trial number 0\n","config method =  CCIL\n","config env =  CartPole-v1\n","Run 1 out of 10\n","state_dim 8\n","epoch 99000/100000, policy loss 0.5032223463058472 \tepoch 0/100 return: 146.0\n","epoch 10/100 return: 250.0\n","epoch 20/100 return: 131.0\n","epoch 30/100 return: 310.0\n","epoch 40/100 return: 155.0\n","epoch 50/100 return: 175.0\n","epoch 60/100 return: 230.0\n","epoch 70/100 return: 186.0\n","epoch 80/100 return: 149.0\n","epoch 90/100 return: 401.0\n","###############    Reward for test environment for run 1: 182.08.   ###############\n","\n","\n","Run 2 out of 10\n","state_dim 8\n","epoch 99000/100000, policy loss 0.4939891993999481 \tepoch 0/100 return: 101.0\n","epoch 10/100 return: 125.0\n","epoch 20/100 return: 238.0\n","epoch 30/100 return: 182.0\n","epoch 40/100 return: 205.0\n","epoch 50/100 return: 187.0\n","epoch 60/100 return: 205.0\n","epoch 70/100 return: 198.0\n","epoch 80/100 return: 193.0\n","epoch 90/100 return: 134.0\n","###############    Reward for test environment for run 2: 208.16.   ###############\n","\n","\n","Run 3 out of 10\n","state_dim 8\n","epoch 99000/100000, policy loss 0.47254496812820435 \tepoch 0/100 return: 132.0\n","epoch 10/100 return: 132.0\n","epoch 20/100 return: 127.0\n","epoch 30/100 return: 92.0\n","epoch 40/100 return: 111.0\n","epoch 50/100 return: 162.0\n","epoch 60/100 return: 139.0\n","epoch 70/100 return: 120.0\n","epoch 80/100 return: 129.0\n","epoch 90/100 return: 188.0\n","###############    Reward for test environment for run 3: 134.85.   ###############\n","\n","\n","Run 4 out of 10\n","state_dim 8\n","epoch 99000/100000, policy loss 0.46808359026908875 \tepoch 0/100 return: 144.0\n","epoch 10/100 return: 277.0\n","epoch 20/100 return: 171.0\n","epoch 30/100 return: 137.0\n","epoch 40/100 return: 160.0\n","epoch 50/100 return: 121.0\n","epoch 60/100 return: 187.0\n","epoch 70/100 return: 500.0\n","epoch 80/100 return: 136.0\n","epoch 90/100 return: 156.0\n","###############    Reward for test environment for run 4: 238.03.   ###############\n","\n","\n","Run 5 out of 10\n","state_dim 8\n","epoch 99000/100000, policy loss 0.5179241895675659 \tepoch 0/100 return: 133.0\n","epoch 10/100 return: 102.0\n","epoch 20/100 return: 138.0\n","epoch 30/100 return: 95.0\n","epoch 40/100 return: 140.0\n","epoch 50/100 return: 145.0\n","epoch 60/100 return: 141.0\n","epoch 70/100 return: 149.0\n","epoch 80/100 return: 87.0\n","epoch 90/100 return: 73.0\n","###############    Reward for test environment for run 5: 152.13.   ###############\n","\n","\n","Run 6 out of 10\n","state_dim 8\n","epoch 99000/100000, policy loss 0.5413562655448914 \tepoch 0/100 return: 123.0\n","epoch 10/100 return: 146.0\n","epoch 20/100 return: 126.0\n","epoch 30/100 return: 127.0\n","epoch 40/100 return: 94.0\n","epoch 50/100 return: 93.0\n","epoch 60/100 return: 124.0\n","epoch 70/100 return: 169.0\n","epoch 80/100 return: 500.0\n","epoch 90/100 return: 156.0\n","###############    Reward for test environment for run 6: 175.18.   ###############\n","\n","\n","Run 7 out of 10\n","state_dim 8\n","epoch 99000/100000, policy loss 0.5551090836524963 \tepoch 0/100 return: 81.0\n","epoch 10/100 return: 79.0\n","epoch 20/100 return: 58.0\n","epoch 30/100 return: 71.0\n","epoch 40/100 return: 65.0\n","epoch 50/100 return: 189.0\n","epoch 60/100 return: 67.0\n","epoch 70/100 return: 172.0\n","epoch 80/100 return: 72.0\n","epoch 90/100 return: 74.0\n","###############    Reward for test environment for run 7: 84.88.   ###############\n","\n","\n","Run 8 out of 10\n","state_dim 8\n","epoch 99000/100000, policy loss 0.4599759578704834 \tepoch 0/100 return: 500.0\n","epoch 10/100 return: 95.0\n","epoch 20/100 return: 83.0\n","epoch 30/100 return: 103.0\n","epoch 40/100 return: 500.0\n","epoch 50/100 return: 74.0\n","epoch 60/100 return: 81.0\n","epoch 70/100 return: 119.0\n","epoch 80/100 return: 500.0\n","epoch 90/100 return: 82.0\n","###############    Reward for test environment for run 8: 178.86.   ###############\n","\n","\n","Run 9 out of 10\n","state_dim 8\n","epoch 99000/100000, policy loss 0.49194562435150146 \tepoch 0/100 return: 83.0\n","epoch 10/100 return: 77.0\n","epoch 20/100 return: 73.0\n","epoch 30/100 return: 59.0\n","epoch 40/100 return: 65.0\n","epoch 50/100 return: 71.0\n","epoch 60/100 return: 78.0\n","epoch 70/100 return: 75.0\n","epoch 80/100 return: 68.0\n","epoch 90/100 return: 76.0\n","###############    Reward for test environment for run 9: 101.55.   ###############\n","\n","\n","Run 10 out of 10\n","state_dim 8\n","epoch 99000/100000, policy loss 0.5118893980979919 \tepoch 0/100 return: 190.0\n","epoch 10/100 return: 104.0\n","epoch 20/100 return: 189.0\n","epoch 30/100 return: 110.0\n","epoch 40/100 return: 213.0\n","epoch 50/100 return: 110.0\n","epoch 60/100 return: 135.0\n","epoch 70/100 return: 175.0\n","epoch 80/100 return: 138.0\n","epoch 90/100 return: 108.0\n","###############    Reward for test environment for run 10: 158.2.   ###############\n","\n","\n","Average reward for 10 repetitions: 161.392\n","ALL RESULTS TRAIL: [161.392]\n","Config: {'ENV': 'CartPole-v1', 'ALG': 'FINAL_Apr24_CCILStudent_origindata_multiply_trajnum30', 'NUM_TRAJS_GIVEN': 30, 'NUM_TRAINING_ENVS': 2, 'NOISE_DIM': 4, 'REP_SIZE': 16, 'TRAJ_SHIFT': 30, 'SAMPLING_RATE': 5, 'NUM_STEPS_TRAIN': 100000, 'NUM_TRAJS_VALID': 100, 'NUM_REPETITIONS': 10, 'BATCH_SIZE': 64, 'MLP_WIDTHS': 64, 'ADAM_ALPHA': 0.001, 'SGLD_BUFFER_SIZE': 10000, 'SGLD_LEARN_RATE': 0.01, 'SGLD_NOISE_COEF': 0.01, 'SGLD_NUM_STEPS': 100, 'SGLD_REINIT_FREQ': 0.05, 'NUM_STEPS_TRAIN_ENERGY_MODEL': 1000, 'TRIAL': 0, 'METHOD': 'CCIL', 'MASK_PROB': 0.5, 'EXPERT_ALG': 'dqn'}\n","Trial number 0\n","config method =  CCIL\n","config env =  CartPole-v1\n","Run 1 out of 10\n","state_dim 8\n","epoch 99000/100000, policy loss 0.4055238366127014 \tepoch 0/100 return: 94.0\n","epoch 10/100 return: 129.0\n","epoch 20/100 return: 77.0\n","epoch 30/100 return: 157.0\n","epoch 40/100 return: 104.0\n","epoch 50/100 return: 255.0\n","epoch 60/100 return: 63.0\n","epoch 70/100 return: 79.0\n","epoch 80/100 return: 81.0\n","epoch 90/100 return: 83.0\n","###############    Reward for test environment for run 1: 123.18.   ###############\n","\n","\n","Run 2 out of 10\n","state_dim 8\n","epoch 99000/100000, policy loss 0.4034263789653778 \tepoch 0/100 return: 262.0\n","epoch 10/100 return: 500.0\n","epoch 20/100 return: 219.0\n","epoch 30/100 return: 207.0\n","epoch 40/100 return: 145.0\n","epoch 50/100 return: 203.0\n","epoch 60/100 return: 207.0\n","epoch 70/100 return: 214.0\n","epoch 80/100 return: 253.0\n","epoch 90/100 return: 500.0\n","###############    Reward for test environment for run 2: 232.76.   ###############\n","\n","\n","Run 3 out of 10\n","state_dim 8\n","epoch 99000/100000, policy loss 0.5447420477867126 \tepoch 0/100 return: 328.0\n","epoch 10/100 return: 329.0\n","epoch 20/100 return: 453.0\n","epoch 30/100 return: 500.0\n","epoch 40/100 return: 338.0\n","epoch 50/100 return: 495.0\n","epoch 60/100 return: 446.0\n","epoch 70/100 return: 401.0\n","epoch 80/100 return: 500.0\n","epoch 90/100 return: 500.0\n","###############    Reward for test environment for run 3: 401.11.   ###############\n","\n","\n","Run 4 out of 10\n","state_dim 8\n","epoch 99000/100000, policy loss 0.5433851480484009 \tepoch 0/100 return: 500.0\n","epoch 10/100 return: 85.0\n","epoch 20/100 return: 500.0\n","epoch 30/100 return: 500.0\n","epoch 40/100 return: 87.0\n","epoch 50/100 return: 131.0\n","epoch 60/100 return: 500.0\n","epoch 70/100 return: 73.0\n","epoch 80/100 return: 500.0\n","epoch 90/100 return: 88.0\n","###############    Reward for test environment for run 4: 292.08.   ###############\n","\n","\n","Run 5 out of 10\n","state_dim 8\n","epoch 99000/100000, policy loss 0.5354658961296082 \tepoch 0/100 return: 117.0\n","epoch 10/100 return: 262.0\n","epoch 20/100 return: 131.0\n","epoch 30/100 return: 154.0\n","epoch 40/100 return: 137.0\n","epoch 50/100 return: 120.0\n","epoch 60/100 return: 183.0\n","epoch 70/100 return: 212.0\n","epoch 80/100 return: 150.0\n","epoch 90/100 return: 179.0\n","###############    Reward for test environment for run 5: 172.43.   ###############\n","\n","\n","Run 6 out of 10\n","state_dim 8\n","epoch 99000/100000, policy loss 0.4373109042644501 \tepoch 0/100 return: 95.0\n","epoch 10/100 return: 83.0\n","epoch 20/100 return: 68.0\n","epoch 30/100 return: 103.0\n","epoch 40/100 return: 89.0\n","epoch 50/100 return: 79.0\n","epoch 60/100 return: 74.0\n","epoch 70/100 return: 64.0\n","epoch 80/100 return: 73.0\n","epoch 90/100 return: 91.0\n","###############    Reward for test environment for run 6: 91.19.   ###############\n","\n","\n","Run 7 out of 10\n","state_dim 8\n","epoch 99000/100000, policy loss 0.517244279384613 \tepoch 0/100 return: 120.0\n","epoch 10/100 return: 77.0\n","epoch 20/100 return: 137.0\n","epoch 30/100 return: 81.0\n","epoch 40/100 return: 81.0\n","epoch 50/100 return: 83.0\n","epoch 60/100 return: 130.0\n","epoch 70/100 return: 71.0\n","epoch 80/100 return: 102.0\n","epoch 90/100 return: 92.0\n","###############    Reward for test environment for run 7: 133.42.   ###############\n","\n","\n","Run 8 out of 10\n","state_dim 8\n","epoch 99000/100000, policy loss 0.4800601005554199 \tepoch 0/100 return: 369.0\n","epoch 10/100 return: 205.0\n","epoch 20/100 return: 261.0\n","epoch 30/100 return: 367.0\n","epoch 40/100 return: 286.0\n","epoch 50/100 return: 375.0\n","epoch 60/100 return: 273.0\n","epoch 70/100 return: 283.0\n","epoch 80/100 return: 216.0\n","epoch 90/100 return: 321.0\n","###############    Reward for test environment for run 8: 321.99.   ###############\n","\n","\n","Run 9 out of 10\n","state_dim 8\n","epoch 99000/100000, policy loss 0.5633431077003479 \tepoch 0/100 return: 417.0\n","epoch 10/100 return: 500.0\n","epoch 20/100 return: 500.0\n","epoch 30/100 return: 370.0\n","epoch 40/100 return: 350.0\n","epoch 50/100 return: 500.0\n","epoch 60/100 return: 500.0\n","epoch 70/100 return: 500.0\n","epoch 80/100 return: 500.0\n","epoch 90/100 return: 500.0\n","###############    Reward for test environment for run 9: 441.85.   ###############\n","\n","\n","Run 10 out of 10\n","state_dim 8\n","epoch 99000/100000, policy loss 0.5404860377311707 \tepoch 0/100 return: 86.0\n","epoch 10/100 return: 61.0\n","epoch 20/100 return: 60.0\n","epoch 30/100 return: 66.0\n","epoch 40/100 return: 110.0\n","epoch 50/100 return: 65.0\n","epoch 60/100 return: 57.0\n","epoch 70/100 return: 90.0\n","epoch 80/100 return: 90.0\n","epoch 90/100 return: 83.0\n","###############    Reward for test environment for run 10: 96.97.   ###############\n","\n","\n","Average reward for 10 repetitions: 230.698\n","ALL RESULTS TRAIL: [230.698]\n","Config: {'ENV': 'CartPole-v1', 'ALG': 'FINAL_Apr24_CCILStudent_origindata_multiply_trajnum40', 'NUM_TRAJS_GIVEN': 40, 'NUM_TRAINING_ENVS': 2, 'NOISE_DIM': 4, 'REP_SIZE': 16, 'TRAJ_SHIFT': 40, 'SAMPLING_RATE': 5, 'NUM_STEPS_TRAIN': 100000, 'NUM_TRAJS_VALID': 100, 'NUM_REPETITIONS': 10, 'BATCH_SIZE': 64, 'MLP_WIDTHS': 64, 'ADAM_ALPHA': 0.001, 'SGLD_BUFFER_SIZE': 10000, 'SGLD_LEARN_RATE': 0.01, 'SGLD_NOISE_COEF': 0.01, 'SGLD_NUM_STEPS': 100, 'SGLD_REINIT_FREQ': 0.05, 'NUM_STEPS_TRAIN_ENERGY_MODEL': 1000, 'TRIAL': 0, 'METHOD': 'CCIL', 'MASK_PROB': 0.5, 'EXPERT_ALG': 'dqn'}\n","Trial number 0\n","config method =  CCIL\n","config env =  CartPole-v1\n","Run 1 out of 10\n","state_dim 8\n","epoch 99000/100000, policy loss 0.6610614657402039 \tepoch 0/100 return: 102.0\n","epoch 10/100 return: 500.0\n","epoch 20/100 return: 113.0\n","epoch 30/100 return: 129.0\n","epoch 40/100 return: 102.0\n","epoch 50/100 return: 126.0\n","epoch 60/100 return: 135.0\n","epoch 70/100 return: 131.0\n","epoch 80/100 return: 108.0\n","epoch 90/100 return: 109.0\n","###############    Reward for test environment for run 1: 170.16.   ###############\n","\n","\n","Run 2 out of 10\n","state_dim 8\n","epoch 99000/100000, policy loss 0.4552469849586487 \tepoch 0/100 return: 500.0\n","epoch 10/100 return: 500.0\n","epoch 20/100 return: 98.0\n","epoch 30/100 return: 106.0\n","epoch 40/100 return: 235.0\n","epoch 50/100 return: 500.0\n","epoch 60/100 return: 92.0\n","epoch 70/100 return: 197.0\n","epoch 80/100 return: 277.0\n","epoch 90/100 return: 271.0\n","###############    Reward for test environment for run 2: 282.69.   ###############\n","\n","\n","Run 3 out of 10\n","state_dim 8\n","epoch 99000/100000, policy loss 0.45000025629997253 \tepoch 0/100 return: 158.0\n","epoch 10/100 return: 212.0\n","epoch 20/100 return: 144.0\n","epoch 30/100 return: 262.0\n","epoch 40/100 return: 232.0\n","epoch 50/100 return: 189.0\n","epoch 60/100 return: 166.0\n","epoch 70/100 return: 121.0\n","epoch 80/100 return: 106.0\n","epoch 90/100 return: 163.0\n","###############    Reward for test environment for run 3: 212.35.   ###############\n","\n","\n","Run 4 out of 10\n","state_dim 8\n","epoch 99000/100000, policy loss 0.41651836037635803 \tepoch 0/100 return: 82.0\n","epoch 10/100 return: 94.0\n","epoch 20/100 return: 79.0\n","epoch 30/100 return: 72.0\n","epoch 40/100 return: 73.0\n","epoch 50/100 return: 90.0\n","epoch 60/100 return: 77.0\n","epoch 70/100 return: 77.0\n","epoch 80/100 return: 101.0\n","epoch 90/100 return: 87.0\n","###############    Reward for test environment for run 4: 101.57.   ###############\n","\n","\n","Run 5 out of 10\n","state_dim 8\n","epoch 99000/100000, policy loss 0.4007575809955597 \tepoch 0/100 return: 117.0\n","epoch 10/100 return: 165.0\n","epoch 20/100 return: 69.0\n","epoch 30/100 return: 52.0\n","epoch 40/100 return: 70.0\n","epoch 50/100 return: 181.0\n","epoch 60/100 return: 48.0\n","epoch 70/100 return: 47.0\n","epoch 80/100 return: 61.0\n","epoch 90/100 return: 51.0\n","###############    Reward for test environment for run 5: 94.8.   ###############\n","\n","\n","Run 6 out of 10\n","state_dim 8\n","epoch 99000/100000, policy loss 0.38622599840164185 \tepoch 0/100 return: 204.0\n","epoch 10/100 return: 165.0\n","epoch 20/100 return: 190.0\n","epoch 30/100 return: 327.0\n","epoch 40/100 return: 206.0\n","epoch 50/100 return: 201.0\n","epoch 60/100 return: 360.0\n","epoch 70/100 return: 217.0\n","epoch 80/100 return: 343.0\n","epoch 90/100 return: 317.0\n","###############    Reward for test environment for run 6: 274.51.   ###############\n","\n","\n","Run 7 out of 10\n","state_dim 8\n","epoch 99000/100000, policy loss 0.4555375874042511 \tepoch 0/100 return: 128.0\n","epoch 10/100 return: 346.0\n","epoch 20/100 return: 130.0\n","epoch 30/100 return: 259.0\n","epoch 40/100 return: 114.0\n","epoch 50/100 return: 500.0\n","epoch 60/100 return: 254.0\n","epoch 70/100 return: 262.0\n","epoch 80/100 return: 272.0\n","epoch 90/100 return: 500.0\n","###############    Reward for test environment for run 7: 257.8.   ###############\n","\n","\n","Run 8 out of 10\n","state_dim 8\n","epoch 99000/100000, policy loss 0.3488893508911133 \tepoch 0/100 return: 93.0\n","epoch 10/100 return: 107.0\n","epoch 20/100 return: 86.0\n","epoch 30/100 return: 120.0\n","epoch 40/100 return: 90.0\n","epoch 50/100 return: 80.0\n","epoch 60/100 return: 85.0\n","epoch 70/100 return: 89.0\n","epoch 80/100 return: 115.0\n","epoch 90/100 return: 125.0\n","###############    Reward for test environment for run 8: 98.9.   ###############\n","\n","\n","Run 9 out of 10\n","state_dim 8\n","epoch 99000/100000, policy loss 0.3837101459503174 \tepoch 0/100 return: 85.0\n","epoch 10/100 return: 71.0\n","epoch 20/100 return: 103.0\n","epoch 30/100 return: 83.0\n","epoch 40/100 return: 111.0\n","epoch 50/100 return: 89.0\n","epoch 60/100 return: 87.0\n","epoch 70/100 return: 85.0\n","epoch 80/100 return: 110.0\n","epoch 90/100 return: 81.0\n","###############    Reward for test environment for run 9: 104.67.   ###############\n","\n","\n","Run 10 out of 10\n","state_dim 8\n","epoch 99000/100000, policy loss 0.47877922654151917 \tepoch 0/100 return: 141.0\n","epoch 10/100 return: 500.0\n","epoch 20/100 return: 200.0\n","epoch 30/100 return: 130.0\n","epoch 40/100 return: 159.0\n","epoch 50/100 return: 158.0\n","epoch 60/100 return: 100.0\n","epoch 70/100 return: 143.0\n","epoch 80/100 return: 99.0\n","epoch 90/100 return: 87.0\n","###############    Reward for test environment for run 10: 180.49.   ###############\n","\n","\n","Average reward for 10 repetitions: 177.794\n","ALL RESULTS TRAIL: [177.794]\n","Config: {'ENV': 'CartPole-v1', 'ALG': 'FINAL_Apr24_CCILStudent_origindata_multiply_trajnum50', 'NUM_TRAJS_GIVEN': 50, 'NUM_TRAINING_ENVS': 2, 'NOISE_DIM': 4, 'REP_SIZE': 16, 'TRAJ_SHIFT': 50, 'SAMPLING_RATE': 5, 'NUM_STEPS_TRAIN': 100000, 'NUM_TRAJS_VALID': 100, 'NUM_REPETITIONS': 10, 'BATCH_SIZE': 64, 'MLP_WIDTHS': 64, 'ADAM_ALPHA': 0.001, 'SGLD_BUFFER_SIZE': 10000, 'SGLD_LEARN_RATE': 0.01, 'SGLD_NOISE_COEF': 0.01, 'SGLD_NUM_STEPS': 100, 'SGLD_REINIT_FREQ': 0.05, 'NUM_STEPS_TRAIN_ENERGY_MODEL': 1000, 'TRIAL': 0, 'METHOD': 'CCIL', 'MASK_PROB': 0.5, 'EXPERT_ALG': 'dqn'}\n","Trial number 0\n","config method =  CCIL\n","config env =  CartPole-v1\n","Run 1 out of 10\n","state_dim 8\n","epoch 99000/100000, policy loss 0.5176186561584473 \tepoch 0/100 return: 136.0\n","epoch 10/100 return: 80.0\n","epoch 20/100 return: 101.0\n","epoch 30/100 return: 96.0\n","epoch 40/100 return: 117.0\n","epoch 50/100 return: 100.0\n","epoch 60/100 return: 107.0\n","epoch 70/100 return: 80.0\n","epoch 80/100 return: 84.0\n","epoch 90/100 return: 115.0\n","###############    Reward for test environment for run 1: 102.8.   ###############\n","\n","\n","Run 2 out of 10\n","state_dim 8\n","epoch 99000/100000, policy loss 0.44479477405548096 \tepoch 0/100 return: 104.0\n","epoch 10/100 return: 69.0\n","epoch 20/100 return: 68.0\n","epoch 30/100 return: 118.0\n","epoch 40/100 return: 67.0\n","epoch 50/100 return: 76.0\n","epoch 60/100 return: 95.0\n","epoch 70/100 return: 87.0\n","epoch 80/100 return: 102.0\n","epoch 90/100 return: 128.0\n","###############    Reward for test environment for run 2: 101.75.   ###############\n","\n","\n","Run 3 out of 10\n","state_dim 8\n","epoch 99000/100000, policy loss 0.5553070902824402 \tepoch 0/100 return: 500.0\n","epoch 10/100 return: 91.0\n","epoch 20/100 return: 500.0\n","epoch 30/100 return: 237.0\n","epoch 40/100 return: 95.0\n","epoch 50/100 return: 500.0\n","epoch 60/100 return: 103.0\n","epoch 70/100 return: 133.0\n","epoch 80/100 return: 128.0\n","epoch 90/100 return: 117.0\n","###############    Reward for test environment for run 3: 223.73.   ###############\n","\n","\n","Run 4 out of 10\n","state_dim 8\n","epoch 99000/100000, policy loss 0.4073803722858429 \tepoch 0/100 return: 80.0\n","epoch 10/100 return: 82.0\n","epoch 20/100 return: 103.0\n","epoch 30/100 return: 132.0\n","epoch 40/100 return: 112.0\n","epoch 50/100 return: 110.0\n","epoch 60/100 return: 117.0\n","epoch 70/100 return: 165.0\n","epoch 80/100 return: 170.0\n","epoch 90/100 return: 69.0\n","###############    Reward for test environment for run 4: 113.27.   ###############\n","\n","\n","Run 5 out of 10\n","state_dim 8\n","epoch 99000/100000, policy loss 0.5127930045127869 \tepoch 0/100 return: 110.0\n","epoch 10/100 return: 137.0\n","epoch 20/100 return: 85.0\n","epoch 30/100 return: 93.0\n","epoch 40/100 return: 103.0\n","epoch 50/100 return: 93.0\n","epoch 60/100 return: 94.0\n","epoch 70/100 return: 241.0\n","epoch 80/100 return: 331.0\n","epoch 90/100 return: 500.0\n","###############    Reward for test environment for run 5: 200.63.   ###############\n","\n","\n","Run 6 out of 10\n","state_dim 8\n","epoch 99000/100000, policy loss 0.5264303684234619 \tepoch 0/100 return: 271.0\n","epoch 10/100 return: 249.0\n","epoch 20/100 return: 157.0\n","epoch 30/100 return: 373.0\n","epoch 40/100 return: 241.0\n","epoch 50/100 return: 231.0\n","epoch 60/100 return: 500.0\n","epoch 70/100 return: 259.0\n","epoch 80/100 return: 248.0\n","epoch 90/100 return: 145.0\n","###############    Reward for test environment for run 6: 302.22.   ###############\n","\n","\n","Run 7 out of 10\n","state_dim 8\n","epoch 99000/100000, policy loss 0.41589221358299255 \tepoch 0/100 return: 151.0\n","epoch 10/100 return: 161.0\n","epoch 20/100 return: 97.0\n","epoch 30/100 return: 107.0\n","epoch 40/100 return: 171.0\n","epoch 50/100 return: 115.0\n","epoch 60/100 return: 169.0\n","epoch 70/100 return: 151.0\n","epoch 80/100 return: 88.0\n","epoch 90/100 return: 122.0\n","###############    Reward for test environment for run 7: 150.54.   ###############\n","\n","\n","Run 8 out of 10\n","state_dim 8\n","epoch 99000/100000, policy loss 0.41012638807296753 \tepoch 0/100 return: 82.0\n","epoch 10/100 return: 81.0\n","epoch 20/100 return: 115.0\n","epoch 30/100 return: 143.0\n","epoch 40/100 return: 90.0\n","epoch 50/100 return: 69.0\n","epoch 60/100 return: 90.0\n","epoch 70/100 return: 102.0\n","epoch 80/100 return: 101.0\n","epoch 90/100 return: 79.0\n","###############    Reward for test environment for run 8: 102.38.   ###############\n","\n","\n","Run 9 out of 10\n","state_dim 8\n","epoch 99000/100000, policy loss 0.5735216736793518 \tepoch 0/100 return: 124.0\n","epoch 10/100 return: 110.0\n","epoch 20/100 return: 133.0\n","epoch 30/100 return: 144.0\n","epoch 40/100 return: 142.0\n","epoch 50/100 return: 103.0\n","epoch 60/100 return: 119.0\n","epoch 70/100 return: 104.0\n","epoch 80/100 return: 133.0\n","epoch 90/100 return: 152.0\n","###############    Reward for test environment for run 9: 128.13.   ###############\n","\n","\n","Run 10 out of 10\n","state_dim 8\n","epoch 99000/100000, policy loss 0.484486848115921 \tepoch 0/100 return: 81.0\n","epoch 10/100 return: 86.0\n","epoch 20/100 return: 60.0\n","epoch 30/100 return: 67.0\n","epoch 40/100 return: 97.0\n","epoch 50/100 return: 60.0\n","epoch 60/100 return: 84.0\n","epoch 70/100 return: 83.0\n","epoch 80/100 return: 60.0\n","epoch 90/100 return: 104.0\n","###############    Reward for test environment for run 10: 90.29.   ###############\n","\n","\n","Average reward for 10 repetitions: 151.57399999999998\n","ALL RESULTS TRAIL: [151.57399999999998]\n"]}]},{"cell_type":"code","source":[""],"metadata":{"id":"dq08yZ2C6EhZ"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["#10 Trails -- CCIL -- acrobot"],"metadata":{"id":"oraDNzHah0fl"}},{"cell_type":"code","source":["config['MASK_PROB'] = 0.5\n","config['NUM_STEPS_TRAIN'] = 100000\n","config['ADAM_ALPHA'] = 0.001\n","config['NUM_REPETITIONS'] = 10\n","config['ENV'] = \"Acrobot-v1\""],"metadata":{"id":"gAL4LjJoh0fl"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["config['METHOD'] = \"CCIL\"\n","\n","for traj_num in [1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 20]:\n","    config[\"NUM_TRAJS_GIVEN\"] = traj_num\n","    config[\"TRAJ_SHIFT\"] = traj_num\n","\n","    config['ALG'] = \"FINAL_Apr24_CCILStudent_origindata_multiply_trajnum\" + str(traj_num)\n","\n","\n","    ###############.  settings   ###############\n","    #config['ALG'] = \"BCIRMStudent_Apr19_replicatedata\"\n","    #config['METHOD'] = \"BCIRM\"\n","    #config['METHOD'] = \"ICIL\"\n","    #config[\"NUM_TRAJS_GIVEN\"] = 50\n","    #config[\"TRAJ_SHIFT\"] = 50\n","    #config['ENV'] == \"CartPole-v1\"\n","    ###############.  settings   ###############\n","\n","    all_results_trail = []\n","\n","    for trail in range(1): \n","        config['TRIAL'] = trail \n","\n","\n","        ###############.  start a trail   ###############\n","\n","        config[\"EXPERT_ALG\"] = yaml.load(open(\"testing/config.yml\"), Loader=yaml.FullLoader)[config[\"ENV\"]]\n","        print(\"Config: %s\" % config)\n","\n","        TRIAL = config[\"TRIAL\"] #args.trial\n","        print(\"Trial number %s\" % TRIAL)\n","\n","        results_dir_base = \"testing/results/\"\n","        results_dir = os.path.join(results_dir_base, config[\"ENV\"], str(config[\"NUM_TRAJS_GIVEN\"]), config[\"ALG\"])\n","\n","        if not os.path.exists(results_dir):\n","            os.makedirs(results_dir)\n","\n","        config_file = \"trial_\" + str(TRIAL) + \"_\" + \"config.pkl\"\n","\n","        results_file_name = \"trial_\" + str(TRIAL) + \"_\" + \"results.csv\"\n","        results_file_path = os.path.join(results_dir, results_file_name)\n","\n","        if os.path.exists(os.path.join(results_dir, config_file)):\n","            raise NameError(\"CONFIG file already exists %s. Choose a different trial number.\" % config_file)\n","        pickle.dump(config, open(os.path.join(results_dir, config_file), \"wb\"))\n","\n","\n","\n","\n","        ###############.  10 runs for each trail   ###############\n","\n","        print(\"config method = \", config['METHOD'])\n","        print(\"config env = \", config['ENV'])\n","\n","        for run_seed in range(config[\"NUM_REPETITIONS\"]):\n","            print(\"Run %s out of %s\" % (run_seed + 1, config[\"NUM_REPETITIONS\"]))\n","            student = make_student(run_seed, config)\n","            student.train(num_updates=config[\"NUM_STEPS_TRAIN\"])\n","\n","            env_wrapper_out_of_sample = EnvWrapper(\n","                env=gym.make(config[\"ENV\"]), mult_factor=get_test_mult_factors(config['NOISE_DIM'] - 1), idx=3, seed=1\n","            )\n","\n","            env_wrapper_out_of_sample.noise = 0\n","\n","            action_match, return_mean, return_std = student.test(\n","                num_episodes=config[\"NUM_TRAJS_VALID\"], env_wrapper=env_wrapper_out_of_sample\n","            )\n","\n","            result = (action_match, return_mean, return_std)\n","            print(\"###############    Reward for test environment for run %s: %s.   ###############\\n\\n\" % (run_seed + 1, return_mean))\n","            save_results(results_file_path, run_seed, action_match, return_mean, return_std)\n","\n","        results_trial = pd.read_csv(\n","            \"testing/results/\"\n","            + config[\"ENV\"]\n","            + \"/\"\n","            + str(config[\"NUM_TRAJS_GIVEN\"])\n","            + \"/\"\n","            + config[\"ALG\"]\n","            + \"/trial_\"\n","            + str(TRIAL)\n","            + \"_results.csv\",\n","            header=None,\n","        )\n","\n","        print(\"Average reward for 10 repetitions: %s\" % np.mean(results_trial[2].values))\n","\n","        all_results_trail.append(np.mean(results_trial[2].values))\n","    print(\"ALL RESULTS TRAIL:\" , all_results_trail)\n","\n","\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"outputId":"e39d1cd3-9842-4968-c07a-12f5f2d7836d","id":"SslrdBJEh0fl"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Config: {'ENV': 'CartPole-v1', 'ALG': 'FINAL_Apr24_CCILStudent_origindata_multiply_trajnum1', 'NUM_TRAJS_GIVEN': 1, 'NUM_TRAINING_ENVS': 2, 'NOISE_DIM': 4, 'REP_SIZE': 16, 'TRAJ_SHIFT': 1, 'SAMPLING_RATE': 5, 'NUM_STEPS_TRAIN': 100000, 'NUM_TRAJS_VALID': 100, 'NUM_REPETITIONS': 10, 'BATCH_SIZE': 64, 'MLP_WIDTHS': 64, 'ADAM_ALPHA': 0.001, 'SGLD_BUFFER_SIZE': 10000, 'SGLD_LEARN_RATE': 0.01, 'SGLD_NOISE_COEF': 0.01, 'SGLD_NUM_STEPS': 100, 'SGLD_REINIT_FREQ': 0.05, 'NUM_STEPS_TRAIN_ENERGY_MODEL': 1000, 'TRIAL': 0, 'METHOD': 'CCIL', 'MASK_PROB': 0.5, 'EXPERT_ALG': 'dqn'}\n","Trial number 0\n","config method =  CCIL\n","config env =  CartPole-v1\n","Run 1 out of 10\n","state_dim 8\n","epoch 2000/100000, policy loss 0.5670180916786194 \t"]}]},{"cell_type":"code","source":[""],"metadata":{"id":"jT1UWt646AK9"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["#10 Trails -- CCIL -- lunarlander * * *"],"metadata":{"id":"S80hq3a2h1Nt"}},{"cell_type":"code","source":["config['MASK_PROB'] = 0.5\n","config['NUM_STEPS_TRAIN'] = 100000\n","config['ADAM_ALPHA'] = 0.001\n","config['NUM_REPETITIONS'] = 10\n","config['ENV'] = 'LunarLander-v2'"],"metadata":{"id":"MNB_ZtF0h1Nt"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["config['METHOD'] = \"CCIL\"\n","\n","for traj_num in [2, 4, 8, 16, 32, 64, 128]:\n","    config[\"NUM_TRAJS_GIVEN\"] = traj_num\n","    config[\"TRAJ_SHIFT\"] = traj_num\n","\n","    config['ALG'] = \"FINAL_Apr24_CCILStudent_origindata_multiply_trajnum\" + str(traj_num)\n","\n","\n","    ###############.  settings   ###############\n","    #config['ALG'] = \"BCIRMStudent_Apr19_replicatedata\"\n","    #config['METHOD'] = \"BCIRM\"\n","    #config['METHOD'] = \"ICIL\"\n","    #config[\"NUM_TRAJS_GIVEN\"] = 50\n","    #config[\"TRAJ_SHIFT\"] = 50\n","    #config['ENV'] == \"CartPole-v1\"\n","    ###############.  settings   ###############\n","\n","    all_results_trail = []\n","\n","    for trail in range(1): \n","        config['TRIAL'] = trail \n","\n","\n","        ###############.  start a trail   ###############\n","\n","        config[\"EXPERT_ALG\"] = yaml.load(open(\"testing/config.yml\"), Loader=yaml.FullLoader)[config[\"ENV\"]]\n","        print(\"Config: %s\" % config)\n","\n","        TRIAL = config[\"TRIAL\"] #args.trial\n","        print(\"Trial number %s\" % TRIAL)\n","\n","        results_dir_base = \"testing/results/\"\n","        results_dir = os.path.join(results_dir_base, config[\"ENV\"], str(config[\"NUM_TRAJS_GIVEN\"]), config[\"ALG\"])\n","\n","        if not os.path.exists(results_dir):\n","            os.makedirs(results_dir)\n","\n","        config_file = \"trial_\" + str(TRIAL) + \"_\" + \"config.pkl\"\n","\n","        results_file_name = \"trial_\" + str(TRIAL) + \"_\" + \"results.csv\"\n","        results_file_path = os.path.join(results_dir, results_file_name)\n","\n","        if os.path.exists(os.path.join(results_dir, config_file)):\n","            raise NameError(\"CONFIG file already exists %s. Choose a different trial number.\" % config_file)\n","        pickle.dump(config, open(os.path.join(results_dir, config_file), \"wb\"))\n","\n","\n","\n","\n","        ###############.  10 runs for each trail   ###############\n","\n","        print(\"config method = \", config['METHOD'])\n","        print(\"config env = \", config['ENV'])\n","\n","        for run_seed in range(config[\"NUM_REPETITIONS\"]):\n","            print(\"Run %s out of %s\" % (run_seed + 1, config[\"NUM_REPETITIONS\"]))\n","            student = make_student(run_seed, config)\n","            student.train(num_updates=config[\"NUM_STEPS_TRAIN\"])\n","\n","            env_wrapper_out_of_sample = EnvWrapper(\n","                env=gym.make(config[\"ENV\"]), mult_factor=get_test_mult_factors(config['NOISE_DIM'] - 1), idx=3, seed=1\n","            )\n","\n","            env_wrapper_out_of_sample.noise = 0\n","\n","            action_match, return_mean, return_std = student.test(\n","                num_episodes=config[\"NUM_TRAJS_VALID\"], env_wrapper=env_wrapper_out_of_sample\n","            )\n","\n","            result = (action_match, return_mean, return_std)\n","            print(\"###############    Reward for test environment for run %s: %s.   ###############\\n\\n\" % (run_seed + 1, return_mean))\n","            save_results(results_file_path, run_seed, action_match, return_mean, return_std)\n","\n","        results_trial = pd.read_csv(\n","            \"testing/results/\"\n","            + config[\"ENV\"]\n","            + \"/\"\n","            + str(config[\"NUM_TRAJS_GIVEN\"])\n","            + \"/\"\n","            + config[\"ALG\"]\n","            + \"/trial_\"\n","            + str(TRIAL)\n","            + \"_results.csv\",\n","            header=None,\n","        )\n","\n","        print(\"Average reward for 10 repetitions: %s\" % np.mean(results_trial[2].values))\n","\n","        all_results_trail.append(np.mean(results_trial[2].values))\n","    print(\"ALL RESULTS TRAIL:\" , all_results_trail)\n","\n","\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"outputId":"9a29a765-731d-4411-aa25-de675dc3c693","id":"tbiWvnsOh1Nt","executionInfo":{"status":"ok","timestamp":1650835967768,"user_tz":240,"elapsed":20355782,"user":{"displayName":"2 Lin","userId":"15119774230930098070"}}},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Config: {'ENV': 'LunarLander-v2', 'ALG': 'FINAL_Apr24_CCILStudent_origindata_multiply_trajnum2', 'NUM_TRAJS_GIVEN': 2, 'NUM_TRAINING_ENVS': 2, 'NOISE_DIM': 4, 'REP_SIZE': 16, 'TRAJ_SHIFT': 2, 'SAMPLING_RATE': 5, 'NUM_STEPS_TRAIN': 100000, 'NUM_TRAJS_VALID': 100, 'NUM_REPETITIONS': 10, 'BATCH_SIZE': 64, 'MLP_WIDTHS': 64, 'ADAM_ALPHA': 0.001, 'SGLD_BUFFER_SIZE': 10000, 'SGLD_LEARN_RATE': 0.01, 'SGLD_NOISE_COEF': 0.01, 'SGLD_NUM_STEPS': 100, 'SGLD_REINIT_FREQ': 0.05, 'NUM_STEPS_TRAIN_ENERGY_MODEL': 1000, 'TRIAL': 0, 'METHOD': 'CCIL', 'MASK_PROB': 0.5, 'EXPERT_ALG': 'dqn'}\n","Trial number 0\n","config method =  CCIL\n","config env =  LunarLander-v2\n","Run 1 out of 10\n","state_dim 12\n","epoch 99000/100000, policy loss 0.1439100056886673 \tepoch 0/100 return: -51.186501740530886\n","epoch 10/100 return: -103.44515067553863\n","epoch 20/100 return: -305.05482144061347\n","epoch 30/100 return: -135.30832061675306\n","epoch 40/100 return: -140.86303351437462\n","epoch 50/100 return: -142.51911123170515\n","epoch 60/100 return: -334.6400113419345\n","epoch 70/100 return: -382.6321344537669\n","epoch 80/100 return: -195.10607478070895\n","epoch 90/100 return: -159.1926583507315\n","###############    Reward for test environment for run 1: -204.46890533679604.   ###############\n","\n","\n","Run 2 out of 10\n","state_dim 12\n","epoch 99000/100000, policy loss 0.14139990508556366 \tepoch 0/100 return: -84.31670422467438\n","epoch 10/100 return: -56.915301747587435\n","epoch 20/100 return: -155.08552259347312\n","epoch 30/100 return: -134.59507929367712\n","epoch 40/100 return: -158.32533839023174\n","epoch 50/100 return: -125.87211129373966\n","epoch 60/100 return: -136.34547517466183\n","epoch 70/100 return: -127.99382325994665\n","epoch 80/100 return: -62.35363334714202\n","epoch 90/100 return: -42.25456818857481\n","###############    Reward for test environment for run 2: -127.36556543149408.   ###############\n","\n","\n","Run 3 out of 10\n","state_dim 12\n","epoch 99000/100000, policy loss 0.6213228106498718 \tepoch 0/100 return: -364.3581522195721\n","epoch 10/100 return: -87.19510892158787\n","epoch 20/100 return: -160.66957951365302\n","epoch 30/100 return: -188.52071972787718\n","epoch 40/100 return: -44.35026379377145\n","epoch 50/100 return: -482.2234468994793\n","epoch 60/100 return: -93.65617005289273\n","epoch 70/100 return: -150.27540918577395\n","epoch 80/100 return: -222.27644743981654\n","epoch 90/100 return: -289.8498890392054\n","###############    Reward for test environment for run 3: -186.61632529674688.   ###############\n","\n","\n","Run 4 out of 10\n","state_dim 12\n","epoch 99000/100000, policy loss 0.2504173517227173 \tepoch 0/100 return: -601.2264119520626\n","epoch 10/100 return: -328.55015307036933\n","epoch 20/100 return: -465.13330586628416\n","epoch 30/100 return: -530.3500462475433\n","epoch 40/100 return: -392.3862795934136\n","epoch 50/100 return: -503.97079526506786\n","epoch 60/100 return: -290.49638006991756\n","epoch 70/100 return: -546.2175013841079\n","epoch 80/100 return: -348.1190963061105\n","epoch 90/100 return: -474.09599921120866\n","###############    Reward for test environment for run 4: -477.7614124687559.   ###############\n","\n","\n","Run 5 out of 10\n","state_dim 12\n","epoch 99000/100000, policy loss 0.30447518825531006 \tepoch 0/100 return: -90.56997806814061\n","epoch 10/100 return: -109.12739315650337\n","epoch 20/100 return: -229.8316547885025\n","epoch 30/100 return: -15.653975567908276\n","epoch 40/100 return: -184.92346741740684\n","epoch 50/100 return: -37.94560045672631\n","epoch 60/100 return: 3.9384203092252505\n","epoch 70/100 return: -47.31599794598481\n","epoch 80/100 return: -64.16765132933057\n","epoch 90/100 return: -24.10534159518052\n","###############    Reward for test environment for run 5: -136.577607268873.   ###############\n","\n","\n","Run 6 out of 10\n","state_dim 12\n","epoch 99000/100000, policy loss 0.1227131336927414 \tepoch 0/100 return: -494.7951771117047\n","epoch 10/100 return: -375.94274050000274\n","epoch 20/100 return: -716.8052618171289\n","epoch 30/100 return: -551.2429433256688\n","epoch 40/100 return: 296.1681292333223\n","epoch 50/100 return: -701.9038866186837\n","epoch 60/100 return: -534.9340836899642\n","epoch 70/100 return: 28.071995442801153\n","epoch 80/100 return: -613.0936214184117\n","epoch 90/100 return: -561.5682565081048\n","###############    Reward for test environment for run 6: -446.03085851404444.   ###############\n","\n","\n","Run 7 out of 10\n","state_dim 12\n","epoch 99000/100000, policy loss 0.3127283453941345 \tepoch 0/100 return: -1184.4492603540436\n","epoch 10/100 return: -202.43986661759018\n","epoch 20/100 return: -693.4403314593518\n","epoch 30/100 return: -518.5118973996703\n","epoch 40/100 return: -352.42610574971974\n","epoch 50/100 return: -296.37612626806407\n","epoch 60/100 return: -707.8302266935034\n","epoch 70/100 return: -438.3950713388866\n","epoch 80/100 return: -453.58414875349257\n","epoch 90/100 return: -601.5131058673425\n","###############    Reward for test environment for run 7: -559.3427444851953.   ###############\n","\n","\n","Run 8 out of 10\n","state_dim 12\n","epoch 99000/100000, policy loss 0.27709174156188965 \tepoch 0/100 return: -468.4028884739463\n","epoch 10/100 return: -577.7073812323588\n","epoch 20/100 return: -100.93584335711319\n","epoch 30/100 return: -512.2025679206414\n","epoch 40/100 return: -531.9133981953358\n","epoch 50/100 return: -481.79114396664534\n","epoch 60/100 return: -838.0199817567727\n","epoch 70/100 return: -570.3406609986973\n","epoch 80/100 return: -331.8009122271351\n","epoch 90/100 return: -529.03093585038\n","###############    Reward for test environment for run 8: -501.11621149266415.   ###############\n","\n","\n","Run 9 out of 10\n","state_dim 12\n","epoch 99000/100000, policy loss 0.26004329323768616 \tepoch 0/100 return: -44.79751960149724\n","epoch 10/100 return: -400.99331375263154\n","epoch 20/100 return: -516.805628054298\n","epoch 30/100 return: -39.697957971770975\n","epoch 40/100 return: -636.9667897903122\n","epoch 50/100 return: -254.86872006404104\n","epoch 60/100 return: -139.56702537776712\n","epoch 70/100 return: -2277.905987813119\n","epoch 80/100 return: -179.36081194879938\n","epoch 90/100 return: -122.85470707818214\n","###############    Reward for test environment for run 9: -425.57985611922743.   ###############\n","\n","\n","Run 10 out of 10\n","state_dim 12\n","epoch 99000/100000, policy loss 0.3033648133277893 \tepoch 0/100 return: -782.6562718631787\n","epoch 10/100 return: -731.3592351005435\n","epoch 20/100 return: -567.1315362835346\n","epoch 30/100 return: -162.574410276104\n","epoch 40/100 return: -130.48855766425572\n","epoch 50/100 return: -487.59528400783626\n","epoch 60/100 return: -637.064493612011\n","epoch 70/100 return: -608.6936043541805\n","epoch 80/100 return: -512.0577463277384\n","epoch 90/100 return: -692.5290500660909\n","###############    Reward for test environment for run 10: -533.7787525051367.   ###############\n","\n","\n","Average reward for 10 repetitions: -359.8638238918934\n","ALL RESULTS TRAIL: [-359.8638238918934]\n","Config: {'ENV': 'LunarLander-v2', 'ALG': 'FINAL_Apr24_CCILStudent_origindata_multiply_trajnum4', 'NUM_TRAJS_GIVEN': 4, 'NUM_TRAINING_ENVS': 2, 'NOISE_DIM': 4, 'REP_SIZE': 16, 'TRAJ_SHIFT': 4, 'SAMPLING_RATE': 5, 'NUM_STEPS_TRAIN': 100000, 'NUM_TRAJS_VALID': 100, 'NUM_REPETITIONS': 10, 'BATCH_SIZE': 64, 'MLP_WIDTHS': 64, 'ADAM_ALPHA': 0.001, 'SGLD_BUFFER_SIZE': 10000, 'SGLD_LEARN_RATE': 0.01, 'SGLD_NOISE_COEF': 0.01, 'SGLD_NUM_STEPS': 100, 'SGLD_REINIT_FREQ': 0.05, 'NUM_STEPS_TRAIN_ENERGY_MODEL': 1000, 'TRIAL': 0, 'METHOD': 'CCIL', 'MASK_PROB': 0.5, 'EXPERT_ALG': 'dqn'}\n","Trial number 0\n","config method =  CCIL\n","config env =  LunarLander-v2\n","Run 1 out of 10\n","state_dim 12\n","epoch 99000/100000, policy loss 0.191926047205925 \tepoch 0/100 return: -52.47038508333516\n","epoch 10/100 return: -159.15236555351336\n","epoch 20/100 return: -156.97751697243268\n","epoch 30/100 return: -159.4177975769692\n","epoch 40/100 return: -129.46536847774888\n","epoch 50/100 return: -100.50481529306042\n","epoch 60/100 return: -66.15753916654685\n","epoch 70/100 return: -72.80611675381938\n","epoch 80/100 return: -269.8812843430004\n","epoch 90/100 return: -166.07429740590914\n","###############    Reward for test environment for run 1: -117.52287824154828.   ###############\n","\n","\n","Run 2 out of 10\n","state_dim 12\n","epoch 99000/100000, policy loss 0.3441367745399475 \tepoch 0/100 return: -196.30052083182522\n","epoch 10/100 return: -373.1291951991439\n","epoch 20/100 return: -57.22887035877648\n","epoch 30/100 return: -42.04590167404085\n","epoch 40/100 return: -186.12732538693908\n","epoch 50/100 return: -369.3932492003846\n","epoch 60/100 return: -80.11384606130547\n","epoch 70/100 return: -310.6500606419795\n","epoch 80/100 return: -223.93491246161216\n","epoch 90/100 return: -465.5301422090705\n","###############    Reward for test environment for run 2: -156.4393913137132.   ###############\n","\n","\n","Run 3 out of 10\n","state_dim 12\n","epoch 99000/100000, policy loss 0.5700613856315613 \tepoch 0/100 return: 6.7226288200071025\n","epoch 10/100 return: -65.57259827947686\n","epoch 20/100 return: -497.10489886778873\n","epoch 30/100 return: -476.4587100923125\n","epoch 40/100 return: -142.30866742949013\n","epoch 50/100 return: -750.6255678421346\n","epoch 60/100 return: -537.7668235560319\n","epoch 70/100 return: -70.58960318943919\n","epoch 80/100 return: -470.558342313016\n","epoch 90/100 return: -473.6999543167671\n","###############    Reward for test environment for run 3: -408.9230768496444.   ###############\n","\n","\n","Run 4 out of 10\n","state_dim 12\n","epoch 99000/100000, policy loss 0.274931401014328 \tepoch 0/100 return: -1182.3447745178883\n","epoch 10/100 return: -117.28965538378158\n","epoch 20/100 return: -1153.9436188623606\n","epoch 30/100 return: -1493.4362573177705\n","epoch 40/100 return: -1341.3779241399498\n","epoch 50/100 return: -986.3965846196251\n","epoch 60/100 return: -2075.9319342952713\n","epoch 70/100 return: -1129.2506590324936\n","epoch 80/100 return: -1760.166314679135\n","epoch 90/100 return: -463.15898324018184\n","###############    Reward for test environment for run 4: -923.0968591930247.   ###############\n","\n","\n","Run 5 out of 10\n","state_dim 12\n","epoch 99000/100000, policy loss 0.29113203287124634 \tepoch 0/100 return: -878.9012987068484\n","epoch 10/100 return: -843.1929622883503\n","epoch 20/100 return: -941.5285774283186\n","epoch 30/100 return: -997.6444022399118\n","epoch 40/100 return: -1067.2795293010913\n","epoch 50/100 return: -1520.465783291111\n","epoch 60/100 return: -269.6213794733941\n","epoch 70/100 return: -1142.8958022764589\n","epoch 80/100 return: -737.1509861458468\n","epoch 90/100 return: -1060.7218711292558\n","###############    Reward for test environment for run 5: -998.9624435143966.   ###############\n","\n","\n","Run 6 out of 10\n","state_dim 12\n","epoch 99000/100000, policy loss 0.5412917733192444 \tepoch 0/100 return: -40.8735752438396\n","epoch 10/100 return: -152.75786870326817\n","epoch 20/100 return: -146.51192840806186\n","epoch 30/100 return: -107.50619646419098\n","epoch 40/100 return: -271.8393078400294\n","epoch 50/100 return: -133.70130578596866\n","epoch 60/100 return: -273.90202893791457\n","epoch 70/100 return: -132.06484387933588\n","epoch 80/100 return: -161.66678514708912\n","epoch 90/100 return: -191.0376497859655\n","###############    Reward for test environment for run 6: -161.54063753947113.   ###############\n","\n","\n","Run 7 out of 10\n","state_dim 12\n","epoch 99000/100000, policy loss 0.3087994456291199 \tepoch 0/100 return: -106.59653820638454\n","epoch 10/100 return: -129.6372919341213\n","epoch 20/100 return: -173.59203219100777\n","epoch 30/100 return: -54.63340836331062\n","epoch 40/100 return: -14.977341631436317\n","epoch 50/100 return: -91.4197816656844\n","epoch 60/100 return: -69.06875742742548\n","epoch 70/100 return: -97.78770863900935\n","epoch 80/100 return: -230.6210865689953\n","epoch 90/100 return: -121.47001649470282\n","###############    Reward for test environment for run 7: -144.17528189273946.   ###############\n","\n","\n","Run 8 out of 10\n","state_dim 12\n","epoch 99000/100000, policy loss 0.3527173101902008 \tepoch 0/100 return: 3.3533197150224083\n","epoch 10/100 return: -71.29113805473693\n","epoch 20/100 return: -139.97252771405493\n","epoch 30/100 return: -86.64424759773824\n","epoch 40/100 return: -27.18685585719068\n","epoch 50/100 return: -40.99654048051342\n","epoch 60/100 return: -21.70765789086866\n","epoch 70/100 return: -152.52559192800737\n","epoch 80/100 return: -165.7172519495797\n","epoch 90/100 return: -101.186048509642\n","###############    Reward for test environment for run 8: -95.88621645984755.   ###############\n","\n","\n","Run 9 out of 10\n","state_dim 12\n","epoch 99000/100000, policy loss 0.4287082552909851 \tepoch 0/100 return: -367.3527176550739\n","epoch 10/100 return: -393.91221948950266\n","epoch 20/100 return: -423.4544368801128\n","epoch 30/100 return: -396.7465655251637\n","epoch 40/100 return: -413.0556546928455\n","epoch 50/100 return: -361.1640203977839\n","epoch 60/100 return: -370.442529733182\n","epoch 70/100 return: -329.31628720385584\n","epoch 80/100 return: -431.07929364167967\n","epoch 90/100 return: -433.3060813018558\n","###############    Reward for test environment for run 9: -364.64318746865194.   ###############\n","\n","\n","Run 10 out of 10\n","state_dim 12\n","epoch 99000/100000, policy loss 0.4508478045463562 \tepoch 0/100 return: -159.33013997435296\n","epoch 10/100 return: -295.66439355969567\n","epoch 20/100 return: 21.522224841519744\n","epoch 30/100 return: -253.45763274481757\n","epoch 40/100 return: -194.66906280355045\n","epoch 50/100 return: -232.02090416901117\n","epoch 60/100 return: -154.0066082630998\n","epoch 70/100 return: 18.183673766417428\n","epoch 80/100 return: -99.51332269141118\n","epoch 90/100 return: -130.95304063269327\n","###############    Reward for test environment for run 10: -107.85628167054578.   ###############\n","\n","\n","Average reward for 10 repetitions: -347.9046254143583\n","ALL RESULTS TRAIL: [-347.9046254143583]\n","Config: {'ENV': 'LunarLander-v2', 'ALG': 'FINAL_Apr24_CCILStudent_origindata_multiply_trajnum8', 'NUM_TRAJS_GIVEN': 8, 'NUM_TRAINING_ENVS': 2, 'NOISE_DIM': 4, 'REP_SIZE': 16, 'TRAJ_SHIFT': 8, 'SAMPLING_RATE': 5, 'NUM_STEPS_TRAIN': 100000, 'NUM_TRAJS_VALID': 100, 'NUM_REPETITIONS': 10, 'BATCH_SIZE': 64, 'MLP_WIDTHS': 64, 'ADAM_ALPHA': 0.001, 'SGLD_BUFFER_SIZE': 10000, 'SGLD_LEARN_RATE': 0.01, 'SGLD_NOISE_COEF': 0.01, 'SGLD_NUM_STEPS': 100, 'SGLD_REINIT_FREQ': 0.05, 'NUM_STEPS_TRAIN_ENERGY_MODEL': 1000, 'TRIAL': 0, 'METHOD': 'CCIL', 'MASK_PROB': 0.5, 'EXPERT_ALG': 'dqn'}\n","Trial number 0\n","config method =  CCIL\n","config env =  LunarLander-v2\n","Run 1 out of 10\n","state_dim 12\n","epoch 99000/100000, policy loss 0.5868178009986877 \tepoch 0/100 return: -235.12238209114344\n","epoch 10/100 return: -354.67688340470374\n","epoch 20/100 return: -29.1657746749556\n","epoch 30/100 return: -65.43579501880737\n","epoch 40/100 return: -383.2077468695715\n","epoch 50/100 return: -290.16271660089086\n","epoch 60/100 return: -14.890196482224425\n","epoch 70/100 return: -212.4278846575926\n","epoch 80/100 return: -391.7132823210316\n","epoch 90/100 return: 202.92388718778716\n","###############    Reward for test environment for run 1: -251.7032149833933.   ###############\n","\n","\n","Run 2 out of 10\n","state_dim 12\n","epoch 99000/100000, policy loss 0.5304977297782898 \tepoch 0/100 return: -232.17696516454674\n","epoch 10/100 return: -299.33115599157304\n","epoch 20/100 return: -230.85239737473685\n","epoch 30/100 return: -308.4582157310664\n","epoch 40/100 return: -177.641173446154\n","epoch 50/100 return: -332.80253710864906\n","epoch 60/100 return: -375.6267640528722\n","epoch 70/100 return: -84.76656537333645\n","epoch 80/100 return: -253.52337577084228\n","epoch 90/100 return: -109.03469980332365\n","###############    Reward for test environment for run 2: -196.30028290345714.   ###############\n","\n","\n","Run 3 out of 10\n","state_dim 12\n","epoch 99000/100000, policy loss 0.4592103958129883 \tepoch 0/100 return: -534.5621353084389\n","epoch 10/100 return: -456.99210702330123\n","epoch 20/100 return: -668.7479242072245\n","epoch 30/100 return: -24.828676329902308\n","epoch 40/100 return: -339.7959449765916\n","epoch 50/100 return: -682.7220180305799\n","epoch 60/100 return: -713.918345672525\n","epoch 70/100 return: -536.321776351735\n","epoch 80/100 return: -701.9112333430918\n","epoch 90/100 return: -23.04765845543497\n","###############    Reward for test environment for run 3: -539.6376493343764.   ###############\n","\n","\n","Run 4 out of 10\n","state_dim 12\n","epoch 99000/100000, policy loss 0.44645774364471436 \tepoch 0/100 return: -133.54650676811355\n","epoch 10/100 return: -416.6755990826399\n","epoch 20/100 return: -195.50104197116457\n","epoch 30/100 return: 182.36159270214625\n","epoch 40/100 return: -23.189183402438985\n","epoch 50/100 return: -151.0990075218827\n","epoch 60/100 return: -231.3070025304995\n","epoch 70/100 return: -363.3332510638602\n","epoch 80/100 return: -359.17890429249604\n","epoch 90/100 return: -247.68635071326375\n","###############    Reward for test environment for run 4: -182.66330197027827.   ###############\n","\n","\n","Run 5 out of 10\n","state_dim 12\n","epoch 99000/100000, policy loss 0.48427194356918335 \tepoch 0/100 return: -425.3001336899969\n","epoch 10/100 return: -265.9019868486576\n","epoch 20/100 return: -166.40114625274362\n","epoch 30/100 return: -303.50752545406954\n","epoch 40/100 return: -268.20362199415626\n","epoch 50/100 return: -434.3002308390783\n","epoch 60/100 return: -392.5137228274244\n","epoch 70/100 return: -62.10295095867406\n","epoch 80/100 return: -286.70258962036394\n","epoch 90/100 return: -171.45717042828323\n","###############    Reward for test environment for run 5: -305.96682183888055.   ###############\n","\n","\n","Run 6 out of 10\n","state_dim 12\n","epoch 99000/100000, policy loss 0.4728259742259979 \tepoch 0/100 return: -655.7226167530481\n","epoch 10/100 return: -365.20179874813715\n","epoch 20/100 return: -519.8914568867347\n","epoch 30/100 return: -394.5460381535582\n","epoch 40/100 return: -618.2182200066478\n","epoch 50/100 return: -602.1046503967495\n","epoch 60/100 return: -389.1924580742447\n","epoch 70/100 return: -584.1339951022321\n","epoch 80/100 return: -603.7092033512481\n","epoch 90/100 return: -511.84847155699197\n","###############    Reward for test environment for run 6: -529.6740997462196.   ###############\n","\n","\n","Run 7 out of 10\n","state_dim 12\n","epoch 99000/100000, policy loss 0.504409670829773 \tepoch 0/100 return: -353.37528495350125\n","epoch 10/100 return: -356.6312678545823\n","epoch 20/100 return: -193.009441971334\n","epoch 30/100 return: -208.48640970207606\n","epoch 40/100 return: -78.8646552205832\n","epoch 50/100 return: -239.87714595058216\n","epoch 60/100 return: -279.153209123612\n","epoch 70/100 return: -382.2819014411252\n","epoch 80/100 return: -356.6192695506345\n","epoch 90/100 return: -86.44931923101493\n","###############    Reward for test environment for run 7: -300.4232888003816.   ###############\n","\n","\n","Run 8 out of 10\n","state_dim 12\n","epoch 99000/100000, policy loss 0.44931796193122864 \tepoch 0/100 return: -29.586547039268254\n","epoch 10/100 return: 160.53533138235383\n","epoch 20/100 return: -68.38062868100018\n","epoch 30/100 return: -179.05395971639237\n","epoch 40/100 return: -275.2293587299496\n","epoch 50/100 return: -273.10748593392225\n","epoch 60/100 return: -277.45151242916177\n","epoch 70/100 return: -246.98288487931242\n","epoch 80/100 return: -244.41062298781705\n","epoch 90/100 return: -199.0577103039084\n","###############    Reward for test environment for run 8: -151.54553568255918.   ###############\n","\n","\n","Run 9 out of 10\n","state_dim 12\n","epoch 99000/100000, policy loss 0.7071197032928467 \tepoch 0/100 return: -641.5363337509493\n","epoch 10/100 return: -437.32414069761677\n","epoch 20/100 return: -573.9923054248417\n","epoch 30/100 return: -593.1704387672341\n","epoch 40/100 return: -530.1601009834638\n","epoch 50/100 return: -542.4499313677911\n","epoch 60/100 return: -538.3289268429959\n","epoch 70/100 return: -700.87791613371\n","epoch 80/100 return: -647.5404569256314\n","epoch 90/100 return: -480.3540797833941\n","###############    Reward for test environment for run 9: -617.2768584001979.   ###############\n","\n","\n","Run 10 out of 10\n","state_dim 12\n","epoch 99000/100000, policy loss 0.427444726228714 \tepoch 0/100 return: -134.57512476804447\n","epoch 10/100 return: -25.693493323632197\n","epoch 20/100 return: -628.0765329832094\n","epoch 30/100 return: -118.68355946053717\n","epoch 40/100 return: -105.7526449879651\n","epoch 50/100 return: -454.72025533666806\n","epoch 60/100 return: -96.68262948602747\n","epoch 70/100 return: -336.66660084554206\n","epoch 80/100 return: -142.62802586000356\n","epoch 90/100 return: -599.7481138279286\n","###############    Reward for test environment for run 10: -352.3035010205688.   ###############\n","\n","\n","Average reward for 10 repetitions: -342.7494554680313\n","ALL RESULTS TRAIL: [-342.7494554680313]\n","Config: {'ENV': 'LunarLander-v2', 'ALG': 'FINAL_Apr24_CCILStudent_origindata_multiply_trajnum16', 'NUM_TRAJS_GIVEN': 16, 'NUM_TRAINING_ENVS': 2, 'NOISE_DIM': 4, 'REP_SIZE': 16, 'TRAJ_SHIFT': 16, 'SAMPLING_RATE': 5, 'NUM_STEPS_TRAIN': 100000, 'NUM_TRAJS_VALID': 100, 'NUM_REPETITIONS': 10, 'BATCH_SIZE': 64, 'MLP_WIDTHS': 64, 'ADAM_ALPHA': 0.001, 'SGLD_BUFFER_SIZE': 10000, 'SGLD_LEARN_RATE': 0.01, 'SGLD_NOISE_COEF': 0.01, 'SGLD_NUM_STEPS': 100, 'SGLD_REINIT_FREQ': 0.05, 'NUM_STEPS_TRAIN_ENERGY_MODEL': 1000, 'TRIAL': 0, 'METHOD': 'CCIL', 'MASK_PROB': 0.5, 'EXPERT_ALG': 'dqn'}\n","Trial number 0\n","config method =  CCIL\n","config env =  LunarLander-v2\n","Run 1 out of 10\n","state_dim 12\n","epoch 99000/100000, policy loss 0.5082786679267883 \tepoch 0/100 return: -752.0525256947656\n","epoch 10/100 return: -850.0008910149398\n","epoch 20/100 return: -835.8377267544661\n","epoch 30/100 return: -996.0603511741477\n","epoch 40/100 return: -932.9915703307396\n","epoch 50/100 return: -99.50924300334135\n","epoch 60/100 return: -604.7091611205326\n","epoch 70/100 return: -437.05576690906105\n","epoch 80/100 return: -461.0275445483224\n","epoch 90/100 return: -483.6660474321707\n","###############    Reward for test environment for run 1: -516.0947277604977.   ###############\n","\n","\n","Run 2 out of 10\n","state_dim 12\n","epoch 99000/100000, policy loss 0.6401078104972839 \tepoch 0/100 return: -256.42057869911093\n","epoch 10/100 return: -59.02898365728726\n","epoch 20/100 return: -87.94501725283743\n","epoch 30/100 return: -305.51077390970585\n","epoch 40/100 return: 36.79954935211049\n","epoch 50/100 return: -276.343508879057\n","epoch 60/100 return: -739.0913764998253\n","epoch 70/100 return: -157.19875872544714\n","epoch 80/100 return: -193.3716215759665\n","epoch 90/100 return: 30.966026535214013\n","###############    Reward for test environment for run 2: -211.52969849330492.   ###############\n","\n","\n","Run 3 out of 10\n","state_dim 12\n","epoch 99000/100000, policy loss 0.6486773490905762 \tepoch 0/100 return: -296.990064363075\n","epoch 10/100 return: -386.7489981627181\n","epoch 20/100 return: -308.39504955996614\n","epoch 30/100 return: -380.5887843454598\n","epoch 40/100 return: -320.2971843960038\n","epoch 50/100 return: -345.48501169239387\n","epoch 60/100 return: -289.58841593820824\n","epoch 70/100 return: -254.79792011292102\n","epoch 80/100 return: -129.6648346930168\n","epoch 90/100 return: -435.6696069215739\n","###############    Reward for test environment for run 3: -295.3771420104315.   ###############\n","\n","\n","Run 4 out of 10\n","state_dim 12\n","epoch 99000/100000, policy loss 0.6867393255233765 \tepoch 0/100 return: -328.2923836804784\n","epoch 10/100 return: -242.0458717966822\n","epoch 20/100 return: -260.61801701609863\n","epoch 30/100 return: -391.70681717474224\n","epoch 40/100 return: -427.441510338184\n","epoch 50/100 return: -377.0519520995342\n","epoch 60/100 return: -219.13393362290418\n","epoch 70/100 return: -73.92770914479448\n","epoch 80/100 return: -149.00904393960838\n","epoch 90/100 return: -386.1461677050362\n","###############    Reward for test environment for run 4: -269.816368810419.   ###############\n","\n","\n","Run 5 out of 10\n","state_dim 12\n","epoch 99000/100000, policy loss 0.8779276609420776 \tepoch 0/100 return: -1359.5020403916171\n","epoch 10/100 return: -649.3239420714392\n","epoch 20/100 return: -1189.1133904205249\n","epoch 30/100 return: -657.7831328081535\n","epoch 40/100 return: -380.8442130543697\n","epoch 50/100 return: -558.0861417168198\n","epoch 60/100 return: -969.5037969010839\n","epoch 70/100 return: -700.5440783547805\n","epoch 80/100 return: -444.9583237488083\n","epoch 90/100 return: -451.45397022119397\n","###############    Reward for test environment for run 5: -507.2692469970401.   ###############\n","\n","\n","Run 6 out of 10\n","state_dim 12\n","epoch 99000/100000, policy loss 0.5188911557197571 \tepoch 0/100 return: -273.5580871691926\n","epoch 10/100 return: -139.58251784741094\n","epoch 20/100 return: -434.1164454684375\n","epoch 30/100 return: -670.7815693056519\n","epoch 40/100 return: -422.1153334893324\n","epoch 50/100 return: -70.84447800059357\n","epoch 60/100 return: -223.5541140682961\n","epoch 70/100 return: -239.06289810747086\n","epoch 80/100 return: -200.0410156322546\n","epoch 90/100 return: -263.6187856976121\n","###############    Reward for test environment for run 6: -392.096117319228.   ###############\n","\n","\n","Run 7 out of 10\n","state_dim 12\n","epoch 99000/100000, policy loss 0.8043710589408875 \tepoch 0/100 return: 38.23108333663939\n","epoch 10/100 return: -1122.7009397099773\n","epoch 20/100 return: 14.057582781883326\n","epoch 30/100 return: -776.7585002632308\n","epoch 40/100 return: -1001.1967944885567\n","epoch 50/100 return: -589.3202139855989\n","epoch 60/100 return: -661.1940841083248\n","epoch 70/100 return: -229.7991446021456\n","epoch 80/100 return: -42.07983590593395\n","epoch 90/100 return: -624.2044572377669\n","###############    Reward for test environment for run 7: -555.2745190330568.   ###############\n","\n","\n","Run 8 out of 10\n","state_dim 12\n","epoch 99000/100000, policy loss 0.6318116188049316 \tepoch 0/100 return: -354.4058463070047\n","epoch 10/100 return: -627.7459397730013\n","epoch 20/100 return: -563.1970460333403\n","epoch 30/100 return: -586.2342977356477\n","epoch 40/100 return: -574.0022433153804\n","epoch 50/100 return: -615.8654308194631\n","epoch 60/100 return: -524.1835744101974\n","epoch 70/100 return: -450.92386468986587\n","epoch 80/100 return: -555.5110490512868\n","epoch 90/100 return: -611.1442593715957\n","###############    Reward for test environment for run 8: -532.1567047718306.   ###############\n","\n","\n","Run 9 out of 10\n","state_dim 12\n","epoch 99000/100000, policy loss 0.931267261505127 \tepoch 0/100 return: -246.8424109055751\n","epoch 10/100 return: -809.3785926242534\n","epoch 20/100 return: -470.4908762641342\n","epoch 30/100 return: 11.13976544693719\n","epoch 40/100 return: -275.96940455219345\n","epoch 50/100 return: -114.37573318552833\n","epoch 60/100 return: 0.8587497516538463\n","epoch 70/100 return: -273.17457106775714\n","epoch 80/100 return: -320.7980020571024\n","epoch 90/100 return: -265.61208703957453\n","###############    Reward for test environment for run 9: -315.4626570765457.   ###############\n","\n","\n","Run 10 out of 10\n","state_dim 12\n","epoch 99000/100000, policy loss 0.6914654970169067 \tepoch 0/100 return: -263.3573724662506\n","epoch 10/100 return: -281.7184978793158\n","epoch 20/100 return: -222.55173353987158\n","epoch 30/100 return: -298.0134310121432\n","epoch 40/100 return: -425.1772394899835\n","epoch 50/100 return: -414.2411032913495\n","epoch 60/100 return: -227.59956490923076\n","epoch 70/100 return: -322.6762855354873\n","epoch 80/100 return: -614.3706354572706\n","epoch 90/100 return: -519.0843062523215\n","###############    Reward for test environment for run 10: -336.47317547619804.   ###############\n","\n","\n","Average reward for 10 repetitions: -393.15503577485526\n","ALL RESULTS TRAIL: [-393.15503577485526]\n","Config: {'ENV': 'LunarLander-v2', 'ALG': 'FINAL_Apr24_CCILStudent_origindata_multiply_trajnum32', 'NUM_TRAJS_GIVEN': 32, 'NUM_TRAINING_ENVS': 2, 'NOISE_DIM': 4, 'REP_SIZE': 16, 'TRAJ_SHIFT': 32, 'SAMPLING_RATE': 5, 'NUM_STEPS_TRAIN': 100000, 'NUM_TRAJS_VALID': 100, 'NUM_REPETITIONS': 10, 'BATCH_SIZE': 64, 'MLP_WIDTHS': 64, 'ADAM_ALPHA': 0.001, 'SGLD_BUFFER_SIZE': 10000, 'SGLD_LEARN_RATE': 0.01, 'SGLD_NOISE_COEF': 0.01, 'SGLD_NUM_STEPS': 100, 'SGLD_REINIT_FREQ': 0.05, 'NUM_STEPS_TRAIN_ENERGY_MODEL': 1000, 'TRIAL': 0, 'METHOD': 'CCIL', 'MASK_PROB': 0.5, 'EXPERT_ALG': 'dqn'}\n","Trial number 0\n","config method =  CCIL\n","config env =  LunarLander-v2\n","Run 1 out of 10\n","state_dim 12\n","epoch 99000/100000, policy loss 0.6004020571708679 \tepoch 0/100 return: -452.296647816298\n","epoch 10/100 return: -399.63086405425287\n","epoch 20/100 return: -784.9337854674073\n","epoch 30/100 return: -880.7384815757808\n","epoch 40/100 return: -693.813045977612\n","epoch 50/100 return: 226.87787695712794\n","epoch 60/100 return: -774.0328593212517\n","epoch 70/100 return: -372.2043743313919\n","epoch 80/100 return: -808.6871873559163\n","epoch 90/100 return: -341.7138048102381\n","###############    Reward for test environment for run 1: -466.00796761804736.   ###############\n","\n","\n","Run 2 out of 10\n","state_dim 12\n","epoch 99000/100000, policy loss 0.7627725601196289 \tepoch 0/100 return: -878.801377116978\n","epoch 10/100 return: -239.8067413929171\n","epoch 20/100 return: -279.713937318932\n","epoch 30/100 return: -324.4190693541383\n","epoch 40/100 return: -223.00083541964568\n","epoch 50/100 return: -994.9621575971034\n","epoch 60/100 return: -194.3980576622717\n","epoch 70/100 return: -468.4077526775137\n","epoch 80/100 return: -342.4972677170366\n","epoch 90/100 return: -434.77333021498555\n","###############    Reward for test environment for run 2: -444.65029932530064.   ###############\n","\n","\n","Run 3 out of 10\n","state_dim 12\n","epoch 99000/100000, policy loss 0.7084943056106567 \tepoch 0/100 return: -632.3211685540712\n","epoch 10/100 return: -3656.5081583757637\n","epoch 20/100 return: -408.1976120999368\n","epoch 30/100 return: -361.47591803118905\n","epoch 40/100 return: -620.1367373272354\n","epoch 50/100 return: -883.0371161287768\n","epoch 60/100 return: -442.35608062999273\n","epoch 70/100 return: -604.0367026623367\n","epoch 80/100 return: -620.7094111618574\n","epoch 90/100 return: -435.16938553678074\n","###############    Reward for test environment for run 3: -774.8617330662262.   ###############\n","\n","\n","Run 4 out of 10\n","state_dim 12\n","epoch 99000/100000, policy loss 0.7019585371017456 \tepoch 0/100 return: -412.73967242166896\n","epoch 10/100 return: -420.6016656955428\n","epoch 20/100 return: -209.4521219635746\n","epoch 30/100 return: -636.3833220869047\n","epoch 40/100 return: -48.32341633051835\n","epoch 50/100 return: -60.74533420086851\n","epoch 60/100 return: -257.73968767568437\n","epoch 70/100 return: -435.6381875386235\n","epoch 80/100 return: -322.5505223170086\n","epoch 90/100 return: -241.24961403761023\n","###############    Reward for test environment for run 4: -343.72458232330393.   ###############\n","\n","\n","Run 5 out of 10\n","state_dim 12\n","epoch 99000/100000, policy loss 0.7506342530250549 \tepoch 0/100 return: -354.83115586272186\n","epoch 10/100 return: -27.44281742344549\n","epoch 20/100 return: -78.73335499196135\n","epoch 30/100 return: -50.50535444641321\n","epoch 40/100 return: -31.556197139937353\n","epoch 50/100 return: -597.1061372691859\n","epoch 60/100 return: -824.976345222496\n","epoch 70/100 return: -1444.057319903466\n","epoch 80/100 return: -180.83687371864966\n","epoch 90/100 return: -564.8370671475038\n","###############    Reward for test environment for run 5: -662.7613155394596.   ###############\n","\n","\n","Run 6 out of 10\n","state_dim 12\n","epoch 99000/100000, policy loss 0.7020269632339478 \tepoch 0/100 return: -218.46326204986258\n","epoch 10/100 return: -479.4344922109985\n","epoch 20/100 return: -357.99442759773666\n","epoch 30/100 return: -371.75354757355103\n","epoch 40/100 return: -411.22444543912525\n","epoch 50/100 return: 252.01248127174927\n","epoch 60/100 return: -14.64785789398772\n","epoch 70/100 return: -307.6652607967179\n","epoch 80/100 return: -203.35652883016962\n","epoch 90/100 return: -57.41904427579847\n","###############    Reward for test environment for run 6: -201.27729910103696.   ###############\n","\n","\n","Run 7 out of 10\n","state_dim 12\n","epoch 99000/100000, policy loss 0.755971372127533 \tepoch 0/100 return: -31.180764301333895\n","epoch 10/100 return: -136.44166923168746\n","epoch 20/100 return: -28.769967548443475\n","epoch 30/100 return: -318.02634173897343\n","epoch 40/100 return: -204.88383307359192\n","epoch 50/100 return: -23.894210526318957\n","epoch 60/100 return: -100.7486881616819\n","epoch 70/100 return: -139.09710758960986\n","epoch 80/100 return: -131.1320373883938\n","epoch 90/100 return: -90.42981114841398\n","###############    Reward for test environment for run 7: -191.55449605722498.   ###############\n","\n","\n","Run 8 out of 10\n","state_dim 12\n","epoch 99000/100000, policy loss 0.8856956958770752 \tepoch 0/100 return: -776.7302229945192\n","epoch 10/100 return: -347.68407018459425\n","epoch 20/100 return: -183.0398790272913\n","epoch 30/100 return: -167.71867268359\n","epoch 40/100 return: -341.1731345733837\n","epoch 50/100 return: -1098.9868157726398\n","epoch 60/100 return: -519.0094657136659\n","epoch 70/100 return: -366.39455434116087\n","epoch 80/100 return: -234.05519471565643\n","epoch 90/100 return: -649.235174347016\n","###############    Reward for test environment for run 8: -367.1240114295521.   ###############\n","\n","\n","Run 9 out of 10\n","state_dim 12\n","epoch 99000/100000, policy loss 0.6334508657455444 \tepoch 0/100 return: 105.95524171081532\n","epoch 10/100 return: -360.40994484492074\n","epoch 20/100 return: -200.95125204898716\n","epoch 30/100 return: -262.1292657079117\n","epoch 40/100 return: -57.942869411868166\n","epoch 50/100 return: -363.3868264767284\n","epoch 60/100 return: -317.64265705227604\n","epoch 70/100 return: -245.8134959786859\n","epoch 80/100 return: -268.7228192764866\n","epoch 90/100 return: -48.54197277546234\n","###############    Reward for test environment for run 9: -245.80050423290152.   ###############\n","\n","\n","Run 10 out of 10\n","state_dim 12\n","epoch 99000/100000, policy loss 0.5886372923851013 \tepoch 0/100 return: -490.92969061247686\n","epoch 10/100 return: -261.3007464556389\n","epoch 20/100 return: -241.0597185810437\n","epoch 30/100 return: -280.36750911229905\n","epoch 40/100 return: -407.14830722878344\n","epoch 50/100 return: -188.40559411413335\n","epoch 60/100 return: -223.0415386370677\n","epoch 70/100 return: -711.895378148588\n","epoch 80/100 return: -413.5732272106228\n","epoch 90/100 return: -456.86604610879107\n","###############    Reward for test environment for run 10: -303.1307983046271.   ###############\n","\n","\n","Average reward for 10 repetitions: -400.08930069976805\n","ALL RESULTS TRAIL: [-400.08930069976805]\n","Config: {'ENV': 'LunarLander-v2', 'ALG': 'FINAL_Apr24_CCILStudent_origindata_multiply_trajnum64', 'NUM_TRAJS_GIVEN': 64, 'NUM_TRAINING_ENVS': 2, 'NOISE_DIM': 4, 'REP_SIZE': 16, 'TRAJ_SHIFT': 64, 'SAMPLING_RATE': 5, 'NUM_STEPS_TRAIN': 100000, 'NUM_TRAJS_VALID': 100, 'NUM_REPETITIONS': 10, 'BATCH_SIZE': 64, 'MLP_WIDTHS': 64, 'ADAM_ALPHA': 0.001, 'SGLD_BUFFER_SIZE': 10000, 'SGLD_LEARN_RATE': 0.01, 'SGLD_NOISE_COEF': 0.01, 'SGLD_NUM_STEPS': 100, 'SGLD_REINIT_FREQ': 0.05, 'NUM_STEPS_TRAIN_ENERGY_MODEL': 1000, 'TRIAL': 0, 'METHOD': 'CCIL', 'MASK_PROB': 0.5, 'EXPERT_ALG': 'dqn'}\n","Trial number 0\n","config method =  CCIL\n","config env =  LunarLander-v2\n","Run 1 out of 10\n","state_dim 12\n","epoch 99000/100000, policy loss 0.6562062501907349 \tepoch 0/100 return: -43.29596305393683\n","epoch 10/100 return: -1508.521669589275\n","epoch 20/100 return: -527.5482224851788\n","epoch 30/100 return: -648.9392759421587\n","epoch 40/100 return: -413.23882901283366\n","epoch 50/100 return: -805.565380333014\n","epoch 60/100 return: -589.1355993884918\n","epoch 70/100 return: -342.1023848510283\n","epoch 80/100 return: -371.0536025213421\n","epoch 90/100 return: -978.9319550011103\n","###############    Reward for test environment for run 1: -642.988819677483.   ###############\n","\n","\n","Run 2 out of 10\n","state_dim 12\n","epoch 99000/100000, policy loss 0.7830504179000854 \tepoch 0/100 return: -402.87280452619893\n","epoch 10/100 return: -494.35825514564414\n","epoch 20/100 return: -374.2800747603712\n","epoch 30/100 return: -114.78137621555214\n","epoch 40/100 return: -547.1053653023557\n","epoch 50/100 return: -479.8150332291337\n","epoch 60/100 return: -92.38455198026072\n","epoch 70/100 return: -122.62305761438357\n","epoch 80/100 return: -508.7856730936618\n","epoch 90/100 return: -405.3571801940051\n","###############    Reward for test environment for run 2: -454.53475483246353.   ###############\n","\n","\n","Run 3 out of 10\n","state_dim 12\n","epoch 99000/100000, policy loss 0.5975179076194763 \tepoch 0/100 return: -184.78931846811417\n","epoch 10/100 return: -454.5237364247071\n","epoch 20/100 return: -244.784029200261\n","epoch 30/100 return: -462.6520549262178\n","epoch 40/100 return: -52.224863137951644\n","epoch 50/100 return: 117.8044381308265\n","epoch 60/100 return: -124.20101900650076\n","epoch 70/100 return: -384.80783757063244\n","epoch 80/100 return: -24.598657766650376\n","epoch 90/100 return: -154.4008408325527\n","###############    Reward for test environment for run 3: -223.07655230028737.   ###############\n","\n","\n","Run 4 out of 10\n","state_dim 12\n","epoch 99000/100000, policy loss 0.8915414214134216 \tepoch 0/100 return: -542.7894899407067\n","epoch 10/100 return: -133.83118527848785\n","epoch 20/100 return: -361.4466208839122\n","epoch 30/100 return: -82.07743661318155\n","epoch 40/100 return: -326.58576765554943\n","epoch 50/100 return: 40.613214149032785\n","epoch 60/100 return: -333.89369521952347\n","epoch 70/100 return: -220.10531092954633\n","epoch 80/100 return: 196.25903660539308\n","epoch 90/100 return: -136.95857098555146\n","###############    Reward for test environment for run 4: -274.29215329840974.   ###############\n","\n","\n","Run 5 out of 10\n","state_dim 12\n","epoch 99000/100000, policy loss 0.7868711948394775 \tepoch 0/100 return: -341.2497601851479\n","epoch 10/100 return: -248.20664964800733\n","epoch 20/100 return: -590.8090202957121\n","epoch 30/100 return: -114.49485781679795\n","epoch 40/100 return: -537.9914979689073\n","epoch 50/100 return: -472.5067486865545\n","epoch 60/100 return: -319.3697993685313\n","epoch 70/100 return: -545.3330151651417\n","epoch 80/100 return: 15.887217063296944\n","epoch 90/100 return: -426.5460353072991\n","###############    Reward for test environment for run 5: -260.2039511395367.   ###############\n","\n","\n","Run 6 out of 10\n","state_dim 12\n","epoch 99000/100000, policy loss 0.5572454333305359 \tepoch 0/100 return: -209.87856787801448\n","epoch 10/100 return: -391.8662368130792\n","epoch 20/100 return: -199.24704868304406\n","epoch 30/100 return: -10.675084036044154\n","epoch 40/100 return: -261.0293572276965\n","epoch 50/100 return: -166.69059942914896\n","epoch 60/100 return: -223.99345268796856\n","epoch 70/100 return: -178.46967718703456\n","epoch 80/100 return: -359.19073093442404\n","epoch 90/100 return: -193.2310979798075\n","###############    Reward for test environment for run 6: -146.97887205435703.   ###############\n","\n","\n","Run 7 out of 10\n","state_dim 12\n","epoch 99000/100000, policy loss 0.8081974387168884 \tepoch 0/100 return: -462.1956505703952\n","epoch 10/100 return: -367.5213259862165\n","epoch 20/100 return: -202.26307948703584\n","epoch 30/100 return: -484.88014594603436\n","epoch 40/100 return: -231.1784968091018\n","epoch 50/100 return: -517.2490617246096\n","epoch 60/100 return: -391.5870569720515\n","epoch 70/100 return: -62.700209506259384\n","epoch 80/100 return: -165.39773394048683\n","epoch 90/100 return: -346.55526835470437\n","###############    Reward for test environment for run 7: -328.85098900743907.   ###############\n","\n","\n","Run 8 out of 10\n","state_dim 12\n","epoch 99000/100000, policy loss 0.6537364721298218 \tepoch 0/100 return: -554.1300561334106\n","epoch 10/100 return: -683.1982810895432\n","epoch 20/100 return: -737.5317689621212\n","epoch 30/100 return: -780.565605888875\n","epoch 40/100 return: -634.6557010788621\n","epoch 50/100 return: -643.9237350419403\n","epoch 60/100 return: -563.4675208422254\n","epoch 70/100 return: -730.3285933584707\n","epoch 80/100 return: -645.7216306378152\n","epoch 90/100 return: -769.7318440242983\n","###############    Reward for test environment for run 8: -577.7260609461002.   ###############\n","\n","\n","Run 9 out of 10\n","state_dim 12\n","epoch 99000/100000, policy loss 0.9522088766098022 \tepoch 0/100 return: -849.4170837241846\n","epoch 10/100 return: -921.0212316862986\n","epoch 20/100 return: 198.45399579686085\n","epoch 30/100 return: 174.92531993198418\n","epoch 40/100 return: -104.86721107989085\n","epoch 50/100 return: -542.6449502192205\n","epoch 60/100 return: -210.3914762332377\n","epoch 70/100 return: -237.10466014513057\n","epoch 80/100 return: -277.95816333346534\n","epoch 90/100 return: -90.21527432112848\n","###############    Reward for test environment for run 9: -284.32616732636524.   ###############\n","\n","\n","Run 10 out of 10\n","state_dim 12\n","epoch 99000/100000, policy loss 0.7527686357498169 \tepoch 0/100 return: -325.751760025614\n","epoch 10/100 return: -210.21002818671377\n","epoch 20/100 return: -308.99320295957534\n","epoch 30/100 return: -349.6220456205331\n","epoch 40/100 return: -299.5336972005248\n","epoch 50/100 return: -340.87711723962366\n","epoch 60/100 return: -340.6250810687099\n","epoch 70/100 return: -340.4965695700794\n","epoch 80/100 return: -60.5376836368663\n","epoch 90/100 return: -322.4347534489604\n","###############    Reward for test environment for run 10: -316.46449188378426.   ###############\n","\n","\n","Average reward for 10 repetitions: -350.9442812466226\n","ALL RESULTS TRAIL: [-350.9442812466226]\n","Config: {'ENV': 'LunarLander-v2', 'ALG': 'FINAL_Apr24_CCILStudent_origindata_multiply_trajnum128', 'NUM_TRAJS_GIVEN': 128, 'NUM_TRAINING_ENVS': 2, 'NOISE_DIM': 4, 'REP_SIZE': 16, 'TRAJ_SHIFT': 128, 'SAMPLING_RATE': 5, 'NUM_STEPS_TRAIN': 100000, 'NUM_TRAJS_VALID': 100, 'NUM_REPETITIONS': 10, 'BATCH_SIZE': 64, 'MLP_WIDTHS': 64, 'ADAM_ALPHA': 0.001, 'SGLD_BUFFER_SIZE': 10000, 'SGLD_LEARN_RATE': 0.01, 'SGLD_NOISE_COEF': 0.01, 'SGLD_NUM_STEPS': 100, 'SGLD_REINIT_FREQ': 0.05, 'NUM_STEPS_TRAIN_ENERGY_MODEL': 1000, 'TRIAL': 0, 'METHOD': 'CCIL', 'MASK_PROB': 0.5, 'EXPERT_ALG': 'dqn'}\n","Trial number 0\n","config method =  CCIL\n","config env =  LunarLander-v2\n","Run 1 out of 10\n","state_dim 12\n","epoch 99000/100000, policy loss 0.9886515736579895 \tepoch 0/100 return: -342.5336683810321\n","epoch 10/100 return: -78.40414589509606\n","epoch 20/100 return: -139.54309379894923\n","epoch 30/100 return: -151.45806778443472\n","epoch 40/100 return: -436.1972642646887\n","epoch 50/100 return: -426.75010284349975\n","epoch 60/100 return: -90.62515586127658\n","epoch 70/100 return: -84.51560794678139\n","epoch 80/100 return: -302.7052408536634\n","epoch 90/100 return: -161.81036864810875\n","###############    Reward for test environment for run 1: -317.351723037311.   ###############\n","\n","\n","Run 2 out of 10\n","state_dim 12\n","epoch 99000/100000, policy loss 0.9828916192054749 \tepoch 0/100 return: -635.1465658784571\n","epoch 10/100 return: -477.43886645922316\n","epoch 20/100 return: -319.25468551859916\n","epoch 30/100 return: -211.94334561150177\n","epoch 40/100 return: -550.4156738057626\n","epoch 50/100 return: -372.457486378488\n","epoch 60/100 return: -343.78464235960564\n","epoch 70/100 return: -509.2017723611689\n","epoch 80/100 return: -755.4791639243401\n","epoch 90/100 return: -291.42208693075617\n","###############    Reward for test environment for run 2: -420.5823485237889.   ###############\n","\n","\n","Run 3 out of 10\n","state_dim 12\n","epoch 99000/100000, policy loss 0.804520845413208 \tepoch 0/100 return: -368.94566946741367\n","epoch 10/100 return: -242.839002221474\n","epoch 20/100 return: -299.90668291551475\n","epoch 30/100 return: -279.017779816871\n","epoch 40/100 return: -381.3208431800072\n","epoch 50/100 return: -256.812884457652\n","epoch 60/100 return: -340.83055703441426\n","epoch 70/100 return: -429.87916349622844\n","epoch 80/100 return: -267.5315948669353\n","epoch 90/100 return: 14.793577994781018\n","###############    Reward for test environment for run 3: -318.5487441308815.   ###############\n","\n","\n","Run 4 out of 10\n","state_dim 12\n","epoch 99000/100000, policy loss 0.813200831413269 \tepoch 0/100 return: -82.76565376900261\n","epoch 10/100 return: -143.3343222972664\n","epoch 20/100 return: -76.50531234023398\n","epoch 30/100 return: -318.61926793599434\n","epoch 40/100 return: -300.8922718262288\n","epoch 50/100 return: -163.6334782059005\n","epoch 60/100 return: -273.9810442414873\n","epoch 70/100 return: -89.9265963469497\n","epoch 80/100 return: -95.39861245487053\n","epoch 90/100 return: -129.45500110194956\n","###############    Reward for test environment for run 4: -244.91082690177254.   ###############\n","\n","\n","Run 5 out of 10\n","state_dim 12\n","epoch 99000/100000, policy loss 0.6805465221405029 \tepoch 0/100 return: -339.29344776819335\n","epoch 10/100 return: -599.2760052680173\n","epoch 20/100 return: -374.48745441656683\n","epoch 30/100 return: -277.2054029837564\n","epoch 40/100 return: -729.8934780945125\n","epoch 50/100 return: -297.2264494063704\n","epoch 60/100 return: -306.0236467104993\n","epoch 70/100 return: -372.57630064459266\n","epoch 80/100 return: -360.5755227450636\n","epoch 90/100 return: -243.3048086143126\n","###############    Reward for test environment for run 5: -313.8124753449077.   ###############\n","\n","\n","Run 6 out of 10\n","state_dim 12\n","epoch 99000/100000, policy loss 0.7611484527587891 \tepoch 0/100 return: -225.80736943674594\n","epoch 10/100 return: -172.59319506572407\n","epoch 20/100 return: -36.22547516310111\n","epoch 30/100 return: -401.7588261375396\n","epoch 40/100 return: -118.91413183665634\n","epoch 50/100 return: -121.42817641172363\n","epoch 60/100 return: -104.56863988153417\n","epoch 70/100 return: -28.3551370119593\n","epoch 80/100 return: -223.82056159878363\n","epoch 90/100 return: -166.20720692722523\n","###############    Reward for test environment for run 6: -189.42349515071547.   ###############\n","\n","\n","Run 7 out of 10\n","state_dim 12\n","epoch 99000/100000, policy loss 0.8515250086784363 \tepoch 0/100 return: -287.9952049091323\n","epoch 10/100 return: -409.0638432057503\n","epoch 20/100 return: -73.60416061309188\n","epoch 30/100 return: -522.4830021777956\n","epoch 40/100 return: -466.66660107113535\n","epoch 50/100 return: -427.6028039950059\n","epoch 60/100 return: -150.76494212586618\n","epoch 70/100 return: -288.30502670886705\n","epoch 80/100 return: -329.59494885229856\n","epoch 90/100 return: -207.49382767462836\n","###############    Reward for test environment for run 7: -352.40930647832903.   ###############\n","\n","\n","Run 8 out of 10\n","state_dim 12\n","epoch 99000/100000, policy loss 0.7001857757568359 \tepoch 0/100 return: -556.7357279940535\n","epoch 10/100 return: -411.07863899336445\n","epoch 20/100 return: -264.9385481490883\n","epoch 30/100 return: 26.56928883017966\n","epoch 40/100 return: -252.25288291034795\n","epoch 50/100 return: -925.5738430633678\n","epoch 60/100 return: 28.38534442115207\n","epoch 70/100 return: 247.59984371921652\n","epoch 80/100 return: -52.44774399098509\n","epoch 90/100 return: -387.20122540176163\n","###############    Reward for test environment for run 8: -407.00465486581083.   ###############\n","\n","\n","Run 9 out of 10\n","state_dim 12\n","epoch 99000/100000, policy loss 0.7761369347572327 \tepoch 0/100 return: -322.79654731870824\n","epoch 10/100 return: -191.3838491228455\n","epoch 20/100 return: -346.118901786234\n","epoch 30/100 return: -245.38318752295197\n","epoch 40/100 return: -373.41569064694227\n","epoch 50/100 return: -242.56314945140727\n","epoch 60/100 return: -261.0063284244339\n","epoch 70/100 return: -151.20014576453306\n","epoch 80/100 return: -270.6775427351786\n","epoch 90/100 return: -365.31653682494766\n","###############    Reward for test environment for run 9: -286.71305152631146.   ###############\n","\n","\n","Run 10 out of 10\n","state_dim 12\n","epoch 99000/100000, policy loss 0.9692462086677551 \tepoch 0/100 return: -938.5830268432617\n","epoch 10/100 return: -292.97804337613627\n","epoch 20/100 return: -168.308887271815\n","epoch 30/100 return: -655.4647217433254\n","epoch 40/100 return: -790.0567083764748\n","epoch 50/100 return: -35.23591753228263\n","epoch 60/100 return: -228.0475696594912\n","epoch 70/100 return: -288.6228949516418\n","epoch 80/100 return: -845.1618408909677\n","epoch 90/100 return: -87.0068140147871\n","###############    Reward for test environment for run 10: -412.2041111531833.   ###############\n","\n","\n","Average reward for 10 repetitions: -326.29607371130123\n","ALL RESULTS TRAIL: [-326.29607371130123]\n"]}]},{"cell_type":"markdown","source":["#trash"],"metadata":{"id":"-gldLOvJhYXC"}},{"cell_type":"code","source":["\n","def _compute_loss(samples):\n","    state = torch.FloatTensor(samples[\"state\"]).to(self.device)\n","    action = torch.LongTensor(samples[\"action\"]).to(self.device)\n","    #next_state = torch.FloatTensor(samples[\"next_state\"]).to(self.device)\n","    #env_ids = torch.LongTensor(samples[\"env\"]).to(self.device)\n","    \n","            \n","    prob = torch.ones(state.size()) * (1 - self.mask_prob)\n","    \n","    mask = torch.bernoulli(prob).to(self.device)\n","    state_concat = torch.cat([state * mask, mask], dim=1)\n","\n","    causal_rep = self.causal_features_encoder(state_concat)  # need this encoder: S -> rep\n","\n","    # 1. Policy loss\n","    qvalues = self.policy_network(causal_rep) # need this encoder:  rep -> A\n","    ce_loss = nn.CrossEntropyLoss()(qvalues, action)\n","\n","    return ce_loss, mask\n"],"metadata":{"id":"LeQCHJRPL9Ue"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["import torch.nn as nn\n","\n","num_masks = 100000\n","best_mask = None\n","best_loss = np.inf\n","\n","self = student \n","\n","\n","for i in range(num_masks): \n","    sample = self.buffer.sample()\n","    ce_loss, curr_mask = _compute_loss(sample)\n","    if ce_loss < best_loss:\n","        best_mask = curr_mask\n","        print(ce_loss)\n","        best_loss = ce_loss\n","\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"NWN64U1xLhsa","executionInfo":{"status":"ok","timestamp":1650780879590,"user_tz":240,"elapsed":131399,"user":{"displayName":"2 Lin","userId":"15119774230930098070"}},"outputId":"b0102539-a384-4ee6-96b3-a09194a73136"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["tensor(0.4874, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.4773, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.4761, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.4751, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.4727, device='cuda:0', grad_fn=<NllLossBackward0>)\n"]}]},{"cell_type":"code","source":["state = sample['state'] \n","prob = torch.ones(state.shape[1]) * (1 - self.mask_prob)\n","mask = torch.bernoulli(prob).to(self.device)\n","mask = torch.tile(mask, (state.shape[0],1))\n","state_concat = torch.cat([state * mask, mask], dim=1)\n","causal_rep = self.causal_features_encoder(state_concat)  # need this encoder: S -> rep\n","\n","# 1. Policy loss\n","qvalues = self.policy_network(causal_rep) # need this encoder:  rep -> A\n","ce_loss = nn.CrossEntropyLoss()(qvalues, action)\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"eUIVAlvyMLu9","executionInfo":{"status":"ok","timestamp":1650781048862,"user_tz":240,"elapsed":270,"user":{"displayName":"2 Lin","userId":"15119774230930098070"}},"outputId":"496e78ff-1e0c-4ca1-f915-530ca99590b4"},"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["tensor([[0., 0., 0.,  ..., 1., 0., 1.],\n","        [0., 0., 0.,  ..., 1., 0., 1.],\n","        [0., 0., 0.,  ..., 1., 0., 1.],\n","        ...,\n","        [0., 0., 0.,  ..., 1., 0., 1.],\n","        [0., 0., 0.,  ..., 1., 0., 1.],\n","        [0., 0., 0.,  ..., 1., 0., 1.]], device='cuda:0')"]},"metadata":{},"execution_count":40}]},{"cell_type":"code","source":["student.best_mask[0,:]"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"xG0QKGYLPJ2i","executionInfo":{"status":"ok","timestamp":1650783581480,"user_tz":240,"elapsed":339,"user":{"displayName":"2 Lin","userId":"15119774230930098070"}},"outputId":"f2780d24-ec29-43e2-e903-18e4a8bce0a4"},"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["tensor([1., 0., 0., 0., 0., 1., 1., 1.], device='cuda:0')"]},"metadata":{},"execution_count":21}]},{"cell_type":"code","source":["env_wrapper_out_of_sample = EnvWrapper(\n","    env=gym.make(config[\"ENV\"]), mult_factor=get_test_mult_factors(config['NOISE_DIM'] - 1), idx=3, seed=1\n",")\n","\n","env_wrapper_out_of_sample.noise = 0\n","\n","action_match, return_mean, return_std = student.test(\n","    num_episodes=config[\"NUM_TRAJS_VALID\"], env_wrapper=env_wrapper_out_of_sample\n",")\n","\n","result = (action_match, return_mean, return_std)\n","result"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"MbgjeWKCQeGQ","executionInfo":{"status":"ok","timestamp":1650784004740,"user_tz":240,"elapsed":1285,"user":{"displayName":"2 Lin","userId":"15119774230930098070"}},"outputId":"55ae5995-8ad5-455b-ea28-b602635d1357"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["epoch 0/100 return: 10.0\n","epoch 10/100 return: 10.0\n","epoch 20/100 return: 9.0\n","epoch 30/100 return: 9.0\n","epoch 40/100 return: 8.0\n","epoch 50/100 return: 9.0\n","epoch 60/100 return: 10.0\n","epoch 70/100 return: 9.0\n","epoch 80/100 return: 10.0\n","epoch 90/100 return: 10.0\n"]},{"output_type":"execute_result","data":{"text/plain":["(0.0, 9.36, 0.7282856582413249)"]},"metadata":{},"execution_count":10}]},{"cell_type":"code","source":["student.buffer.total_size"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"ypa0Wl-aVQmf","executionInfo":{"status":"ok","timestamp":1650783097144,"user_tz":240,"elapsed":249,"user":{"displayName":"2 Lin","userId":"15119774230930098070"}},"outputId":"4707a8b2-7000-4dff-d93b-968e059dd18a"},"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["498000"]},"metadata":{},"execution_count":21}]},{"cell_type":"code","source":["config"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"TMBNn4cbhKWe","executionInfo":{"status":"ok","timestamp":1650786087423,"user_tz":240,"elapsed":299,"user":{"displayName":"2 Lin","userId":"15119774230930098070"}},"outputId":"ee7bc44f-f36f-4c75-ee4d-ed290a093b05"},"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["{'ADAM_ALPHA': 0.001,\n"," 'ALG': 'BCIRMStudent_Apr17',\n"," 'BATCH_SIZE': 64,\n"," 'ENV': 'CartPole-v1',\n"," 'MASK_PROB': 0.5,\n"," 'METHOD': 'CCIL',\n"," 'MLP_WIDTHS': 64,\n"," 'NOISE_DIM': 4,\n"," 'NUM_REPETITIONS': 15,\n"," 'NUM_STEPS_TRAIN': 100000,\n"," 'NUM_STEPS_TRAIN_ENERGY_MODEL': 1000,\n"," 'NUM_TRAINING_ENVS': 2,\n"," 'NUM_TRAJS_GIVEN': 20,\n"," 'NUM_TRAJS_VALID': 100,\n"," 'REP_SIZE': 16,\n"," 'SAMPLING_RATE': 5,\n"," 'SGLD_BUFFER_SIZE': 10000,\n"," 'SGLD_LEARN_RATE': 0.01,\n"," 'SGLD_NOISE_COEF': 0.01,\n"," 'SGLD_NUM_STEPS': 100,\n"," 'SGLD_REINIT_FREQ': 0.05,\n"," 'TRAJ_SHIFT': 20,\n"," 'TRIAL': 0}"]},"metadata":{},"execution_count":8}]}]}