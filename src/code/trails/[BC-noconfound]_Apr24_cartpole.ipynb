{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"[BC-noconfound]_Apr24_cartpole.ipynb","provenance":[],"collapsed_sections":[],"toc_visible":true},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"},"accelerator":"GPU"},"cells":[{"cell_type":"code","execution_count":1,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"cXttaQe-507l","executionInfo":{"status":"ok","timestamp":1650827374591,"user_tz":240,"elapsed":20478,"user":{"displayName":"2 Lin","userId":"15119774230930098070"}},"outputId":"2dcafa63-c804-4fcc-c61b-c5f62b8ba2a6"},"outputs":[{"output_type":"stream","name":"stdout","text":["Mounted at /content/drive\n"]}],"source":["from google.colab import drive\n","drive.mount('/content/drive')\n","\n","\n","import os\n","\n","import torch\n","os.chdir('/content/drive/MyDrive/ImitationLearning/Invariant-Causal-Imitation-Learning-main/')\n"]},{"cell_type":"markdown","source":["# load"],"metadata":{"id":"CMy2nIGPEiAs"}},{"cell_type":"code","source":["!pip install mpi4py \n","!pip install box2d-py\n","!pip install box2d \n","!pip3 install gym[Box_2D] \n","!pip install gym==0.17.2 -qqq\n","!pip install numpy~=1.18.2 -qqq\n","!pip install pandas~=1.0.4 -qqq\n","!pip install PyYAML~=5.4.1 -qqq\n","!pip install scikit-learn~=0.22.2 -qqq\n","!pip install scipy~=1.1.0 -qqq\n","!pip install stable-baselines~=2.10.1 -qqq\n","!pip install tensorflow~=1.15.0 -qqq\n","!pip install torch>=1.6.0 -qqq\n","!pip install tqdm~=4.32.1 -qqq\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"AhOh_Fj16Eow","executionInfo":{"status":"ok","timestamp":1650827530001,"user_tz":240,"elapsed":153315,"user":{"displayName":"2 Lin","userId":"15119774230930098070"}},"outputId":"8d52742e-af14-42ad-9fdd-335304de0212"},"execution_count":2,"outputs":[{"output_type":"stream","name":"stdout","text":["Collecting mpi4py\n","  Downloading mpi4py-3.1.3.tar.gz (2.5 MB)\n","\u001b[K     |████████████████████████████████| 2.5 MB 7.5 MB/s \n","\u001b[?25h  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n","  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n","    Preparing wheel metadata ... \u001b[?25l\u001b[?25hdone\n","Building wheels for collected packages: mpi4py\n","  Building wheel for mpi4py (PEP 517) ... \u001b[?25l\u001b[?25hdone\n","  Created wheel for mpi4py: filename=mpi4py-3.1.3-cp37-cp37m-linux_x86_64.whl size=2185303 sha256=8dd1390a4e7d0526cf775bcd4fc49651a523a86d511531f6b8d3e0346e0051e1\n","  Stored in directory: /root/.cache/pip/wheels/7a/07/14/6a0c63fa2c6e473c6edc40985b7d89f05c61ff25ee7f0ad9ac\n","Successfully built mpi4py\n","Installing collected packages: mpi4py\n","Successfully installed mpi4py-3.1.3\n","Collecting box2d-py\n","  Downloading box2d_py-2.3.8-cp37-cp37m-manylinux1_x86_64.whl (448 kB)\n","\u001b[K     |████████████████████████████████| 448 kB 8.6 MB/s \n","\u001b[?25hInstalling collected packages: box2d-py\n","Successfully installed box2d-py-2.3.8\n","Collecting box2d\n","  Downloading Box2D-2.3.10-cp37-cp37m-manylinux1_x86_64.whl (1.3 MB)\n","\u001b[K     |████████████████████████████████| 1.3 MB 7.1 MB/s \n","\u001b[?25hInstalling collected packages: box2d\n","Successfully installed box2d-2.3.10\n","Requirement already satisfied: gym[Box_2D] in /usr/local/lib/python3.7/dist-packages (0.17.3)\n","\u001b[33mWARNING: gym 0.17.3 does not provide the extra 'box_2d'\u001b[0m\n","Requirement already satisfied: pyglet<=1.5.0,>=1.4.0 in /usr/local/lib/python3.7/dist-packages (from gym[Box_2D]) (1.5.0)\n","Requirement already satisfied: scipy in /usr/local/lib/python3.7/dist-packages (from gym[Box_2D]) (1.4.1)\n","Requirement already satisfied: cloudpickle<1.7.0,>=1.2.0 in /usr/local/lib/python3.7/dist-packages (from gym[Box_2D]) (1.3.0)\n","Requirement already satisfied: numpy>=1.10.4 in /usr/local/lib/python3.7/dist-packages (from gym[Box_2D]) (1.21.6)\n","Requirement already satisfied: future in /usr/local/lib/python3.7/dist-packages (from pyglet<=1.5.0,>=1.4.0->gym[Box_2D]) (0.16.0)\n","\u001b[K     |████████████████████████████████| 1.6 MB 9.4 MB/s \n","\u001b[?25h  Building wheel for gym (setup.py) ... \u001b[?25l\u001b[?25hdone\n","\u001b[K     |████████████████████████████████| 20.1 MB 1.3 MB/s \n","\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n","tensorflow 2.8.0 requires tf-estimator-nightly==2.8.0.dev2021122109, which is not installed.\n","tensorflow 2.8.0 requires numpy>=1.20, but you have numpy 1.18.5 which is incompatible.\n","tables 3.7.0 requires numpy>=1.19.0, but you have numpy 1.18.5 which is incompatible.\n","jaxlib 0.3.2+cuda11.cudnn805 requires numpy>=1.19, but you have numpy 1.18.5 which is incompatible.\n","jax 0.3.4 requires numpy>=1.19, but you have numpy 1.18.5 which is incompatible.\n","datascience 0.10.6 requires folium==0.2.1, but you have folium 0.8.3 which is incompatible.\n","albumentations 0.1.12 requires imgaug<0.2.7,>=0.2.5, but you have imgaug 0.2.9 which is incompatible.\u001b[0m\n","\u001b[K     |████████████████████████████████| 10.1 MB 7.1 MB/s \n","\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n","google-colab 1.0.0 requires pandas>=1.1.0; python_version >= \"3.0\", but you have pandas 1.0.5 which is incompatible.\u001b[0m\n","\u001b[K     |████████████████████████████████| 636 kB 8.6 MB/s \n","\u001b[K     |████████████████████████████████| 7.1 MB 6.5 MB/s \n","\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n","yellowbrick 1.4 requires scikit-learn>=1.0.0, but you have scikit-learn 0.22.2.post1 which is incompatible.\n","imbalanced-learn 0.8.1 requires scikit-learn>=0.24, but you have scikit-learn 0.22.2.post1 which is incompatible.\u001b[0m\n","\u001b[K     |████████████████████████████████| 31.2 MB 1.4 MB/s \n","\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n","yellowbrick 1.4 requires scikit-learn>=1.0.0, but you have scikit-learn 0.22.2.post1 which is incompatible.\n","pymc3 3.11.4 requires scipy>=1.2.0, but you have scipy 1.1.0 which is incompatible.\n","plotnine 0.6.0 requires scipy>=1.2.0, but you have scipy 1.1.0 which is incompatible.\n","jaxlib 0.3.2+cuda11.cudnn805 requires numpy>=1.19, but you have numpy 1.18.5 which is incompatible.\n","jax 0.3.4 requires numpy>=1.19, but you have numpy 1.18.5 which is incompatible.\n","jax 0.3.4 requires scipy>=1.2.1, but you have scipy 1.1.0 which is incompatible.\n","imbalanced-learn 0.8.1 requires scikit-learn>=0.24, but you have scikit-learn 0.22.2.post1 which is incompatible.\n","albumentations 0.1.12 requires imgaug<0.2.7,>=0.2.5, but you have imgaug 0.2.9 which is incompatible.\u001b[0m\n","\u001b[K     |████████████████████████████████| 240 kB 8.3 MB/s \n","\u001b[K     |████████████████████████████████| 110.5 MB 1.3 MB/s \n","\u001b[K     |████████████████████████████████| 2.9 MB 49.7 MB/s \n","\u001b[K     |████████████████████████████████| 50 kB 7.2 MB/s \n","\u001b[K     |████████████████████████████████| 3.8 MB 52.5 MB/s \n","\u001b[K     |████████████████████████████████| 503 kB 43.0 MB/s \n","\u001b[?25h  Building wheel for gast (setup.py) ... \u001b[?25l\u001b[?25hdone\n","\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n","tensorflow-probability 0.16.0 requires gast>=0.3.2, but you have gast 0.2.2 which is incompatible.\n","kapre 0.3.7 requires tensorflow>=2.0.0, but you have tensorflow 1.15.5 which is incompatible.\u001b[0m\n","\u001b[K     |████████████████████████████████| 50 kB 4.2 MB/s \n","\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n","spacy 2.2.4 requires tqdm<5.0.0,>=4.38.0, but you have tqdm 4.32.2 which is incompatible.\n","panel 0.12.1 requires tqdm>=4.48.0, but you have tqdm 4.32.2 which is incompatible.\n","fbprophet 0.7.1 requires tqdm>=4.36.1, but you have tqdm 4.32.2 which is incompatible.\u001b[0m\n","\u001b[?25h"]}]},{"cell_type":"markdown","source":["#config"],"metadata":{"id":"UoOvpQGcfM2h"}},{"cell_type":"code","source":["\n","config = {\n","    \"ENV\": \"CartPole-v1\",\n","    \"ALG\": \"BCIRMStudent_Apr17\",\n","    \"NUM_TRAJS_GIVEN\": 20, #\n","    \"NUM_TRAINING_ENVS\": 2,\n","    \"NOISE_DIM\": 4,\n","    \"REP_SIZE\": 16,\n","    \"TRAJ_SHIFT\": 20, # 20,\n","    \"SAMPLING_RATE\": 5,\n","    \"NUM_STEPS_TRAIN\": 10000,\n","    \"NUM_TRAJS_VALID\": 100,\n","    \"NUM_REPETITIONS\": 10,\n","    \"BATCH_SIZE\": 64,\n","    \"MLP_WIDTHS\": 64,\n","    \"ADAM_ALPHA\": 1e-3,\n","    \"SGLD_BUFFER_SIZE\": 10000,\n","    \"SGLD_LEARN_RATE\": 0.01,\n","    \"SGLD_NOISE_COEF\": 0.01,\n","    \"SGLD_NUM_STEPS\": 100,\n","    \"SGLD_REINIT_FREQ\": 0.05,\n","    \"NUM_STEPS_TRAIN_ENERGY_MODEL\": 1000,\n","    'TRIAL': 0\n","}\n","\n","\n","#config['ENV'] = \"LunarLander-v2\"\n","config['ENV'] = \"CartPole-v1\"\n","\n","#config['METHOD'] = \"BCIRM\"\n","config['METHOD'] = \"BC-noconfound\"\n","\n","\n","\n","if config['METHOD'] == 'BCIRM':\n","    config['l2_regularizer_weight'] = 0.001\n","    config['penalty_weight'] = 10000\n","    config['penalty_anneal_iters'] = 2500\n","\n"],"metadata":{"id":"4qkwWpjMfNzN","executionInfo":{"status":"ok","timestamp":1650827551030,"user_tz":240,"elapsed":121,"user":{"displayName":"2 Lin","userId":"15119774230930098070"}}},"execution_count":5,"outputs":[]},{"cell_type":"markdown","source":["#testing/il"],"metadata":{"id":"tXRGSCjZfbnX"}},{"cell_type":"code","source":["import argparse\n","import os\n","import pickle\n","\n","import gym\n","import numpy as np\n","import pandas as pd\n","import yaml\n","import numpy as np\n","\n","try:\n","    from paths import get_model_path, get_trajs_path  # noqa\n","except (ModuleNotFoundError, ImportError):\n","    from testing.paths import get_model_path, get_trajs_path  # pylint: disable=reimported\n","\n","from contrib.energy_model import EnergyModel\n","from contrib.env_wrapper import EnvWrapper, get_test_mult_factors\n","from network import (\n","    EnvDiscriminator,\n","    FeaturesDecoder,\n","    FeaturesEncoder,\n","    MineNetwork,\n","    ObservationsDecoder,\n","    StudentNetwork,\n",")\n","from student import ICILStudent, BCStudent, BCIRMStudent, BCStudent_noconfound\n","from testing.train_utils import fill_buffer, make_agent, save_results\n"],"metadata":{"id":"JIj45lBefdU5","executionInfo":{"status":"ok","timestamp":1650827556209,"user_tz":240,"elapsed":1575,"user":{"displayName":"2 Lin","userId":"15119774230930098070"}}},"execution_count":6,"outputs":[]},{"cell_type":"markdown","source":["# make student"],"metadata":{"id":"hsMME-Bm3oMT"}},{"cell_type":"code","source":["\n","\n","# pylint: disable=redefined-outer-name\n","def make_student(run_seed, config):\n","    env = gym.make(config[\"ENV\"])\n","    trajs_path = get_trajs_path(config[\"ENV\"], \"student_\" + config[\"ALG\"], env_id=\"student\", run_seed=run_seed)\n","    model_path = get_model_path(config[\"ENV\"], \"student_\" + config[\"ALG\"], run_seed=run_seed)\n","\n","    state_dim = env.observation_space.shape[0] + config[\"NOISE_DIM\"]\n","    action_dim = env.action_space.n\n","    num_training_envs = config[\"NUM_TRAINING_ENVS\"]\n","\n","    # run_seed = run_seed\n","    batch_size = config[\"BATCH_SIZE\"]\n","    teacher = make_agent(config[\"ENV\"], config[\"EXPERT_ALG\"], config[\"NUM_TRAINING_ENVS\"])\n","    teacher.load_pretrained()\n","\n","    buffer = fill_buffer(\n","        trajs_path=teacher.trajs_paths,\n","        batch_size=batch_size,\n","        run_seed=run_seed,\n","        traj_shift=config[\"TRAJ_SHIFT\"],\n","        buffer_size_in_trajs=config[\"NUM_TRAJS_GIVEN\"],\n","        sampling_rate=config[\"SAMPLING_RATE\"],\n","    )\n","\n","    if buffer.total_size < batch_size:\n","        batch_size = buffer.total_size\n","\n","\n","\n","    ##########################      COMMON      ##########################\n","\n","    print(\"state_dim\", state_dim)\n","\n","    causal_features_encoder = FeaturesEncoder(\n","        input_size=state_dim, representation_size=config[\"REP_SIZE\"], width=config[\"MLP_WIDTHS\"]\n","    )\n","\n","    policy_network = StudentNetwork(in_dim=config[\"REP_SIZE\"], out_dim=action_dim, width=config[\"MLP_WIDTHS\"])\n","\n","    #print(\"config method = \", config['METHOD'])\n","\n","\n","    ##########################       BC       #######################\n","\n","    if config['METHOD'] == 'BC':\n","\n","        return BCStudent(\n","            env=env,\n","            trajs_paths=trajs_path,\n","            model_path=model_path,\n","            num_training_envs=num_training_envs,\n","            teacher=teacher,\n","            causal_features_encoder=causal_features_encoder,\n","            policy_network=policy_network,\n","            buffer=buffer,\n","            adam_alpha=config[\"ADAM_ALPHA\"],\n","            config = config\n","        )\n","\n","    ##########################       BC - non confound       #######################\n","\n","    if config['METHOD'] == \"BC-noconfound\":\n","\n","        causal_features_encoder = FeaturesEncoder(\n","                input_size=state_dim-4, representation_size=config[\"REP_SIZE\"], width=config[\"MLP_WIDTHS\"]\n","            )\n","\n","        return BCStudent_noconfound(\n","            env=env,\n","            trajs_paths=trajs_path,\n","            model_path=model_path,\n","            num_training_envs=num_training_envs,\n","            teacher=teacher,\n","            causal_features_encoder=causal_features_encoder,\n","            policy_network=policy_network,\n","            buffer=buffer,\n","            adam_alpha=config[\"ADAM_ALPHA\"],\n","            config = config\n","        )\n","\n","\n","\n","    ##########################       BC IRM       #######################\n","\n","\n","    elif config['METHOD'] == 'BCIRM':\n","\n","        return BCIRMStudent(\n","            env=env,\n","            trajs_paths=trajs_path,\n","            model_path=model_path,\n","            num_training_envs=num_training_envs,\n","            teacher=teacher,\n","            causal_features_encoder=causal_features_encoder,\n","            policy_network=policy_network,\n","            buffer=buffer,\n","            adam_alpha=config[\"ADAM_ALPHA\"],\n","            config = config\n","        )\n","\n","    ##########################       ICIL        #######################\n","\n","    elif config['METHOD'] == 'ICIL':\n","        energy_model = EnergyModel(\n","            in_dim=state_dim,\n","            width=config[\"MLP_WIDTHS\"],\n","            batch_size=batch_size,\n","            adam_alpha=config[\"ADAM_ALPHA\"],\n","            buffer=buffer,\n","            sgld_buffer_size=config[\"SGLD_BUFFER_SIZE\"],\n","            sgld_learn_rate=config[\"SGLD_LEARN_RATE\"],\n","            sgld_noise_coef=config[\"SGLD_NOISE_COEF\"],\n","            sgld_num_steps=config[\"SGLD_NUM_STEPS\"],\n","            sgld_reinit_freq=config[\"SGLD_REINIT_FREQ\"],\n","        )\n","        energy_model.train(num_updates=config[\"NUM_STEPS_TRAIN_ENERGY_MODEL\"])\n","\n","\n","        causal_features_decoder = FeaturesDecoder(\n","            action_size=action_dim, representation_size=config[\"REP_SIZE\"], width=config[\"MLP_WIDTHS\"]\n","        )\n","\n","        observations_decoder = ObservationsDecoder(\n","            representation_size=config[\"REP_SIZE\"], out_size=state_dim, width=config[\"MLP_WIDTHS\"]\n","        )\n","\n","\n","        env_discriminator = EnvDiscriminator(\n","            representation_size=config[\"REP_SIZE\"], num_envs=config[\"NUM_TRAINING_ENVS\"], width=config[\"MLP_WIDTHS\"]\n","        )\n","\n","        noise_features_encoders = [\n","            FeaturesEncoder(input_size=state_dim, representation_size=config[\"REP_SIZE\"], width=config[\"MLP_WIDTHS\"])\n","            for i in range(num_training_envs)\n","        ]\n","        noise_features_decoders = [\n","            FeaturesDecoder(action_size=action_dim, representation_size=config[\"REP_SIZE\"], width=config[\"MLP_WIDTHS\"])\n","            for i in range(num_training_envs)\n","        ]\n","\n","        mine_network = MineNetwork(x_dim=config[\"REP_SIZE\"], z_dim=config[\"REP_SIZE\"], width=config[\"MLP_WIDTHS\"])\n","\n","        return ICILStudent(\n","            env=env,\n","            trajs_paths=trajs_path,\n","            model_path=model_path,\n","            num_training_envs=num_training_envs,\n","            teacher=teacher,\n","            causal_features_encoder=causal_features_encoder,\n","            noise_features_encoders=noise_features_encoders,\n","            causal_features_decoder=causal_features_decoder,\n","            noise_features_decoders=noise_features_decoders,\n","            observations_decoder=observations_decoder,\n","            env_discriminator=env_discriminator,\n","            policy_network=policy_network,\n","            energy_model=energy_model,\n","            mine_network=mine_network,\n","            buffer=buffer,\n","            adam_alpha=config[\"ADAM_ALPHA\"],\n","            config = config\n","        )\n","\n","\n","def init_arg():\n","    parser = argparse.ArgumentParser()\n","    parser.add_argument(\"--env_name\", default=\"CartPole-v1\")\n","    parser.add_argument(\"--num_trajectories\", default=20, type=int)\n","    parser.add_argument(\"--trial\", default=0, type=int)\n","    return parser.parse_args()\n"],"metadata":{"id":"BYsHrHrlffKj","executionInfo":{"status":"ok","timestamp":1650827689358,"user_tz":240,"elapsed":137,"user":{"displayName":"2 Lin","userId":"15119774230930098070"}}},"execution_count":10,"outputs":[]},{"cell_type":"code","source":["config[\"REP_SIZE\"]"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"ie5S70J1_4NQ","executionInfo":{"status":"ok","timestamp":1650827691893,"user_tz":240,"elapsed":141,"user":{"displayName":"2 Lin","userId":"15119774230930098070"}},"outputId":"7e063988-2402-4c63-dcf5-6788a6340b42"},"execution_count":11,"outputs":[{"output_type":"execute_result","data":{"text/plain":["16"]},"metadata":{},"execution_count":11}]},{"cell_type":"markdown","source":["#10 Trails -- BC -- cartpole"],"metadata":{"id":"Lkejd0k3UWIi"}},{"cell_type":"code","source":["config['ENV'] = 'CartPole-v1'\n","config['METHOD'] = \"BC-noconfound\"\n","\n","for traj_num in [1, 2, 5, 10, 20, 30, 40, 50]:\n","    config[\"NUM_TRAJS_GIVEN\"] = traj_num\n","    config[\"TRAJ_SHIFT\"] = traj_num\n","\n","\n","    config['ALG'] = \"FINAL_Apr24_BCStudent_noconfound_origindata_trajnum\" + str(traj_num)\n","\n","\n","    ###############.  settings   ###############\n","    #config['ALG'] = \"BCStudent_Apr19_replicatedata\"\n","    #config['METHOD'] = \"BC\"\n","    #config['ENV'] == \"CartPole-v1\"\n","    #config['ENV'] == \"LunarLander-v2\"\n","    #config[\"NUM_TRAJS_GIVEN\"] = 20\n","    #config[\"TRAJ_SHIFT\"] = 20\n","    ###############.  settings   ###############\n","\n","\n","\n","    if config['METHOD'] == 'BCIRM':\n","        config['l2_regularizer_weight'] = 0.001\n","        config['penalty_weight'] = 10000\n","        config['penalty_anneal_iters'] = 100\n","\n","    all_results_trail = []\n","\n","    for trail in range(1): \n","        config['TRIAL'] = trail \n","\n","\n","        ###############.  start a trail   ###############\n","\n","        config[\"EXPERT_ALG\"] = yaml.load(open(\"testing/config.yml\"), Loader=yaml.FullLoader)[config[\"ENV\"]]\n","        print(\"Config: %s\" % config)\n","\n","        TRIAL = config[\"TRIAL\"] #args.trial\n","        print(\"Trial number %s\" % TRIAL)\n","\n","        results_dir_base = \"testing/results/\"\n","        results_dir = os.path.join(results_dir_base, config[\"ENV\"], str(config[\"NUM_TRAJS_GIVEN\"]), config[\"ALG\"])\n","\n","        if not os.path.exists(results_dir):\n","            os.makedirs(results_dir)\n","\n","        config_file = \"trial_\" + str(TRIAL) + \"_\" + \"config.pkl\"\n","\n","        results_file_name = \"trial_\" + str(TRIAL) + \"_\" + \"results.csv\"\n","        results_file_path = os.path.join(results_dir, results_file_name)\n","\n","        if os.path.exists(os.path.join(results_dir, config_file)):\n","            raise NameError(\"CONFIG file already exists %s. Choose a different trial number.\" % config_file)\n","        pickle.dump(config, open(os.path.join(results_dir, config_file), \"wb\"))\n","\n","\n","\n","\n","        ###############.  10 runs for each trail   ###############\n","\n","        print(\"config method = \", config['METHOD'])\n","        print(\"config env = \", config['ENV'])\n","\n","        for run_seed in range(config[\"NUM_REPETITIONS\"]):\n","            print(\"Run %s out of %s\" % (run_seed + 1, config[\"NUM_REPETITIONS\"]))\n","            student = make_student(run_seed, config)\n","            student.train(num_updates=config[\"NUM_STEPS_TRAIN\"])\n","\n","            env_wrapper_out_of_sample = EnvWrapper(\n","                env=gym.make(config[\"ENV\"]), mult_factor=get_test_mult_factors(config['NOISE_DIM'] - 1), idx=3, seed=1\n","            )\n","            action_match, return_mean, return_std = student.test(\n","                num_episodes=config[\"NUM_TRAJS_VALID\"], env_wrapper=env_wrapper_out_of_sample\n","            )\n","\n","            result = (action_match, return_mean, return_std)\n","            print(\"###############    Reward for test environment for run %s: %s.   ###############\\n\\n\" % (run_seed + 1, return_mean))\n","            save_results(results_file_path, run_seed, action_match, return_mean, return_std)\n","\n","        results_trial = pd.read_csv(\n","            \"testing/results/\"\n","            + config[\"ENV\"]\n","            + \"/\"\n","            + str(config[\"NUM_TRAJS_GIVEN\"])\n","            + \"/\"\n","            + config[\"ALG\"]\n","            + \"/trial_\"\n","            + str(TRIAL)\n","            + \"_results.csv\",\n","            header=None,\n","        )\n","\n","        print(\"Average reward for 10 repetitions: %s\" % np.mean(results_trial[2].values))\n","\n","        all_results_trail.append(np.mean(results_trial[2].values))\n","\n","    print(\"ALL RESULTS TRAIL:\" , all_results_trail)"],"metadata":{"id":"iS_lRP8MUWIi","executionInfo":{"status":"ok","timestamp":1650834510162,"user_tz":240,"elapsed":6593435,"user":{"displayName":"2 Lin","userId":"15119774230930098070"}},"colab":{"base_uri":"https://localhost:8080/"},"outputId":"305a8319-927a-44e9-fbbf-b68c87fb9659"},"execution_count":18,"outputs":[{"output_type":"stream","name":"stdout","text":["Config: {'ENV': 'CartPole-v1', 'ALG': 'FINAL_Apr24_BCStudent_noconfound_origindata_trajnum1', 'NUM_TRAJS_GIVEN': 1, 'NUM_TRAINING_ENVS': 2, 'NOISE_DIM': 4, 'REP_SIZE': 16, 'TRAJ_SHIFT': 1, 'SAMPLING_RATE': 5, 'NUM_STEPS_TRAIN': 10000, 'NUM_TRAJS_VALID': 100, 'NUM_REPETITIONS': 10, 'BATCH_SIZE': 64, 'MLP_WIDTHS': 64, 'ADAM_ALPHA': 0.001, 'SGLD_BUFFER_SIZE': 10000, 'SGLD_LEARN_RATE': 0.01, 'SGLD_NOISE_COEF': 0.01, 'SGLD_NUM_STEPS': 100, 'SGLD_REINIT_FREQ': 0.05, 'NUM_STEPS_TRAIN_ENERGY_MODEL': 1000, 'TRIAL': 0, 'METHOD': 'BC-noconfound', 'EXPERT_ALG': 'dqn'}\n","Trial number 0\n","config method =  BC-noconfound\n","config env =  CartPole-v1\n","Run 1 out of 10\n","state_dim 8\n","epoch 9000/10000, policy loss 0.0068330843932926655\tepoch 0/100 return: 378.0\n","epoch 10/100 return: 499.0\n","epoch 20/100 return: 433.0\n","epoch 30/100 return: 437.0\n","epoch 40/100 return: 483.0\n","epoch 50/100 return: 454.0\n","epoch 60/100 return: 482.0\n","epoch 70/100 return: 495.0\n","epoch 80/100 return: 470.0\n","epoch 90/100 return: 496.0\n","###############    Reward for test environment for run 1: 446.78.   ###############\n","\n","\n","Run 2 out of 10\n","state_dim 8\n","epoch 9000/10000, policy loss 0.007889384403824806\tepoch 0/100 return: 204.0\n","epoch 10/100 return: 500.0\n","epoch 20/100 return: 500.0\n","epoch 30/100 return: 207.0\n","epoch 40/100 return: 186.0\n","epoch 50/100 return: 219.0\n","epoch 60/100 return: 500.0\n","epoch 70/100 return: 199.0\n","epoch 80/100 return: 500.0\n","epoch 90/100 return: 222.0\n","###############    Reward for test environment for run 2: 287.04.   ###############\n","\n","\n","Run 3 out of 10\n","state_dim 8\n","epoch 9000/10000, policy loss 0.03729340806603432\tepoch 0/100 return: 500.0\n","epoch 10/100 return: 500.0\n","epoch 20/100 return: 500.0\n","epoch 30/100 return: 500.0\n","epoch 40/100 return: 500.0\n","epoch 50/100 return: 500.0\n","epoch 60/100 return: 500.0\n","epoch 70/100 return: 500.0\n","epoch 80/100 return: 500.0\n","epoch 90/100 return: 500.0\n","###############    Reward for test environment for run 3: 483.74.   ###############\n","\n","\n","Run 4 out of 10\n","state_dim 8\n","epoch 9000/10000, policy loss 0.0009558038436807692\tepoch 0/100 return: 500.0\n","epoch 10/100 return: 500.0\n","epoch 20/100 return: 500.0\n","epoch 30/100 return: 500.0\n","epoch 40/100 return: 500.0\n","epoch 50/100 return: 500.0\n","epoch 60/100 return: 500.0\n","epoch 70/100 return: 500.0\n","epoch 80/100 return: 500.0\n","epoch 90/100 return: 500.0\n","###############    Reward for test environment for run 4: 469.89.   ###############\n","\n","\n","Run 5 out of 10\n","state_dim 8\n","epoch 9000/10000, policy loss 0.03277077153325081\tepoch 0/100 return: 500.0\n","epoch 10/100 return: 500.0\n","epoch 20/100 return: 147.0\n","epoch 30/100 return: 175.0\n","epoch 40/100 return: 176.0\n","epoch 50/100 return: 368.0\n","epoch 60/100 return: 366.0\n","epoch 70/100 return: 273.0\n","epoch 80/100 return: 353.0\n","epoch 90/100 return: 162.0\n","###############    Reward for test environment for run 5: 291.64.   ###############\n","\n","\n","Run 6 out of 10\n","state_dim 8\n","epoch 9000/10000, policy loss 0.03501620888710022\tepoch 0/100 return: 500.0\n","epoch 10/100 return: 500.0\n","epoch 20/100 return: 500.0\n","epoch 30/100 return: 500.0\n","epoch 40/100 return: 500.0\n","epoch 50/100 return: 500.0\n","epoch 60/100 return: 500.0\n","epoch 70/100 return: 500.0\n","epoch 80/100 return: 500.0\n","epoch 90/100 return: 500.0\n","###############    Reward for test environment for run 6: 500.0.   ###############\n","\n","\n","Run 7 out of 10\n","state_dim 8\n","epoch 9000/10000, policy loss 0.00012962959590367973\tepoch 0/100 return: 500.0\n","epoch 10/100 return: 500.0\n","epoch 20/100 return: 500.0\n","epoch 30/100 return: 500.0\n","epoch 40/100 return: 500.0\n","epoch 50/100 return: 500.0\n","epoch 60/100 return: 500.0\n","epoch 70/100 return: 500.0\n","epoch 80/100 return: 500.0\n","epoch 90/100 return: 500.0\n","###############    Reward for test environment for run 7: 500.0.   ###############\n","\n","\n","Run 8 out of 10\n","state_dim 8\n","epoch 9000/10000, policy loss 0.0008804830140434206\tepoch 0/100 return: 26.0\n","epoch 10/100 return: 47.0\n","epoch 20/100 return: 32.0\n","epoch 30/100 return: 47.0\n","epoch 40/100 return: 71.0\n","epoch 50/100 return: 32.0\n","epoch 60/100 return: 60.0\n","epoch 70/100 return: 42.0\n","epoch 80/100 return: 38.0\n","epoch 90/100 return: 28.0\n","###############    Reward for test environment for run 8: 46.07.   ###############\n","\n","\n","Run 9 out of 10\n","state_dim 8\n","epoch 9000/10000, policy loss 0.0008614130201749504\tepoch 0/100 return: 279.0\n","epoch 10/100 return: 206.0\n","epoch 20/100 return: 227.0\n","epoch 30/100 return: 227.0\n","epoch 40/100 return: 113.0\n","epoch 50/100 return: 223.0\n","epoch 60/100 return: 237.0\n","epoch 70/100 return: 241.0\n","epoch 80/100 return: 225.0\n","epoch 90/100 return: 226.0\n","###############    Reward for test environment for run 9: 247.94.   ###############\n","\n","\n","Run 10 out of 10\n","state_dim 8\n","epoch 9000/10000, policy loss 0.0013282153522595763\tepoch 0/100 return: 500.0\n","epoch 10/100 return: 500.0\n","epoch 20/100 return: 500.0\n","epoch 30/100 return: 500.0\n","epoch 40/100 return: 500.0\n","epoch 50/100 return: 500.0\n","epoch 60/100 return: 500.0\n","epoch 70/100 return: 500.0\n","epoch 80/100 return: 500.0\n","epoch 90/100 return: 500.0\n","###############    Reward for test environment for run 10: 498.96.   ###############\n","\n","\n","Average reward for 10 repetitions: 377.206\n","ALL RESULTS TRAIL: [377.206]\n","Config: {'ENV': 'CartPole-v1', 'ALG': 'FINAL_Apr24_BCStudent_noconfound_origindata_trajnum2', 'NUM_TRAJS_GIVEN': 2, 'NUM_TRAINING_ENVS': 2, 'NOISE_DIM': 4, 'REP_SIZE': 16, 'TRAJ_SHIFT': 2, 'SAMPLING_RATE': 5, 'NUM_STEPS_TRAIN': 10000, 'NUM_TRAJS_VALID': 100, 'NUM_REPETITIONS': 10, 'BATCH_SIZE': 64, 'MLP_WIDTHS': 64, 'ADAM_ALPHA': 0.001, 'SGLD_BUFFER_SIZE': 10000, 'SGLD_LEARN_RATE': 0.01, 'SGLD_NOISE_COEF': 0.01, 'SGLD_NUM_STEPS': 100, 'SGLD_REINIT_FREQ': 0.05, 'NUM_STEPS_TRAIN_ENERGY_MODEL': 1000, 'TRIAL': 0, 'METHOD': 'BC-noconfound', 'EXPERT_ALG': 'dqn'}\n","Trial number 0\n","config method =  BC-noconfound\n","config env =  CartPole-v1\n","Run 1 out of 10\n","state_dim 8\n","epoch 9000/10000, policy loss 0.14326944947242737\tepoch 0/100 return: 500.0\n","epoch 10/100 return: 500.0\n","epoch 20/100 return: 500.0\n","epoch 30/100 return: 500.0\n","epoch 40/100 return: 455.0\n","epoch 50/100 return: 500.0\n","epoch 60/100 return: 500.0\n","epoch 70/100 return: 500.0\n","epoch 80/100 return: 500.0\n","epoch 90/100 return: 500.0\n","###############    Reward for test environment for run 1: 479.92.   ###############\n","\n","\n","Run 2 out of 10\n","state_dim 8\n","epoch 9000/10000, policy loss 0.06638316810131073\tepoch 0/100 return: 482.0\n","epoch 10/100 return: 500.0\n","epoch 20/100 return: 500.0\n","epoch 30/100 return: 500.0\n","epoch 40/100 return: 500.0\n","epoch 50/100 return: 500.0\n","epoch 60/100 return: 258.0\n","epoch 70/100 return: 500.0\n","epoch 80/100 return: 500.0\n","epoch 90/100 return: 418.0\n","###############    Reward for test environment for run 2: 387.54.   ###############\n","\n","\n","Run 3 out of 10\n","state_dim 8\n","epoch 9000/10000, policy loss 0.22512784600257874\tepoch 0/100 return: 153.0\n","epoch 10/100 return: 146.0\n","epoch 20/100 return: 169.0\n","epoch 30/100 return: 123.0\n","epoch 40/100 return: 104.0\n","epoch 50/100 return: 131.0\n","epoch 60/100 return: 163.0\n","epoch 70/100 return: 103.0\n","epoch 80/100 return: 111.0\n","epoch 90/100 return: 126.0\n","###############    Reward for test environment for run 3: 148.13.   ###############\n","\n","\n","Run 4 out of 10\n","state_dim 8\n","epoch 9000/10000, policy loss 0.09484401345252991\tepoch 0/100 return: 500.0\n","epoch 10/100 return: 500.0\n","epoch 20/100 return: 500.0\n","epoch 30/100 return: 500.0\n","epoch 40/100 return: 500.0\n","epoch 50/100 return: 500.0\n","epoch 60/100 return: 500.0\n","epoch 70/100 return: 500.0\n","epoch 80/100 return: 500.0\n","epoch 90/100 return: 500.0\n","###############    Reward for test environment for run 4: 500.0.   ###############\n","\n","\n","Run 5 out of 10\n","state_dim 8\n","epoch 9000/10000, policy loss 0.12127536535263062\tepoch 0/100 return: 500.0\n","epoch 10/100 return: 175.0\n","epoch 20/100 return: 474.0\n","epoch 30/100 return: 500.0\n","epoch 40/100 return: 500.0\n","epoch 50/100 return: 500.0\n","epoch 60/100 return: 500.0\n","epoch 70/100 return: 500.0\n","epoch 80/100 return: 212.0\n","epoch 90/100 return: 432.0\n","###############    Reward for test environment for run 5: 374.79.   ###############\n","\n","\n","Run 6 out of 10\n","state_dim 8\n","epoch 9000/10000, policy loss 0.14352881908416748\tepoch 0/100 return: 152.0\n","epoch 10/100 return: 168.0\n","epoch 20/100 return: 125.0\n","epoch 30/100 return: 115.0\n","epoch 40/100 return: 165.0\n","epoch 50/100 return: 227.0\n","epoch 60/100 return: 125.0\n","epoch 70/100 return: 151.0\n","epoch 80/100 return: 128.0\n","epoch 90/100 return: 148.0\n","###############    Reward for test environment for run 6: 154.34.   ###############\n","\n","\n","Run 7 out of 10\n","state_dim 8\n","epoch 9000/10000, policy loss 0.10754819214344025\tepoch 0/100 return: 500.0\n","epoch 10/100 return: 401.0\n","epoch 20/100 return: 397.0\n","epoch 30/100 return: 377.0\n","epoch 40/100 return: 500.0\n","epoch 50/100 return: 500.0\n","epoch 60/100 return: 490.0\n","epoch 70/100 return: 500.0\n","epoch 80/100 return: 500.0\n","epoch 90/100 return: 500.0\n","###############    Reward for test environment for run 7: 446.98.   ###############\n","\n","\n","Run 8 out of 10\n","state_dim 8\n","epoch 9000/10000, policy loss 0.05915716290473938\tepoch 0/100 return: 500.0\n","epoch 10/100 return: 500.0\n","epoch 20/100 return: 500.0\n","epoch 30/100 return: 500.0\n","epoch 40/100 return: 500.0\n","epoch 50/100 return: 500.0\n","epoch 60/100 return: 500.0\n","epoch 70/100 return: 500.0\n","epoch 80/100 return: 500.0\n","epoch 90/100 return: 500.0\n","###############    Reward for test environment for run 8: 500.0.   ###############\n","\n","\n","Run 9 out of 10\n","state_dim 8\n","epoch 9000/10000, policy loss 0.07390963286161423\tepoch 0/100 return: 500.0\n","epoch 10/100 return: 500.0\n","epoch 20/100 return: 213.0\n","epoch 30/100 return: 500.0\n","epoch 40/100 return: 451.0\n","epoch 50/100 return: 500.0\n","epoch 60/100 return: 500.0\n","epoch 70/100 return: 500.0\n","epoch 80/100 return: 500.0\n","epoch 90/100 return: 500.0\n","###############    Reward for test environment for run 9: 473.99.   ###############\n","\n","\n","Run 10 out of 10\n","state_dim 8\n","epoch 9000/10000, policy loss 0.11650539934635162\tepoch 0/100 return: 500.0\n","epoch 10/100 return: 500.0\n","epoch 20/100 return: 500.0\n","epoch 30/100 return: 500.0\n","epoch 40/100 return: 500.0\n","epoch 50/100 return: 500.0\n","epoch 60/100 return: 500.0\n","epoch 70/100 return: 500.0\n","epoch 80/100 return: 500.0\n","epoch 90/100 return: 500.0\n","###############    Reward for test environment for run 10: 482.48.   ###############\n","\n","\n","Average reward for 10 repetitions: 394.81700000000006\n","ALL RESULTS TRAIL: [394.81700000000006]\n","Config: {'ENV': 'CartPole-v1', 'ALG': 'FINAL_Apr24_BCStudent_noconfound_origindata_trajnum5', 'NUM_TRAJS_GIVEN': 5, 'NUM_TRAINING_ENVS': 2, 'NOISE_DIM': 4, 'REP_SIZE': 16, 'TRAJ_SHIFT': 5, 'SAMPLING_RATE': 5, 'NUM_STEPS_TRAIN': 10000, 'NUM_TRAJS_VALID': 100, 'NUM_REPETITIONS': 10, 'BATCH_SIZE': 64, 'MLP_WIDTHS': 64, 'ADAM_ALPHA': 0.001, 'SGLD_BUFFER_SIZE': 10000, 'SGLD_LEARN_RATE': 0.01, 'SGLD_NOISE_COEF': 0.01, 'SGLD_NUM_STEPS': 100, 'SGLD_REINIT_FREQ': 0.05, 'NUM_STEPS_TRAIN_ENERGY_MODEL': 1000, 'TRIAL': 0, 'METHOD': 'BC-noconfound', 'EXPERT_ALG': 'dqn'}\n","Trial number 0\n","config method =  BC-noconfound\n","config env =  CartPole-v1\n","Run 1 out of 10\n","state_dim 8\n","epoch 9000/10000, policy loss 0.31482619047164917\tepoch 0/100 return: 500.0\n","epoch 10/100 return: 500.0\n","epoch 20/100 return: 500.0\n","epoch 30/100 return: 500.0\n","epoch 40/100 return: 500.0\n","epoch 50/100 return: 500.0\n","epoch 60/100 return: 179.0\n","epoch 70/100 return: 500.0\n","epoch 80/100 return: 500.0\n","epoch 90/100 return: 500.0\n","###############    Reward for test environment for run 1: 401.32.   ###############\n","\n","\n","Run 2 out of 10\n","state_dim 8\n","epoch 9000/10000, policy loss 0.47091808915138245\tepoch 0/100 return: 500.0\n","epoch 10/100 return: 500.0\n","epoch 20/100 return: 500.0\n","epoch 30/100 return: 500.0\n","epoch 40/100 return: 500.0\n","epoch 50/100 return: 500.0\n","epoch 60/100 return: 500.0\n","epoch 70/100 return: 500.0\n","epoch 80/100 return: 500.0\n","epoch 90/100 return: 500.0\n","###############    Reward for test environment for run 2: 500.0.   ###############\n","\n","\n","Run 3 out of 10\n","state_dim 8\n","epoch 9000/10000, policy loss 0.3203516900539398\tepoch 0/100 return: 500.0\n","epoch 10/100 return: 386.0\n","epoch 20/100 return: 421.0\n","epoch 30/100 return: 424.0\n","epoch 40/100 return: 500.0\n","epoch 50/100 return: 500.0\n","epoch 60/100 return: 500.0\n","epoch 70/100 return: 500.0\n","epoch 80/100 return: 500.0\n","epoch 90/100 return: 500.0\n","###############    Reward for test environment for run 3: 483.56.   ###############\n","\n","\n","Run 4 out of 10\n","state_dim 8\n","epoch 9000/10000, policy loss 0.3674566149711609\tepoch 0/100 return: 500.0\n","epoch 10/100 return: 500.0\n","epoch 20/100 return: 500.0\n","epoch 30/100 return: 500.0\n","epoch 40/100 return: 500.0\n","epoch 50/100 return: 500.0\n","epoch 60/100 return: 500.0\n","epoch 70/100 return: 500.0\n","epoch 80/100 return: 500.0\n","epoch 90/100 return: 500.0\n","###############    Reward for test environment for run 4: 500.0.   ###############\n","\n","\n","Run 5 out of 10\n","state_dim 8\n","epoch 9000/10000, policy loss 0.4366397261619568\tepoch 0/100 return: 500.0\n","epoch 10/100 return: 500.0\n","epoch 20/100 return: 500.0\n","epoch 30/100 return: 500.0\n","epoch 40/100 return: 500.0\n","epoch 50/100 return: 500.0\n","epoch 60/100 return: 500.0\n","epoch 70/100 return: 500.0\n","epoch 80/100 return: 500.0\n","epoch 90/100 return: 500.0\n","###############    Reward for test environment for run 5: 500.0.   ###############\n","\n","\n","Run 6 out of 10\n","state_dim 8\n","epoch 9000/10000, policy loss 0.4249001145362854\tepoch 0/100 return: 500.0\n","epoch 10/100 return: 500.0\n","epoch 20/100 return: 500.0\n","epoch 30/100 return: 500.0\n","epoch 40/100 return: 500.0\n","epoch 50/100 return: 500.0\n","epoch 60/100 return: 500.0\n","epoch 70/100 return: 500.0\n","epoch 80/100 return: 500.0\n","epoch 90/100 return: 500.0\n","###############    Reward for test environment for run 6: 500.0.   ###############\n","\n","\n","Run 7 out of 10\n","state_dim 8\n","epoch 9000/10000, policy loss 0.2827189564704895\tepoch 0/100 return: 500.0\n","epoch 10/100 return: 391.0\n","epoch 20/100 return: 500.0\n","epoch 30/100 return: 500.0\n","epoch 40/100 return: 500.0\n","epoch 50/100 return: 500.0\n","epoch 60/100 return: 500.0\n","epoch 70/100 return: 500.0\n","epoch 80/100 return: 500.0\n","epoch 90/100 return: 500.0\n","###############    Reward for test environment for run 7: 466.57.   ###############\n","\n","\n","Run 8 out of 10\n","state_dim 8\n","epoch 9000/10000, policy loss 0.40604037046432495\tepoch 0/100 return: 500.0\n","epoch 10/100 return: 500.0\n","epoch 20/100 return: 500.0\n","epoch 30/100 return: 500.0\n","epoch 40/100 return: 500.0\n","epoch 50/100 return: 500.0\n","epoch 60/100 return: 500.0\n","epoch 70/100 return: 500.0\n","epoch 80/100 return: 500.0\n","epoch 90/100 return: 500.0\n","###############    Reward for test environment for run 8: 500.0.   ###############\n","\n","\n","Run 9 out of 10\n","state_dim 8\n","epoch 9000/10000, policy loss 0.3079661428928375\tepoch 0/100 return: 500.0\n","epoch 10/100 return: 500.0\n","epoch 20/100 return: 500.0\n","epoch 30/100 return: 500.0\n","epoch 40/100 return: 500.0\n","epoch 50/100 return: 500.0\n","epoch 60/100 return: 500.0\n","epoch 70/100 return: 500.0\n","epoch 80/100 return: 500.0\n","epoch 90/100 return: 500.0\n","###############    Reward for test environment for run 9: 500.0.   ###############\n","\n","\n","Run 10 out of 10\n","state_dim 8\n","epoch 9000/10000, policy loss 0.3749406337738037\tepoch 0/100 return: 500.0\n","epoch 10/100 return: 500.0\n","epoch 20/100 return: 500.0\n","epoch 30/100 return: 500.0\n","epoch 40/100 return: 500.0\n","epoch 50/100 return: 500.0\n","epoch 60/100 return: 500.0\n","epoch 70/100 return: 500.0\n","epoch 80/100 return: 500.0\n","epoch 90/100 return: 500.0\n","###############    Reward for test environment for run 10: 500.0.   ###############\n","\n","\n","Average reward for 10 repetitions: 485.145\n","ALL RESULTS TRAIL: [485.145]\n","Config: {'ENV': 'CartPole-v1', 'ALG': 'FINAL_Apr24_BCStudent_noconfound_origindata_trajnum10', 'NUM_TRAJS_GIVEN': 10, 'NUM_TRAINING_ENVS': 2, 'NOISE_DIM': 4, 'REP_SIZE': 16, 'TRAJ_SHIFT': 10, 'SAMPLING_RATE': 5, 'NUM_STEPS_TRAIN': 10000, 'NUM_TRAJS_VALID': 100, 'NUM_REPETITIONS': 10, 'BATCH_SIZE': 64, 'MLP_WIDTHS': 64, 'ADAM_ALPHA': 0.001, 'SGLD_BUFFER_SIZE': 10000, 'SGLD_LEARN_RATE': 0.01, 'SGLD_NOISE_COEF': 0.01, 'SGLD_NUM_STEPS': 100, 'SGLD_REINIT_FREQ': 0.05, 'NUM_STEPS_TRAIN_ENERGY_MODEL': 1000, 'TRIAL': 0, 'METHOD': 'BC-noconfound', 'EXPERT_ALG': 'dqn'}\n","Trial number 0\n","config method =  BC-noconfound\n","config env =  CartPole-v1\n","Run 1 out of 10\n","state_dim 8\n","epoch 9000/10000, policy loss 0.42144250869750977\tepoch 0/100 return: 500.0\n","epoch 10/100 return: 500.0\n","epoch 20/100 return: 500.0\n","epoch 30/100 return: 500.0\n","epoch 40/100 return: 500.0\n","epoch 50/100 return: 500.0\n","epoch 60/100 return: 500.0\n","epoch 70/100 return: 500.0\n","epoch 80/100 return: 500.0\n","epoch 90/100 return: 500.0\n","###############    Reward for test environment for run 1: 500.0.   ###############\n","\n","\n","Run 2 out of 10\n","state_dim 8\n","epoch 9000/10000, policy loss 0.4901166260242462\tepoch 0/100 return: 500.0\n","epoch 10/100 return: 500.0\n","epoch 20/100 return: 500.0\n","epoch 30/100 return: 500.0\n","epoch 40/100 return: 500.0\n","epoch 50/100 return: 500.0\n","epoch 60/100 return: 500.0\n","epoch 70/100 return: 500.0\n","epoch 80/100 return: 500.0\n","epoch 90/100 return: 500.0\n","###############    Reward for test environment for run 2: 500.0.   ###############\n","\n","\n","Run 3 out of 10\n","state_dim 8\n","epoch 9000/10000, policy loss 0.34115979075431824\tepoch 0/100 return: 500.0\n","epoch 10/100 return: 500.0\n","epoch 20/100 return: 500.0\n","epoch 30/100 return: 500.0\n","epoch 40/100 return: 500.0\n","epoch 50/100 return: 500.0\n","epoch 60/100 return: 500.0\n","epoch 70/100 return: 500.0\n","epoch 80/100 return: 500.0\n","epoch 90/100 return: 500.0\n","###############    Reward for test environment for run 3: 500.0.   ###############\n","\n","\n","Run 4 out of 10\n","state_dim 8\n","epoch 9000/10000, policy loss 0.39984947443008423\tepoch 0/100 return: 500.0\n","epoch 10/100 return: 500.0\n","epoch 20/100 return: 500.0\n","epoch 30/100 return: 500.0\n","epoch 40/100 return: 500.0\n","epoch 50/100 return: 500.0\n","epoch 60/100 return: 500.0\n","epoch 70/100 return: 500.0\n","epoch 80/100 return: 500.0\n","epoch 90/100 return: 500.0\n","###############    Reward for test environment for run 4: 500.0.   ###############\n","\n","\n","Run 5 out of 10\n","state_dim 8\n","epoch 9000/10000, policy loss 0.34972062706947327\tepoch 0/100 return: 500.0\n","epoch 10/100 return: 500.0\n","epoch 20/100 return: 500.0\n","epoch 30/100 return: 500.0\n","epoch 40/100 return: 500.0\n","epoch 50/100 return: 500.0\n","epoch 60/100 return: 500.0\n","epoch 70/100 return: 500.0\n","epoch 80/100 return: 500.0\n","epoch 90/100 return: 500.0\n","###############    Reward for test environment for run 5: 500.0.   ###############\n","\n","\n","Run 6 out of 10\n","state_dim 8\n","epoch 9000/10000, policy loss 0.33676761388778687\tepoch 0/100 return: 500.0\n","epoch 10/100 return: 500.0\n","epoch 20/100 return: 500.0\n","epoch 30/100 return: 500.0\n","epoch 40/100 return: 500.0\n","epoch 50/100 return: 500.0\n","epoch 60/100 return: 500.0\n","epoch 70/100 return: 500.0\n","epoch 80/100 return: 500.0\n","epoch 90/100 return: 500.0\n","###############    Reward for test environment for run 6: 500.0.   ###############\n","\n","\n","Run 7 out of 10\n","state_dim 8\n","epoch 9000/10000, policy loss 0.4288591742515564\tepoch 0/100 return: 500.0\n","epoch 10/100 return: 500.0\n","epoch 20/100 return: 500.0\n","epoch 30/100 return: 500.0\n","epoch 40/100 return: 500.0\n","epoch 50/100 return: 500.0\n","epoch 60/100 return: 500.0\n","epoch 70/100 return: 500.0\n","epoch 80/100 return: 500.0\n","epoch 90/100 return: 500.0\n","###############    Reward for test environment for run 7: 500.0.   ###############\n","\n","\n","Run 8 out of 10\n","state_dim 8\n","epoch 9000/10000, policy loss 0.40661659836769104\tepoch 0/100 return: 500.0\n","epoch 10/100 return: 500.0\n","epoch 20/100 return: 500.0\n","epoch 30/100 return: 500.0\n","epoch 40/100 return: 500.0\n","epoch 50/100 return: 500.0\n","epoch 60/100 return: 500.0\n","epoch 70/100 return: 500.0\n","epoch 80/100 return: 500.0\n","epoch 90/100 return: 500.0\n","###############    Reward for test environment for run 8: 500.0.   ###############\n","\n","\n","Run 9 out of 10\n","state_dim 8\n","epoch 9000/10000, policy loss 0.3909512758255005\tepoch 0/100 return: 500.0\n","epoch 10/100 return: 500.0\n","epoch 20/100 return: 500.0\n","epoch 30/100 return: 500.0\n","epoch 40/100 return: 500.0\n","epoch 50/100 return: 500.0\n","epoch 60/100 return: 500.0\n","epoch 70/100 return: 500.0\n","epoch 80/100 return: 500.0\n","epoch 90/100 return: 500.0\n","###############    Reward for test environment for run 9: 500.0.   ###############\n","\n","\n","Run 10 out of 10\n","state_dim 8\n","epoch 9000/10000, policy loss 0.38563230633735657\tepoch 0/100 return: 500.0\n","epoch 10/100 return: 500.0\n","epoch 20/100 return: 500.0\n","epoch 30/100 return: 500.0\n","epoch 40/100 return: 500.0\n","epoch 50/100 return: 500.0\n","epoch 60/100 return: 500.0\n","epoch 70/100 return: 500.0\n","epoch 80/100 return: 500.0\n","epoch 90/100 return: 500.0\n","###############    Reward for test environment for run 10: 500.0.   ###############\n","\n","\n","Average reward for 10 repetitions: 500.0\n","ALL RESULTS TRAIL: [500.0]\n","Config: {'ENV': 'CartPole-v1', 'ALG': 'FINAL_Apr24_BCStudent_noconfound_origindata_trajnum20', 'NUM_TRAJS_GIVEN': 20, 'NUM_TRAINING_ENVS': 2, 'NOISE_DIM': 4, 'REP_SIZE': 16, 'TRAJ_SHIFT': 20, 'SAMPLING_RATE': 5, 'NUM_STEPS_TRAIN': 10000, 'NUM_TRAJS_VALID': 100, 'NUM_REPETITIONS': 10, 'BATCH_SIZE': 64, 'MLP_WIDTHS': 64, 'ADAM_ALPHA': 0.001, 'SGLD_BUFFER_SIZE': 10000, 'SGLD_LEARN_RATE': 0.01, 'SGLD_NOISE_COEF': 0.01, 'SGLD_NUM_STEPS': 100, 'SGLD_REINIT_FREQ': 0.05, 'NUM_STEPS_TRAIN_ENERGY_MODEL': 1000, 'TRIAL': 0, 'METHOD': 'BC-noconfound', 'EXPERT_ALG': 'dqn'}\n","Trial number 0\n","config method =  BC-noconfound\n","config env =  CartPole-v1\n","Run 1 out of 10\n","state_dim 8\n","epoch 9000/10000, policy loss 0.42650702595710754\tepoch 0/100 return: 500.0\n","epoch 10/100 return: 500.0\n","epoch 20/100 return: 500.0\n","epoch 30/100 return: 500.0\n","epoch 40/100 return: 500.0\n","epoch 50/100 return: 500.0\n","epoch 60/100 return: 500.0\n","epoch 70/100 return: 500.0\n","epoch 80/100 return: 500.0\n","epoch 90/100 return: 500.0\n","###############    Reward for test environment for run 1: 500.0.   ###############\n","\n","\n","Run 2 out of 10\n","state_dim 8\n","epoch 9000/10000, policy loss 0.29012107849121094\tepoch 0/100 return: 500.0\n","epoch 10/100 return: 500.0\n","epoch 20/100 return: 500.0\n","epoch 30/100 return: 500.0\n","epoch 40/100 return: 500.0\n","epoch 50/100 return: 500.0\n","epoch 60/100 return: 500.0\n","epoch 70/100 return: 500.0\n","epoch 80/100 return: 500.0\n","epoch 90/100 return: 500.0\n","###############    Reward for test environment for run 2: 500.0.   ###############\n","\n","\n","Run 3 out of 10\n","state_dim 8\n","epoch 9000/10000, policy loss 0.4149119555950165\tepoch 0/100 return: 500.0\n","epoch 10/100 return: 500.0\n","epoch 20/100 return: 500.0\n","epoch 30/100 return: 500.0\n","epoch 40/100 return: 500.0\n","epoch 50/100 return: 500.0\n","epoch 60/100 return: 500.0\n","epoch 70/100 return: 500.0\n","epoch 80/100 return: 500.0\n","epoch 90/100 return: 500.0\n","###############    Reward for test environment for run 3: 500.0.   ###############\n","\n","\n","Run 4 out of 10\n","state_dim 8\n","epoch 9000/10000, policy loss 0.4031614661216736\tepoch 0/100 return: 500.0\n","epoch 10/100 return: 500.0\n","epoch 20/100 return: 500.0\n","epoch 30/100 return: 500.0\n","epoch 40/100 return: 500.0\n","epoch 50/100 return: 500.0\n","epoch 60/100 return: 500.0\n","epoch 70/100 return: 500.0\n","epoch 80/100 return: 500.0\n","epoch 90/100 return: 500.0\n","###############    Reward for test environment for run 4: 500.0.   ###############\n","\n","\n","Run 5 out of 10\n","state_dim 8\n","epoch 9000/10000, policy loss 0.3941657245159149\tepoch 0/100 return: 500.0\n","epoch 10/100 return: 500.0\n","epoch 20/100 return: 500.0\n","epoch 30/100 return: 500.0\n","epoch 40/100 return: 500.0\n","epoch 50/100 return: 500.0\n","epoch 60/100 return: 500.0\n","epoch 70/100 return: 500.0\n","epoch 80/100 return: 500.0\n","epoch 90/100 return: 500.0\n","###############    Reward for test environment for run 5: 500.0.   ###############\n","\n","\n","Run 6 out of 10\n","state_dim 8\n","epoch 9000/10000, policy loss 0.3737599551677704\tepoch 0/100 return: 500.0\n","epoch 10/100 return: 500.0\n","epoch 20/100 return: 500.0\n","epoch 30/100 return: 500.0\n","epoch 40/100 return: 500.0\n","epoch 50/100 return: 500.0\n","epoch 60/100 return: 500.0\n","epoch 70/100 return: 500.0\n","epoch 80/100 return: 500.0\n","epoch 90/100 return: 500.0\n","###############    Reward for test environment for run 6: 500.0.   ###############\n","\n","\n","Run 7 out of 10\n","state_dim 8\n","epoch 9000/10000, policy loss 0.45116767287254333\tepoch 0/100 return: 500.0\n","epoch 10/100 return: 500.0\n","epoch 20/100 return: 500.0\n","epoch 30/100 return: 500.0\n","epoch 40/100 return: 500.0\n","epoch 50/100 return: 500.0\n","epoch 60/100 return: 500.0\n","epoch 70/100 return: 500.0\n","epoch 80/100 return: 500.0\n","epoch 90/100 return: 500.0\n","###############    Reward for test environment for run 7: 500.0.   ###############\n","\n","\n","Run 8 out of 10\n","state_dim 8\n","epoch 9000/10000, policy loss 0.36746567487716675\tepoch 0/100 return: 500.0\n","epoch 10/100 return: 500.0\n","epoch 20/100 return: 500.0\n","epoch 30/100 return: 500.0\n","epoch 40/100 return: 500.0\n","epoch 50/100 return: 500.0\n","epoch 60/100 return: 500.0\n","epoch 70/100 return: 500.0\n","epoch 80/100 return: 500.0\n","epoch 90/100 return: 500.0\n","###############    Reward for test environment for run 8: 500.0.   ###############\n","\n","\n","Run 9 out of 10\n","state_dim 8\n","epoch 9000/10000, policy loss 0.39149346947669983\tepoch 0/100 return: 500.0\n","epoch 10/100 return: 500.0\n","epoch 20/100 return: 500.0\n","epoch 30/100 return: 500.0\n","epoch 40/100 return: 500.0\n","epoch 50/100 return: 500.0\n","epoch 60/100 return: 500.0\n","epoch 70/100 return: 500.0\n","epoch 80/100 return: 500.0\n","epoch 90/100 return: 500.0\n","###############    Reward for test environment for run 9: 500.0.   ###############\n","\n","\n","Run 10 out of 10\n","state_dim 8\n","epoch 9000/10000, policy loss 0.4497320055961609\tepoch 0/100 return: 500.0\n","epoch 10/100 return: 500.0\n","epoch 20/100 return: 500.0\n","epoch 30/100 return: 500.0\n","epoch 40/100 return: 500.0\n","epoch 50/100 return: 500.0\n","epoch 60/100 return: 500.0\n","epoch 70/100 return: 500.0\n","epoch 80/100 return: 500.0\n","epoch 90/100 return: 500.0\n","###############    Reward for test environment for run 10: 500.0.   ###############\n","\n","\n","Average reward for 10 repetitions: 500.0\n","ALL RESULTS TRAIL: [500.0]\n","Config: {'ENV': 'CartPole-v1', 'ALG': 'FINAL_Apr24_BCStudent_noconfound_origindata_trajnum30', 'NUM_TRAJS_GIVEN': 30, 'NUM_TRAINING_ENVS': 2, 'NOISE_DIM': 4, 'REP_SIZE': 16, 'TRAJ_SHIFT': 30, 'SAMPLING_RATE': 5, 'NUM_STEPS_TRAIN': 10000, 'NUM_TRAJS_VALID': 100, 'NUM_REPETITIONS': 10, 'BATCH_SIZE': 64, 'MLP_WIDTHS': 64, 'ADAM_ALPHA': 0.001, 'SGLD_BUFFER_SIZE': 10000, 'SGLD_LEARN_RATE': 0.01, 'SGLD_NOISE_COEF': 0.01, 'SGLD_NUM_STEPS': 100, 'SGLD_REINIT_FREQ': 0.05, 'NUM_STEPS_TRAIN_ENERGY_MODEL': 1000, 'TRIAL': 0, 'METHOD': 'BC-noconfound', 'EXPERT_ALG': 'dqn'}\n","Trial number 0\n","config method =  BC-noconfound\n","config env =  CartPole-v1\n","Run 1 out of 10\n","state_dim 8\n","epoch 9000/10000, policy loss 0.38404232263565063\tepoch 0/100 return: 500.0\n","epoch 10/100 return: 500.0\n","epoch 20/100 return: 500.0\n","epoch 30/100 return: 500.0\n","epoch 40/100 return: 500.0\n","epoch 50/100 return: 500.0\n","epoch 60/100 return: 500.0\n","epoch 70/100 return: 500.0\n","epoch 80/100 return: 500.0\n","epoch 90/100 return: 500.0\n","###############    Reward for test environment for run 1: 500.0.   ###############\n","\n","\n","Run 2 out of 10\n","state_dim 8\n","epoch 9000/10000, policy loss 0.30095458030700684\tepoch 0/100 return: 500.0\n","epoch 10/100 return: 500.0\n","epoch 20/100 return: 500.0\n","epoch 30/100 return: 500.0\n","epoch 40/100 return: 500.0\n","epoch 50/100 return: 500.0\n","epoch 60/100 return: 500.0\n","epoch 70/100 return: 500.0\n","epoch 80/100 return: 500.0\n","epoch 90/100 return: 500.0\n","###############    Reward for test environment for run 2: 500.0.   ###############\n","\n","\n","Run 3 out of 10\n","state_dim 8\n","epoch 9000/10000, policy loss 0.30918797850608826\tepoch 0/100 return: 500.0\n","epoch 10/100 return: 500.0\n","epoch 20/100 return: 500.0\n","epoch 30/100 return: 500.0\n","epoch 40/100 return: 500.0\n","epoch 50/100 return: 500.0\n","epoch 60/100 return: 500.0\n","epoch 70/100 return: 500.0\n","epoch 80/100 return: 500.0\n","epoch 90/100 return: 500.0\n","###############    Reward for test environment for run 3: 500.0.   ###############\n","\n","\n","Run 4 out of 10\n","state_dim 8\n","epoch 9000/10000, policy loss 0.415420800447464\tepoch 0/100 return: 500.0\n","epoch 10/100 return: 500.0\n","epoch 20/100 return: 500.0\n","epoch 30/100 return: 500.0\n","epoch 40/100 return: 500.0\n","epoch 50/100 return: 500.0\n","epoch 60/100 return: 500.0\n","epoch 70/100 return: 500.0\n","epoch 80/100 return: 500.0\n","epoch 90/100 return: 500.0\n","###############    Reward for test environment for run 4: 500.0.   ###############\n","\n","\n","Run 5 out of 10\n","state_dim 8\n","epoch 9000/10000, policy loss 0.4053388833999634\tepoch 0/100 return: 500.0\n","epoch 10/100 return: 500.0\n","epoch 20/100 return: 500.0\n","epoch 30/100 return: 500.0\n","epoch 40/100 return: 500.0\n","epoch 50/100 return: 500.0\n","epoch 60/100 return: 500.0\n","epoch 70/100 return: 500.0\n","epoch 80/100 return: 500.0\n","epoch 90/100 return: 500.0\n","###############    Reward for test environment for run 5: 500.0.   ###############\n","\n","\n","Run 6 out of 10\n","state_dim 8\n","epoch 9000/10000, policy loss 0.27530670166015625\tepoch 0/100 return: 500.0\n","epoch 10/100 return: 500.0\n","epoch 20/100 return: 500.0\n","epoch 30/100 return: 500.0\n","epoch 40/100 return: 500.0\n","epoch 50/100 return: 500.0\n","epoch 60/100 return: 500.0\n","epoch 70/100 return: 500.0\n","epoch 80/100 return: 500.0\n","epoch 90/100 return: 500.0\n","###############    Reward for test environment for run 6: 500.0.   ###############\n","\n","\n","Run 7 out of 10\n","state_dim 8\n","epoch 9000/10000, policy loss 0.4109592139720917\tepoch 0/100 return: 500.0\n","epoch 10/100 return: 500.0\n","epoch 20/100 return: 500.0\n","epoch 30/100 return: 500.0\n","epoch 40/100 return: 500.0\n","epoch 50/100 return: 500.0\n","epoch 60/100 return: 500.0\n","epoch 70/100 return: 500.0\n","epoch 80/100 return: 500.0\n","epoch 90/100 return: 500.0\n","###############    Reward for test environment for run 7: 500.0.   ###############\n","\n","\n","Run 8 out of 10\n","state_dim 8\n","epoch 9000/10000, policy loss 0.3405308723449707\tepoch 0/100 return: 500.0\n","epoch 10/100 return: 500.0\n","epoch 20/100 return: 500.0\n","epoch 30/100 return: 500.0\n","epoch 40/100 return: 500.0\n","epoch 50/100 return: 500.0\n","epoch 60/100 return: 500.0\n","epoch 70/100 return: 500.0\n","epoch 80/100 return: 500.0\n","epoch 90/100 return: 500.0\n","###############    Reward for test environment for run 8: 500.0.   ###############\n","\n","\n","Run 9 out of 10\n","state_dim 8\n","epoch 9000/10000, policy loss 0.40146028995513916\tepoch 0/100 return: 500.0\n","epoch 10/100 return: 500.0\n","epoch 20/100 return: 500.0\n","epoch 30/100 return: 500.0\n","epoch 40/100 return: 500.0\n","epoch 50/100 return: 500.0\n","epoch 60/100 return: 500.0\n","epoch 70/100 return: 500.0\n","epoch 80/100 return: 500.0\n","epoch 90/100 return: 500.0\n","###############    Reward for test environment for run 9: 500.0.   ###############\n","\n","\n","Run 10 out of 10\n","state_dim 8\n","epoch 9000/10000, policy loss 0.36918386816978455\tepoch 0/100 return: 500.0\n","epoch 10/100 return: 500.0\n","epoch 20/100 return: 500.0\n","epoch 30/100 return: 500.0\n","epoch 40/100 return: 500.0\n","epoch 50/100 return: 500.0\n","epoch 60/100 return: 500.0\n","epoch 70/100 return: 500.0\n","epoch 80/100 return: 500.0\n","epoch 90/100 return: 500.0\n","###############    Reward for test environment for run 10: 500.0.   ###############\n","\n","\n","Average reward for 10 repetitions: 500.0\n","ALL RESULTS TRAIL: [500.0]\n","Config: {'ENV': 'CartPole-v1', 'ALG': 'FINAL_Apr24_BCStudent_noconfound_origindata_trajnum40', 'NUM_TRAJS_GIVEN': 40, 'NUM_TRAINING_ENVS': 2, 'NOISE_DIM': 4, 'REP_SIZE': 16, 'TRAJ_SHIFT': 40, 'SAMPLING_RATE': 5, 'NUM_STEPS_TRAIN': 10000, 'NUM_TRAJS_VALID': 100, 'NUM_REPETITIONS': 10, 'BATCH_SIZE': 64, 'MLP_WIDTHS': 64, 'ADAM_ALPHA': 0.001, 'SGLD_BUFFER_SIZE': 10000, 'SGLD_LEARN_RATE': 0.01, 'SGLD_NOISE_COEF': 0.01, 'SGLD_NUM_STEPS': 100, 'SGLD_REINIT_FREQ': 0.05, 'NUM_STEPS_TRAIN_ENERGY_MODEL': 1000, 'TRIAL': 0, 'METHOD': 'BC-noconfound', 'EXPERT_ALG': 'dqn'}\n","Trial number 0\n","config method =  BC-noconfound\n","config env =  CartPole-v1\n","Run 1 out of 10\n","state_dim 8\n","epoch 9000/10000, policy loss 0.520821213722229\tepoch 0/100 return: 500.0\n","epoch 10/100 return: 500.0\n","epoch 20/100 return: 500.0\n","epoch 30/100 return: 500.0\n","epoch 40/100 return: 500.0\n","epoch 50/100 return: 500.0\n","epoch 60/100 return: 500.0\n","epoch 70/100 return: 500.0\n","epoch 80/100 return: 500.0\n","epoch 90/100 return: 500.0\n","###############    Reward for test environment for run 1: 500.0.   ###############\n","\n","\n","Run 2 out of 10\n","state_dim 8\n","epoch 9000/10000, policy loss 0.42408686876296997\tepoch 0/100 return: 500.0\n","epoch 10/100 return: 500.0\n","epoch 20/100 return: 500.0\n","epoch 30/100 return: 500.0\n","epoch 40/100 return: 500.0\n","epoch 50/100 return: 500.0\n","epoch 60/100 return: 500.0\n","epoch 70/100 return: 500.0\n","epoch 80/100 return: 500.0\n","epoch 90/100 return: 500.0\n","###############    Reward for test environment for run 2: 500.0.   ###############\n","\n","\n","Run 3 out of 10\n","state_dim 8\n","epoch 9000/10000, policy loss 0.33353304862976074\tepoch 0/100 return: 500.0\n","epoch 10/100 return: 500.0\n","epoch 20/100 return: 500.0\n","epoch 30/100 return: 500.0\n","epoch 40/100 return: 500.0\n","epoch 50/100 return: 500.0\n","epoch 60/100 return: 500.0\n","epoch 70/100 return: 500.0\n","epoch 80/100 return: 500.0\n","epoch 90/100 return: 500.0\n","###############    Reward for test environment for run 3: 500.0.   ###############\n","\n","\n","Run 4 out of 10\n","state_dim 8\n","epoch 9000/10000, policy loss 0.46728044748306274\tepoch 0/100 return: 500.0\n","epoch 10/100 return: 500.0\n","epoch 20/100 return: 500.0\n","epoch 30/100 return: 500.0\n","epoch 40/100 return: 500.0\n","epoch 50/100 return: 500.0\n","epoch 60/100 return: 500.0\n","epoch 70/100 return: 500.0\n","epoch 80/100 return: 500.0\n","epoch 90/100 return: 500.0\n","###############    Reward for test environment for run 4: 500.0.   ###############\n","\n","\n","Run 5 out of 10\n","state_dim 8\n","epoch 9000/10000, policy loss 0.4081452190876007\tepoch 0/100 return: 500.0\n","epoch 10/100 return: 500.0\n","epoch 20/100 return: 500.0\n","epoch 30/100 return: 500.0\n","epoch 40/100 return: 500.0\n","epoch 50/100 return: 500.0\n","epoch 60/100 return: 500.0\n","epoch 70/100 return: 500.0\n","epoch 80/100 return: 500.0\n","epoch 90/100 return: 500.0\n","###############    Reward for test environment for run 5: 500.0.   ###############\n","\n","\n","Run 6 out of 10\n","state_dim 8\n","epoch 9000/10000, policy loss 0.4634763300418854\tepoch 0/100 return: 500.0\n","epoch 10/100 return: 500.0\n","epoch 20/100 return: 500.0\n","epoch 30/100 return: 500.0\n","epoch 40/100 return: 500.0\n","epoch 50/100 return: 500.0\n","epoch 60/100 return: 500.0\n","epoch 70/100 return: 500.0\n","epoch 80/100 return: 500.0\n","epoch 90/100 return: 500.0\n","###############    Reward for test environment for run 6: 500.0.   ###############\n","\n","\n","Run 7 out of 10\n","state_dim 8\n","epoch 9000/10000, policy loss 0.2500438690185547\tepoch 0/100 return: 500.0\n","epoch 10/100 return: 500.0\n","epoch 20/100 return: 500.0\n","epoch 30/100 return: 500.0\n","epoch 40/100 return: 500.0\n","epoch 50/100 return: 500.0\n","epoch 60/100 return: 500.0\n","epoch 70/100 return: 500.0\n","epoch 80/100 return: 500.0\n","epoch 90/100 return: 500.0\n","###############    Reward for test environment for run 7: 500.0.   ###############\n","\n","\n","Run 8 out of 10\n","state_dim 8\n","epoch 9000/10000, policy loss 0.36203062534332275\tepoch 0/100 return: 500.0\n","epoch 10/100 return: 500.0\n","epoch 20/100 return: 500.0\n","epoch 30/100 return: 500.0\n","epoch 40/100 return: 500.0\n","epoch 50/100 return: 500.0\n","epoch 60/100 return: 500.0\n","epoch 70/100 return: 500.0\n","epoch 80/100 return: 500.0\n","epoch 90/100 return: 500.0\n","###############    Reward for test environment for run 8: 500.0.   ###############\n","\n","\n","Run 9 out of 10\n","state_dim 8\n","epoch 9000/10000, policy loss 0.46970799565315247\tepoch 0/100 return: 500.0\n","epoch 10/100 return: 500.0\n","epoch 20/100 return: 500.0\n","epoch 30/100 return: 500.0\n","epoch 40/100 return: 500.0\n","epoch 50/100 return: 500.0\n","epoch 60/100 return: 500.0\n","epoch 70/100 return: 500.0\n","epoch 80/100 return: 500.0\n","epoch 90/100 return: 500.0\n","###############    Reward for test environment for run 9: 500.0.   ###############\n","\n","\n","Run 10 out of 10\n","state_dim 8\n","epoch 9000/10000, policy loss 0.34435978531837463\tepoch 0/100 return: 500.0\n","epoch 10/100 return: 500.0\n","epoch 20/100 return: 500.0\n","epoch 30/100 return: 500.0\n","epoch 40/100 return: 500.0\n","epoch 50/100 return: 500.0\n","epoch 60/100 return: 500.0\n","epoch 70/100 return: 500.0\n","epoch 80/100 return: 500.0\n","epoch 90/100 return: 500.0\n","###############    Reward for test environment for run 10: 500.0.   ###############\n","\n","\n","Average reward for 10 repetitions: 500.0\n","ALL RESULTS TRAIL: [500.0]\n","Config: {'ENV': 'CartPole-v1', 'ALG': 'FINAL_Apr24_BCStudent_noconfound_origindata_trajnum50', 'NUM_TRAJS_GIVEN': 50, 'NUM_TRAINING_ENVS': 2, 'NOISE_DIM': 4, 'REP_SIZE': 16, 'TRAJ_SHIFT': 50, 'SAMPLING_RATE': 5, 'NUM_STEPS_TRAIN': 10000, 'NUM_TRAJS_VALID': 100, 'NUM_REPETITIONS': 10, 'BATCH_SIZE': 64, 'MLP_WIDTHS': 64, 'ADAM_ALPHA': 0.001, 'SGLD_BUFFER_SIZE': 10000, 'SGLD_LEARN_RATE': 0.01, 'SGLD_NOISE_COEF': 0.01, 'SGLD_NUM_STEPS': 100, 'SGLD_REINIT_FREQ': 0.05, 'NUM_STEPS_TRAIN_ENERGY_MODEL': 1000, 'TRIAL': 0, 'METHOD': 'BC-noconfound', 'EXPERT_ALG': 'dqn'}\n","Trial number 0\n","config method =  BC-noconfound\n","config env =  CartPole-v1\n","Run 1 out of 10\n","state_dim 8\n","epoch 9000/10000, policy loss 0.42698347568511963\tepoch 0/100 return: 500.0\n","epoch 10/100 return: 500.0\n","epoch 20/100 return: 500.0\n","epoch 30/100 return: 500.0\n","epoch 40/100 return: 500.0\n","epoch 50/100 return: 500.0\n","epoch 60/100 return: 500.0\n","epoch 70/100 return: 500.0\n","epoch 80/100 return: 500.0\n","epoch 90/100 return: 500.0\n","###############    Reward for test environment for run 1: 500.0.   ###############\n","\n","\n","Run 2 out of 10\n","state_dim 8\n","epoch 9000/10000, policy loss 0.4151482880115509\tepoch 0/100 return: 500.0\n","epoch 10/100 return: 500.0\n","epoch 20/100 return: 500.0\n","epoch 30/100 return: 500.0\n","epoch 40/100 return: 500.0\n","epoch 50/100 return: 500.0\n","epoch 60/100 return: 500.0\n","epoch 70/100 return: 500.0\n","epoch 80/100 return: 500.0\n","epoch 90/100 return: 500.0\n","###############    Reward for test environment for run 2: 500.0.   ###############\n","\n","\n","Run 3 out of 10\n","state_dim 8\n","epoch 9000/10000, policy loss 0.49712687730789185\tepoch 0/100 return: 500.0\n","epoch 10/100 return: 500.0\n","epoch 20/100 return: 500.0\n","epoch 30/100 return: 500.0\n","epoch 40/100 return: 500.0\n","epoch 50/100 return: 500.0\n","epoch 60/100 return: 500.0\n","epoch 70/100 return: 500.0\n","epoch 80/100 return: 500.0\n","epoch 90/100 return: 500.0\n","###############    Reward for test environment for run 3: 500.0.   ###############\n","\n","\n","Run 4 out of 10\n","state_dim 8\n","epoch 9000/10000, policy loss 0.5344114303588867\tepoch 0/100 return: 500.0\n","epoch 10/100 return: 500.0\n","epoch 20/100 return: 500.0\n","epoch 30/100 return: 500.0\n","epoch 40/100 return: 500.0\n","epoch 50/100 return: 500.0\n","epoch 60/100 return: 500.0\n","epoch 70/100 return: 500.0\n","epoch 80/100 return: 500.0\n","epoch 90/100 return: 500.0\n","###############    Reward for test environment for run 4: 500.0.   ###############\n","\n","\n","Run 5 out of 10\n","state_dim 8\n","epoch 9000/10000, policy loss 0.3852671682834625\tepoch 0/100 return: 500.0\n","epoch 10/100 return: 500.0\n","epoch 20/100 return: 500.0\n","epoch 30/100 return: 500.0\n","epoch 40/100 return: 500.0\n","epoch 50/100 return: 500.0\n","epoch 60/100 return: 500.0\n","epoch 70/100 return: 500.0\n","epoch 80/100 return: 500.0\n","epoch 90/100 return: 500.0\n","###############    Reward for test environment for run 5: 500.0.   ###############\n","\n","\n","Run 6 out of 10\n","state_dim 8\n","epoch 9000/10000, policy loss 0.36715078353881836\tepoch 0/100 return: 500.0\n","epoch 10/100 return: 500.0\n","epoch 20/100 return: 500.0\n","epoch 30/100 return: 500.0\n","epoch 40/100 return: 500.0\n","epoch 50/100 return: 500.0\n","epoch 60/100 return: 500.0\n","epoch 70/100 return: 500.0\n","epoch 80/100 return: 500.0\n","epoch 90/100 return: 500.0\n","###############    Reward for test environment for run 6: 500.0.   ###############\n","\n","\n","Run 7 out of 10\n","state_dim 8\n","epoch 9000/10000, policy loss 0.3647138476371765\tepoch 0/100 return: 500.0\n","epoch 10/100 return: 500.0\n","epoch 20/100 return: 500.0\n","epoch 30/100 return: 500.0\n","epoch 40/100 return: 500.0\n","epoch 50/100 return: 500.0\n","epoch 60/100 return: 500.0\n","epoch 70/100 return: 500.0\n","epoch 80/100 return: 500.0\n","epoch 90/100 return: 500.0\n","###############    Reward for test environment for run 7: 500.0.   ###############\n","\n","\n","Run 8 out of 10\n","state_dim 8\n","epoch 9000/10000, policy loss 0.35165050625801086\tepoch 0/100 return: 500.0\n","epoch 10/100 return: 500.0\n","epoch 20/100 return: 500.0\n","epoch 30/100 return: 500.0\n","epoch 40/100 return: 500.0\n","epoch 50/100 return: 500.0\n","epoch 60/100 return: 500.0\n","epoch 70/100 return: 500.0\n","epoch 80/100 return: 500.0\n","epoch 90/100 return: 500.0\n","###############    Reward for test environment for run 8: 500.0.   ###############\n","\n","\n","Run 9 out of 10\n","state_dim 8\n","epoch 9000/10000, policy loss 0.39114493131637573\tepoch 0/100 return: 500.0\n","epoch 10/100 return: 500.0\n","epoch 20/100 return: 500.0\n","epoch 30/100 return: 500.0\n","epoch 40/100 return: 500.0\n","epoch 50/100 return: 500.0\n","epoch 60/100 return: 500.0\n","epoch 70/100 return: 500.0\n","epoch 80/100 return: 500.0\n","epoch 90/100 return: 500.0\n","###############    Reward for test environment for run 9: 500.0.   ###############\n","\n","\n","Run 10 out of 10\n","state_dim 8\n","epoch 9000/10000, policy loss 0.5678296089172363\tepoch 0/100 return: 500.0\n","epoch 10/100 return: 500.0\n","epoch 20/100 return: 500.0\n","epoch 30/100 return: 500.0\n","epoch 40/100 return: 500.0\n","epoch 50/100 return: 500.0\n","epoch 60/100 return: 500.0\n","epoch 70/100 return: 500.0\n","epoch 80/100 return: 500.0\n","epoch 90/100 return: 500.0\n","###############    Reward for test environment for run 10: 500.0.   ###############\n","\n","\n","Average reward for 10 repetitions: 500.0\n","ALL RESULTS TRAIL: [500.0]\n"]}]},{"cell_type":"code","source":[""],"metadata":{"id":"8EsDN4t12vnB"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["#10 Trails -- BC -- acrobot"],"metadata":{"id":"66KPbPKOony4"}},{"cell_type":"code","source":["config['METHOD'] = \"BC-noconfound\"\n","config['ENV'] = 'Acrobot-v1'\n","\n","for traj_num in [1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 20]:\n","    config[\"NUM_TRAJS_GIVEN\"] = traj_num\n","    config[\"TRAJ_SHIFT\"] = traj_num\n","\n","\n","    config['ALG'] = \"FINAL_Apr24_BCStudent_noconfound_origindata_trajnum\" + str(traj_num)\n","\n","\n","    ###############.  settings   ###############\n","    #config['ALG'] = \"BCStudent_Apr19_replicatedata\"\n","    #config['METHOD'] = \"BC\"\n","    #config['ENV'] == \"CartPole-v1\"\n","    #config['ENV'] == \"LunarLander-v2\"\n","    #config[\"NUM_TRAJS_GIVEN\"] = 20\n","    #config[\"TRAJ_SHIFT\"] = 20\n","    ###############.  settings   ###############\n","\n","\n","\n","    if config['METHOD'] == 'BCIRM':\n","        config['l2_regularizer_weight'] = 0.001\n","        config['penalty_weight'] = 10000\n","        config['penalty_anneal_iters'] = 100\n","\n","    all_results_trail = []\n","\n","    for trail in range(1): \n","        config['TRIAL'] = trail \n","\n","\n","        ###############.  start a trail   ###############\n","\n","        config[\"EXPERT_ALG\"] = yaml.load(open(\"testing/config.yml\"), Loader=yaml.FullLoader)[config[\"ENV\"]]\n","        print(\"Config: %s\" % config)\n","\n","        TRIAL = config[\"TRIAL\"] #args.trial\n","        print(\"Trial number %s\" % TRIAL)\n","\n","        results_dir_base = \"testing/results/\"\n","        results_dir = os.path.join(results_dir_base, config[\"ENV\"], str(config[\"NUM_TRAJS_GIVEN\"]), config[\"ALG\"])\n","\n","        if not os.path.exists(results_dir):\n","            os.makedirs(results_dir)\n","\n","        config_file = \"trial_\" + str(TRIAL) + \"_\" + \"config.pkl\"\n","\n","        results_file_name = \"trial_\" + str(TRIAL) + \"_\" + \"results.csv\"\n","        results_file_path = os.path.join(results_dir, results_file_name)\n","\n","        if os.path.exists(os.path.join(results_dir, config_file)):\n","            raise NameError(\"CONFIG file already exists %s. Choose a different trial number.\" % config_file)\n","        pickle.dump(config, open(os.path.join(results_dir, config_file), \"wb\"))\n","\n","\n","\n","\n","        ###############.  10 runs for each trail   ###############\n","\n","        print(\"config method = \", config['METHOD'])\n","        print(\"config env = \", config['ENV'])\n","\n","        for run_seed in range(config[\"NUM_REPETITIONS\"]):\n","            print(\"Run %s out of %s\" % (run_seed + 1, config[\"NUM_REPETITIONS\"]))\n","            student = make_student(run_seed, config)\n","            student.train(num_updates=config[\"NUM_STEPS_TRAIN\"])\n","\n","            env_wrapper_out_of_sample = EnvWrapper(\n","                env=gym.make(config[\"ENV\"]), mult_factor=get_test_mult_factors(config['NOISE_DIM'] - 1), idx=3, seed=1\n","            )\n","            action_match, return_mean, return_std = student.test(\n","                num_episodes=config[\"NUM_TRAJS_VALID\"], env_wrapper=env_wrapper_out_of_sample\n","            )\n","\n","            result = (action_match, return_mean, return_std)\n","            print(\"###############    Reward for test environment for run %s: %s.   ###############\\n\\n\" % (run_seed + 1, return_mean))\n","            save_results(results_file_path, run_seed, action_match, return_mean, return_std)\n","\n","        results_trial = pd.read_csv(\n","            \"testing/results/\"\n","            + config[\"ENV\"]\n","            + \"/\"\n","            + str(config[\"NUM_TRAJS_GIVEN\"])\n","            + \"/\"\n","            + config[\"ALG\"]\n","            + \"/trial_\"\n","            + str(TRIAL)\n","            + \"_results.csv\",\n","            header=None,\n","        )\n","\n","        print(\"Average reward for 10 repetitions: %s\" % np.mean(results_trial[2].values))\n","\n","        all_results_trail.append(np.mean(results_trial[2].values))\n","\n","    print(\"ALL RESULTS TRAIL:\" , all_results_trail)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"outputId":"5347e07c-fa11-4349-d47d-151f45953877","id":"LtppeiX3ony5","executionInfo":{"status":"ok","timestamp":1650838539380,"user_tz":240,"elapsed":4029222,"user":{"displayName":"2 Lin","userId":"15119774230930098070"}}},"execution_count":19,"outputs":[{"output_type":"stream","name":"stdout","text":["Config: {'ENV': 'Acrobot-v1', 'ALG': 'FINAL_Apr24_BCStudent_noconfound_origindata_trajnum1', 'NUM_TRAJS_GIVEN': 1, 'NUM_TRAINING_ENVS': 2, 'NOISE_DIM': 4, 'REP_SIZE': 16, 'TRAJ_SHIFT': 1, 'SAMPLING_RATE': 5, 'NUM_STEPS_TRAIN': 10000, 'NUM_TRAJS_VALID': 100, 'NUM_REPETITIONS': 10, 'BATCH_SIZE': 64, 'MLP_WIDTHS': 64, 'ADAM_ALPHA': 0.001, 'SGLD_BUFFER_SIZE': 10000, 'SGLD_LEARN_RATE': 0.01, 'SGLD_NOISE_COEF': 0.01, 'SGLD_NUM_STEPS': 100, 'SGLD_REINIT_FREQ': 0.05, 'NUM_STEPS_TRAIN_ENERGY_MODEL': 1000, 'TRIAL': 0, 'METHOD': 'BC-noconfound', 'EXPERT_ALG': 'dqn'}\n","Trial number 0\n","config method =  BC-noconfound\n","config env =  Acrobot-v1\n","Run 1 out of 10\n"]},{"output_type":"stream","name":"stderr","text":["/content/drive/.shortcut-targets-by-id/15_BhZyJSvQWMTBbzA9iHkhBymeY5HxXY/Invariant-Causal-Imitation-Learning-main/testing/train_utils.py:106: UserWarning: Buffer smaller than batch size\n","  warnings.warn(\"Buffer smaller than batch size\")\n"]},{"output_type":"stream","name":"stdout","text":["state_dim 10\n","epoch 9000/10000, policy loss 1.6442658790083442e-08\tepoch 0/100 return: -65.0\n","epoch 10/100 return: -500.0\n","epoch 20/100 return: -73.0\n","epoch 30/100 return: -87.0\n","epoch 40/100 return: -66.0\n","epoch 50/100 return: -100.0\n","epoch 60/100 return: -100.0\n","epoch 70/100 return: -67.0\n","epoch 80/100 return: -97.0\n","epoch 90/100 return: -80.0\n","###############    Reward for test environment for run 1: -117.12.   ###############\n","\n","\n","Run 2 out of 10\n"]},{"output_type":"stream","name":"stderr","text":["/content/drive/.shortcut-targets-by-id/15_BhZyJSvQWMTBbzA9iHkhBymeY5HxXY/Invariant-Causal-Imitation-Learning-main/testing/train_utils.py:106: UserWarning: Buffer smaller than batch size\n","  warnings.warn(\"Buffer smaller than batch size\")\n"]},{"output_type":"stream","name":"stdout","text":["state_dim 10\n","epoch 9000/10000, policy loss 0.0\tepoch 0/100 return: -101.0\n","epoch 10/100 return: -91.0\n","epoch 20/100 return: -70.0\n","epoch 30/100 return: -79.0\n","epoch 40/100 return: -500.0\n","epoch 50/100 return: -79.0\n","epoch 60/100 return: -66.0\n","epoch 70/100 return: -91.0\n","epoch 80/100 return: -82.0\n","epoch 90/100 return: -100.0\n","###############    Reward for test environment for run 2: -97.36.   ###############\n","\n","\n","Run 3 out of 10\n"]},{"output_type":"stream","name":"stderr","text":["/content/drive/.shortcut-targets-by-id/15_BhZyJSvQWMTBbzA9iHkhBymeY5HxXY/Invariant-Causal-Imitation-Learning-main/testing/train_utils.py:106: UserWarning: Buffer smaller than batch size\n","  warnings.warn(\"Buffer smaller than batch size\")\n"]},{"output_type":"stream","name":"stdout","text":["state_dim 10\n","epoch 9000/10000, policy loss 2.167441515155133e-08\tepoch 0/100 return: -91.0\n","epoch 10/100 return: -78.0\n","epoch 20/100 return: -83.0\n","epoch 30/100 return: -80.0\n","epoch 40/100 return: -65.0\n","epoch 50/100 return: -101.0\n","epoch 60/100 return: -74.0\n","epoch 70/100 return: -73.0\n","epoch 80/100 return: -235.0\n","epoch 90/100 return: -89.0\n","###############    Reward for test environment for run 3: -85.65.   ###############\n","\n","\n","Run 4 out of 10\n"]},{"output_type":"stream","name":"stderr","text":["/content/drive/.shortcut-targets-by-id/15_BhZyJSvQWMTBbzA9iHkhBymeY5HxXY/Invariant-Causal-Imitation-Learning-main/testing/train_utils.py:106: UserWarning: Buffer smaller than batch size\n","  warnings.warn(\"Buffer smaller than batch size\")\n"]},{"output_type":"stream","name":"stdout","text":["state_dim 10\n","epoch 9000/10000, policy loss 2.119275954726163e-08\tepoch 0/100 return: -80.0\n","epoch 10/100 return: -79.0\n","epoch 20/100 return: -92.0\n","epoch 30/100 return: -78.0\n","epoch 40/100 return: -76.0\n","epoch 50/100 return: -80.0\n","epoch 60/100 return: -194.0\n","epoch 70/100 return: -81.0\n","epoch 80/100 return: -74.0\n","epoch 90/100 return: -94.0\n","###############    Reward for test environment for run 4: -95.4.   ###############\n","\n","\n","Run 5 out of 10\n"]},{"output_type":"stream","name":"stderr","text":["/content/drive/.shortcut-targets-by-id/15_BhZyJSvQWMTBbzA9iHkhBymeY5HxXY/Invariant-Causal-Imitation-Learning-main/testing/train_utils.py:106: UserWarning: Buffer smaller than batch size\n","  warnings.warn(\"Buffer smaller than batch size\")\n"]},{"output_type":"stream","name":"stdout","text":["state_dim 10\n","epoch 9000/10000, policy loss 1.5381843354589364e-08\tepoch 0/100 return: -65.0\n","epoch 10/100 return: -500.0\n","epoch 20/100 return: -89.0\n","epoch 30/100 return: -122.0\n","epoch 40/100 return: -80.0\n","epoch 50/100 return: -199.0\n","epoch 60/100 return: -88.0\n","epoch 70/100 return: -86.0\n","epoch 80/100 return: -90.0\n","epoch 90/100 return: -86.0\n","###############    Reward for test environment for run 5: -118.77.   ###############\n","\n","\n","Run 6 out of 10\n"]},{"output_type":"stream","name":"stderr","text":["/content/drive/.shortcut-targets-by-id/15_BhZyJSvQWMTBbzA9iHkhBymeY5HxXY/Invariant-Causal-Imitation-Learning-main/testing/train_utils.py:106: UserWarning: Buffer smaller than batch size\n","  warnings.warn(\"Buffer smaller than batch size\")\n"]},{"output_type":"stream","name":"stdout","text":["state_dim 10\n","epoch 9000/10000, policy loss 1.7660633844229778e-08\tepoch 0/100 return: -84.0\n","epoch 10/100 return: -72.0\n","epoch 20/100 return: -80.0\n","epoch 30/100 return: -115.0\n","epoch 40/100 return: -95.0\n","epoch 50/100 return: -83.0\n","epoch 60/100 return: -85.0\n","epoch 70/100 return: -86.0\n","epoch 80/100 return: -64.0\n","epoch 90/100 return: -101.0\n","###############    Reward for test environment for run 6: -84.78.   ###############\n","\n","\n","Run 7 out of 10\n"]},{"output_type":"stream","name":"stderr","text":["/content/drive/.shortcut-targets-by-id/15_BhZyJSvQWMTBbzA9iHkhBymeY5HxXY/Invariant-Causal-Imitation-Learning-main/testing/train_utils.py:106: UserWarning: Buffer smaller than batch size\n","  warnings.warn(\"Buffer smaller than batch size\")\n"]},{"output_type":"stream","name":"stdout","text":["state_dim 10\n","epoch 9000/10000, policy loss 1.4449610397093693e-08\tepoch 0/100 return: -70.0\n","epoch 10/100 return: -80.0\n","epoch 20/100 return: -73.0\n","epoch 30/100 return: -71.0\n","epoch 40/100 return: -74.0\n","epoch 50/100 return: -100.0\n","epoch 60/100 return: -72.0\n","epoch 70/100 return: -78.0\n","epoch 80/100 return: -77.0\n","epoch 90/100 return: -73.0\n","###############    Reward for test environment for run 7: -80.31.   ###############\n","\n","\n","Run 8 out of 10\n"]},{"output_type":"stream","name":"stderr","text":["/content/drive/.shortcut-targets-by-id/15_BhZyJSvQWMTBbzA9iHkhBymeY5HxXY/Invariant-Causal-Imitation-Learning-main/testing/train_utils.py:106: UserWarning: Buffer smaller than batch size\n","  warnings.warn(\"Buffer smaller than batch size\")\n"]},{"output_type":"stream","name":"stdout","text":["state_dim 10\n","epoch 9000/10000, policy loss 1.7660633844229778e-08\tepoch 0/100 return: -127.0\n","epoch 10/100 return: -132.0\n","epoch 20/100 return: -91.0\n","epoch 30/100 return: -77.0\n","epoch 40/100 return: -72.0\n","epoch 50/100 return: -63.0\n","epoch 60/100 return: -64.0\n","epoch 70/100 return: -72.0\n","epoch 80/100 return: -76.0\n","epoch 90/100 return: -73.0\n","###############    Reward for test environment for run 8: -86.43.   ###############\n","\n","\n","Run 9 out of 10\n"]},{"output_type":"stream","name":"stderr","text":["/content/drive/.shortcut-targets-by-id/15_BhZyJSvQWMTBbzA9iHkhBymeY5HxXY/Invariant-Causal-Imitation-Learning-main/testing/train_utils.py:106: UserWarning: Buffer smaller than batch size\n","  warnings.warn(\"Buffer smaller than batch size\")\n"]},{"output_type":"stream","name":"stdout","text":["state_dim 10\n","epoch 9000/10000, policy loss 2.7247834211152622e-08\tepoch 0/100 return: -78.0\n","epoch 10/100 return: -104.0\n","epoch 20/100 return: -90.0\n","epoch 30/100 return: -77.0\n","epoch 40/100 return: -72.0\n","epoch 50/100 return: -93.0\n","epoch 60/100 return: -84.0\n","epoch 70/100 return: -71.0\n","epoch 80/100 return: -85.0\n","epoch 90/100 return: -74.0\n","###############    Reward for test environment for run 9: -83.2.   ###############\n","\n","\n","Run 10 out of 10\n"]},{"output_type":"stream","name":"stderr","text":["/content/drive/.shortcut-targets-by-id/15_BhZyJSvQWMTBbzA9iHkhBymeY5HxXY/Invariant-Causal-Imitation-Learning-main/testing/train_utils.py:106: UserWarning: Buffer smaller than batch size\n","  warnings.warn(\"Buffer smaller than batch size\")\n"]},{"output_type":"stream","name":"stdout","text":["state_dim 10\n","epoch 9000/10000, policy loss 0.0\tepoch 0/100 return: -66.0\n","epoch 10/100 return: -74.0\n","epoch 20/100 return: -65.0\n","epoch 30/100 return: -500.0\n","epoch 40/100 return: -84.0\n","epoch 50/100 return: -86.0\n","epoch 60/100 return: -500.0\n","epoch 70/100 return: -91.0\n","epoch 80/100 return: -86.0\n","epoch 90/100 return: -118.0\n","###############    Reward for test environment for run 10: -155.4.   ###############\n","\n","\n","Average reward for 10 repetitions: -100.44200000000001\n","ALL RESULTS TRAIL: [-100.44200000000001]\n","Config: {'ENV': 'Acrobot-v1', 'ALG': 'FINAL_Apr24_BCStudent_noconfound_origindata_trajnum2', 'NUM_TRAJS_GIVEN': 2, 'NUM_TRAINING_ENVS': 2, 'NOISE_DIM': 4, 'REP_SIZE': 16, 'TRAJ_SHIFT': 2, 'SAMPLING_RATE': 5, 'NUM_STEPS_TRAIN': 10000, 'NUM_TRAJS_VALID': 100, 'NUM_REPETITIONS': 10, 'BATCH_SIZE': 64, 'MLP_WIDTHS': 64, 'ADAM_ALPHA': 0.001, 'SGLD_BUFFER_SIZE': 10000, 'SGLD_LEARN_RATE': 0.01, 'SGLD_NOISE_COEF': 0.01, 'SGLD_NUM_STEPS': 100, 'SGLD_REINIT_FREQ': 0.05, 'NUM_STEPS_TRAIN_ENERGY_MODEL': 1000, 'TRIAL': 0, 'METHOD': 'BC-noconfound', 'EXPERT_ALG': 'dqn'}\n","Trial number 0\n","config method =  BC-noconfound\n","config env =  Acrobot-v1\n","Run 1 out of 10\n"]},{"output_type":"stream","name":"stderr","text":["/content/drive/.shortcut-targets-by-id/15_BhZyJSvQWMTBbzA9iHkhBymeY5HxXY/Invariant-Causal-Imitation-Learning-main/testing/train_utils.py:106: UserWarning: Buffer smaller than batch size\n","  warnings.warn(\"Buffer smaller than batch size\")\n"]},{"output_type":"stream","name":"stdout","text":["state_dim 10\n","epoch 9000/10000, policy loss 2.8176737387752837e-08\tepoch 0/100 return: -75.0\n","epoch 10/100 return: -86.0\n","epoch 20/100 return: -91.0\n","epoch 30/100 return: -72.0\n","epoch 40/100 return: -279.0\n","epoch 50/100 return: -73.0\n","epoch 60/100 return: -85.0\n","epoch 70/100 return: -72.0\n","epoch 80/100 return: -66.0\n","epoch 90/100 return: -72.0\n","###############    Reward for test environment for run 1: -84.3.   ###############\n","\n","\n","Run 2 out of 10\n","state_dim 10\n","epoch 9000/10000, policy loss 5.5879311844364565e-08\tepoch 0/100 return: -79.0\n","epoch 10/100 return: -79.0\n","epoch 20/100 return: -149.0\n","epoch 30/100 return: -73.0\n","epoch 40/100 return: -96.0\n","epoch 50/100 return: -85.0\n","epoch 60/100 return: -104.0\n","epoch 70/100 return: -73.0\n","epoch 80/100 return: -66.0\n","epoch 90/100 return: -65.0\n","###############    Reward for test environment for run 2: -85.99.   ###############\n","\n","\n","Run 3 out of 10\n"]},{"output_type":"stream","name":"stderr","text":["/content/drive/.shortcut-targets-by-id/15_BhZyJSvQWMTBbzA9iHkhBymeY5HxXY/Invariant-Causal-Imitation-Learning-main/testing/train_utils.py:106: UserWarning: Buffer smaller than batch size\n","  warnings.warn(\"Buffer smaller than batch size\")\n"]},{"output_type":"stream","name":"stdout","text":["state_dim 10\n","epoch 9000/10000, policy loss 3.232793943652723e-08\tepoch 0/100 return: -96.0\n","epoch 10/100 return: -77.0\n","epoch 20/100 return: -67.0\n","epoch 30/100 return: -64.0\n","epoch 40/100 return: -78.0\n","epoch 50/100 return: -81.0\n","epoch 60/100 return: -64.0\n","epoch 70/100 return: -77.0\n","epoch 80/100 return: -85.0\n","epoch 90/100 return: -92.0\n","###############    Reward for test environment for run 3: -79.81.   ###############\n","\n","\n","Run 4 out of 10\n"]},{"output_type":"stream","name":"stderr","text":["/content/drive/.shortcut-targets-by-id/15_BhZyJSvQWMTBbzA9iHkhBymeY5HxXY/Invariant-Causal-Imitation-Learning-main/testing/train_utils.py:106: UserWarning: Buffer smaller than batch size\n","  warnings.warn(\"Buffer smaller than batch size\")\n"]},{"output_type":"stream","name":"stdout","text":["state_dim 10\n","epoch 9000/10000, policy loss 3.1268005074025496e-08\tepoch 0/100 return: -500.0\n","epoch 10/100 return: -78.0\n","epoch 20/100 return: -88.0\n","epoch 30/100 return: -81.0\n","epoch 40/100 return: -77.0\n","epoch 50/100 return: -76.0\n","epoch 60/100 return: -84.0\n","epoch 70/100 return: -84.0\n","epoch 80/100 return: -80.0\n","epoch 90/100 return: -72.0\n","###############    Reward for test environment for run 4: -91.51.   ###############\n","\n","\n","Run 5 out of 10\n","state_dim 10\n","epoch 9000/10000, policy loss 1.2852241582095303e-07\tepoch 0/100 return: -201.0\n","epoch 10/100 return: -80.0\n","epoch 20/100 return: -203.0\n","epoch 30/100 return: -103.0\n","epoch 40/100 return: -103.0\n","epoch 50/100 return: -195.0\n","epoch 60/100 return: -72.0\n","epoch 70/100 return: -241.0\n","epoch 80/100 return: -72.0\n","epoch 90/100 return: -104.0\n","###############    Reward for test environment for run 5: -99.05.   ###############\n","\n","\n","Run 6 out of 10\n"]},{"output_type":"stream","name":"stderr","text":["/content/drive/.shortcut-targets-by-id/15_BhZyJSvQWMTBbzA9iHkhBymeY5HxXY/Invariant-Causal-Imitation-Learning-main/testing/train_utils.py:106: UserWarning: Buffer smaller than batch size\n","  warnings.warn(\"Buffer smaller than batch size\")\n"]},{"output_type":"stream","name":"stdout","text":["state_dim 10\n","epoch 9000/10000, policy loss 8.207847912444777e-08\tepoch 0/100 return: -72.0\n","epoch 10/100 return: -79.0\n","epoch 20/100 return: -101.0\n","epoch 30/100 return: -78.0\n","epoch 40/100 return: -69.0\n","epoch 50/100 return: -81.0\n","epoch 60/100 return: -87.0\n","epoch 70/100 return: -73.0\n","epoch 80/100 return: -77.0\n","epoch 90/100 return: -73.0\n","###############    Reward for test environment for run 6: -83.06.   ###############\n","\n","\n","Run 7 out of 10\n"]},{"output_type":"stream","name":"stderr","text":["/content/drive/.shortcut-targets-by-id/15_BhZyJSvQWMTBbzA9iHkhBymeY5HxXY/Invariant-Causal-Imitation-Learning-main/testing/train_utils.py:106: UserWarning: Buffer smaller than batch size\n","  warnings.warn(\"Buffer smaller than batch size\")\n"]},{"output_type":"stream","name":"stdout","text":["state_dim 10\n","epoch 9000/10000, policy loss 1.5634002537012748e-08\tepoch 0/100 return: -87.0\n","epoch 10/100 return: -78.0\n","epoch 20/100 return: -75.0\n","epoch 30/100 return: -79.0\n","epoch 40/100 return: -80.0\n","epoch 50/100 return: -76.0\n","epoch 60/100 return: -71.0\n","epoch 70/100 return: -85.0\n","epoch 80/100 return: -97.0\n","epoch 90/100 return: -73.0\n","###############    Reward for test environment for run 7: -82.53.   ###############\n","\n","\n","Run 8 out of 10\n","state_dim 10\n","epoch 9000/10000, policy loss 4.097818262494002e-08\tepoch 0/100 return: -81.0\n","epoch 10/100 return: -94.0\n","epoch 20/100 return: -64.0\n","epoch 30/100 return: -72.0\n","epoch 40/100 return: -73.0\n","epoch 50/100 return: -78.0\n","epoch 60/100 return: -90.0\n","epoch 70/100 return: -119.0\n","epoch 80/100 return: -76.0\n","epoch 90/100 return: -92.0\n","###############    Reward for test environment for run 8: -82.61.   ###############\n","\n","\n","Run 9 out of 10\n"]},{"output_type":"stream","name":"stderr","text":["/content/drive/.shortcut-targets-by-id/15_BhZyJSvQWMTBbzA9iHkhBymeY5HxXY/Invariant-Causal-Imitation-Learning-main/testing/train_utils.py:106: UserWarning: Buffer smaller than batch size\n","  warnings.warn(\"Buffer smaller than batch size\")\n"]},{"output_type":"stream","name":"stdout","text":["state_dim 10\n","epoch 9000/10000, policy loss 4.601058734010621e-08\tepoch 0/100 return: -223.0\n","epoch 10/100 return: -89.0\n","epoch 20/100 return: -80.0\n","epoch 30/100 return: -100.0\n","epoch 40/100 return: -69.0\n","epoch 50/100 return: -123.0\n","epoch 60/100 return: -147.0\n","epoch 70/100 return: -85.0\n","epoch 80/100 return: -96.0\n","epoch 90/100 return: -335.0\n","###############    Reward for test environment for run 9: -115.29.   ###############\n","\n","\n","Run 10 out of 10\n"]},{"output_type":"stream","name":"stderr","text":["/content/drive/.shortcut-targets-by-id/15_BhZyJSvQWMTBbzA9iHkhBymeY5HxXY/Invariant-Causal-Imitation-Learning-main/testing/train_utils.py:106: UserWarning: Buffer smaller than batch size\n","  warnings.warn(\"Buffer smaller than batch size\")\n"]},{"output_type":"stream","name":"stdout","text":["state_dim 10\n","epoch 9000/10000, policy loss 5.019337834255566e-08\tepoch 0/100 return: -89.0\n","epoch 10/100 return: -77.0\n","epoch 20/100 return: -72.0\n","epoch 30/100 return: -77.0\n","epoch 40/100 return: -72.0\n","epoch 50/100 return: -77.0\n","epoch 60/100 return: -76.0\n","epoch 70/100 return: -90.0\n","epoch 80/100 return: -94.0\n","epoch 90/100 return: -83.0\n","###############    Reward for test environment for run 10: -84.63.   ###############\n","\n","\n","Average reward for 10 repetitions: -88.878\n","ALL RESULTS TRAIL: [-88.878]\n","Config: {'ENV': 'Acrobot-v1', 'ALG': 'FINAL_Apr24_BCStudent_noconfound_origindata_trajnum3', 'NUM_TRAJS_GIVEN': 3, 'NUM_TRAINING_ENVS': 2, 'NOISE_DIM': 4, 'REP_SIZE': 16, 'TRAJ_SHIFT': 3, 'SAMPLING_RATE': 5, 'NUM_STEPS_TRAIN': 10000, 'NUM_TRAJS_VALID': 100, 'NUM_REPETITIONS': 10, 'BATCH_SIZE': 64, 'MLP_WIDTHS': 64, 'ADAM_ALPHA': 0.001, 'SGLD_BUFFER_SIZE': 10000, 'SGLD_LEARN_RATE': 0.01, 'SGLD_NOISE_COEF': 0.01, 'SGLD_NUM_STEPS': 100, 'SGLD_REINIT_FREQ': 0.05, 'NUM_STEPS_TRAIN_ENERGY_MODEL': 1000, 'TRIAL': 0, 'METHOD': 'BC-noconfound', 'EXPERT_ALG': 'dqn'}\n","Trial number 0\n","config method =  BC-noconfound\n","config env =  Acrobot-v1\n","Run 1 out of 10\n","state_dim 10\n","epoch 9000/10000, policy loss 8.568160581035045e-08\tepoch 0/100 return: -78.0\n","epoch 10/100 return: -74.0\n","epoch 20/100 return: -89.0\n","epoch 30/100 return: -73.0\n","epoch 40/100 return: -94.0\n","epoch 50/100 return: -78.0\n","epoch 60/100 return: -74.0\n","epoch 70/100 return: -72.0\n","epoch 80/100 return: -98.0\n","epoch 90/100 return: -73.0\n","###############    Reward for test environment for run 1: -89.21.   ###############\n","\n","\n","Run 2 out of 10\n","state_dim 10\n","epoch 9000/10000, policy loss 1.0430802888095059e-07\tepoch 0/100 return: -78.0\n","epoch 10/100 return: -82.0\n","epoch 20/100 return: -72.0\n","epoch 30/100 return: -77.0\n","epoch 40/100 return: -81.0\n","epoch 50/100 return: -86.0\n","epoch 60/100 return: -75.0\n","epoch 70/100 return: -92.0\n","epoch 80/100 return: -64.0\n","epoch 90/100 return: -74.0\n","###############    Reward for test environment for run 2: -83.4.   ###############\n","\n","\n","Run 3 out of 10\n","state_dim 10\n","epoch 9000/10000, policy loss 1.303850751810387e-07\tepoch 0/100 return: -78.0\n","epoch 10/100 return: -71.0\n","epoch 20/100 return: -109.0\n","epoch 30/100 return: -72.0\n","epoch 40/100 return: -87.0\n","epoch 50/100 return: -113.0\n","epoch 60/100 return: -103.0\n","epoch 70/100 return: -94.0\n","epoch 80/100 return: -78.0\n","epoch 90/100 return: -85.0\n","###############    Reward for test environment for run 3: -84.19.   ###############\n","\n","\n","Run 4 out of 10\n","state_dim 10\n","epoch 9000/10000, policy loss 4.172312060291006e-07\tepoch 0/100 return: -71.0\n","epoch 10/100 return: -76.0\n","epoch 20/100 return: -92.0\n","epoch 30/100 return: -89.0\n","epoch 40/100 return: -73.0\n","epoch 50/100 return: -73.0\n","epoch 60/100 return: -77.0\n","epoch 70/100 return: -93.0\n","epoch 80/100 return: -87.0\n","epoch 90/100 return: -117.0\n","###############    Reward for test environment for run 4: -82.64.   ###############\n","\n","\n","Run 5 out of 10\n","state_dim 10\n","epoch 9000/10000, policy loss 7.636842269675981e-08\tepoch 0/100 return: -78.0\n","epoch 10/100 return: -189.0\n","epoch 20/100 return: -90.0\n","epoch 30/100 return: -74.0\n","epoch 40/100 return: -86.0\n","epoch 50/100 return: -84.0\n","epoch 60/100 return: -77.0\n","epoch 70/100 return: -110.0\n","epoch 80/100 return: -77.0\n","epoch 90/100 return: -78.0\n","###############    Reward for test environment for run 5: -84.82.   ###############\n","\n","\n","Run 6 out of 10\n","state_dim 10\n","epoch 9000/10000, policy loss 2.160665530936967e-07\tepoch 0/100 return: -89.0\n","epoch 10/100 return: -79.0\n","epoch 20/100 return: -76.0\n","epoch 30/100 return: -78.0\n","epoch 40/100 return: -103.0\n","epoch 50/100 return: -94.0\n","epoch 60/100 return: -69.0\n","epoch 70/100 return: -100.0\n","epoch 80/100 return: -72.0\n","epoch 90/100 return: -136.0\n","###############    Reward for test environment for run 6: -102.65.   ###############\n","\n","\n","Run 7 out of 10\n","state_dim 10\n","epoch 9000/10000, policy loss 1.154838216166354e-07\tepoch 0/100 return: -84.0\n","epoch 10/100 return: -83.0\n","epoch 20/100 return: -92.0\n","epoch 30/100 return: -168.0\n","epoch 40/100 return: -85.0\n","epoch 50/100 return: -79.0\n","epoch 60/100 return: -78.0\n","epoch 70/100 return: -77.0\n","epoch 80/100 return: -99.0\n","epoch 90/100 return: -116.0\n","###############    Reward for test environment for run 7: -83.87.   ###############\n","\n","\n","Run 8 out of 10\n","state_dim 10\n","epoch 9000/10000, policy loss 1.2852243003180774e-07\tepoch 0/100 return: -78.0\n","epoch 10/100 return: -91.0\n","epoch 20/100 return: -71.0\n","epoch 30/100 return: -86.0\n","epoch 40/100 return: -79.0\n","epoch 50/100 return: -69.0\n","epoch 60/100 return: -78.0\n","epoch 70/100 return: -89.0\n","epoch 80/100 return: -76.0\n","epoch 90/100 return: -91.0\n","###############    Reward for test environment for run 8: -82.26.   ###############\n","\n","\n","Run 9 out of 10\n","state_dim 10\n","epoch 9000/10000, policy loss 6.351599495246774e-07\tepoch 0/100 return: -88.0\n","epoch 10/100 return: -101.0\n","epoch 20/100 return: -83.0\n","epoch 30/100 return: -66.0\n","epoch 40/100 return: -66.0\n","epoch 50/100 return: -149.0\n","epoch 60/100 return: -65.0\n","epoch 70/100 return: -72.0\n","epoch 80/100 return: -76.0\n","epoch 90/100 return: -65.0\n","###############    Reward for test environment for run 9: -84.53.   ###############\n","\n","\n","Run 10 out of 10\n","state_dim 10\n","epoch 9000/10000, policy loss 1.0058271016077924e-07\tepoch 0/100 return: -72.0\n","epoch 10/100 return: -77.0\n","epoch 20/100 return: -73.0\n","epoch 30/100 return: -78.0\n","epoch 40/100 return: -72.0\n","epoch 50/100 return: -81.0\n","epoch 60/100 return: -79.0\n","epoch 70/100 return: -251.0\n","epoch 80/100 return: -65.0\n","epoch 90/100 return: -83.0\n","###############    Reward for test environment for run 10: -82.24.   ###############\n","\n","\n","Average reward for 10 repetitions: -85.981\n","ALL RESULTS TRAIL: [-85.981]\n","Config: {'ENV': 'Acrobot-v1', 'ALG': 'FINAL_Apr24_BCStudent_noconfound_origindata_trajnum4', 'NUM_TRAJS_GIVEN': 4, 'NUM_TRAINING_ENVS': 2, 'NOISE_DIM': 4, 'REP_SIZE': 16, 'TRAJ_SHIFT': 4, 'SAMPLING_RATE': 5, 'NUM_STEPS_TRAIN': 10000, 'NUM_TRAJS_VALID': 100, 'NUM_REPETITIONS': 10, 'BATCH_SIZE': 64, 'MLP_WIDTHS': 64, 'ADAM_ALPHA': 0.001, 'SGLD_BUFFER_SIZE': 10000, 'SGLD_LEARN_RATE': 0.01, 'SGLD_NOISE_COEF': 0.01, 'SGLD_NUM_STEPS': 100, 'SGLD_REINIT_FREQ': 0.05, 'NUM_STEPS_TRAIN_ENERGY_MODEL': 1000, 'TRIAL': 0, 'METHOD': 'BC-noconfound', 'EXPERT_ALG': 'dqn'}\n","Trial number 0\n","config method =  BC-noconfound\n","config env =  Acrobot-v1\n","Run 1 out of 10\n","state_dim 10\n","epoch 9000/10000, policy loss 2.5331905817438383e-07\tepoch 0/100 return: -100.0\n","epoch 10/100 return: -72.0\n","epoch 20/100 return: -88.0\n","epoch 30/100 return: -92.0\n","epoch 40/100 return: -65.0\n","epoch 50/100 return: -73.0\n","epoch 60/100 return: -65.0\n","epoch 70/100 return: -91.0\n","epoch 80/100 return: -90.0\n","epoch 90/100 return: -94.0\n","###############    Reward for test environment for run 1: -85.03.   ###############\n","\n","\n","Run 2 out of 10\n","state_dim 10\n","epoch 9000/10000, policy loss 4.656591841012414e-07\tepoch 0/100 return: -92.0\n","epoch 10/100 return: -73.0\n","epoch 20/100 return: -80.0\n","epoch 30/100 return: -166.0\n","epoch 40/100 return: -65.0\n","epoch 50/100 return: -66.0\n","epoch 60/100 return: -72.0\n","epoch 70/100 return: -85.0\n","epoch 80/100 return: -72.0\n","epoch 90/100 return: -91.0\n","###############    Reward for test environment for run 2: -86.1.   ###############\n","\n","\n","Run 3 out of 10\n","state_dim 10\n","epoch 9000/10000, policy loss 1.3206000630816561e-06\tepoch 0/100 return: -83.0\n","epoch 10/100 return: -69.0\n","epoch 20/100 return: -87.0\n","epoch 30/100 return: -79.0\n","epoch 40/100 return: -83.0\n","epoch 50/100 return: -72.0\n","epoch 60/100 return: -89.0\n","epoch 70/100 return: -113.0\n","epoch 80/100 return: -78.0\n","epoch 90/100 return: -72.0\n","###############    Reward for test environment for run 3: -84.15.   ###############\n","\n","\n","Run 4 out of 10\n","state_dim 10\n","epoch 9000/10000, policy loss 6.519250916880992e-08\tepoch 0/100 return: -86.0\n","epoch 10/100 return: -80.0\n","epoch 20/100 return: -71.0\n","epoch 30/100 return: -69.0\n","epoch 40/100 return: -74.0\n","epoch 50/100 return: -78.0\n","epoch 60/100 return: -89.0\n","epoch 70/100 return: -69.0\n","epoch 80/100 return: -103.0\n","epoch 90/100 return: -76.0\n","###############    Reward for test environment for run 4: -81.57.   ###############\n","\n","\n","Run 5 out of 10\n","state_dim 10\n","epoch 9000/10000, policy loss 3.930169043542264e-07\tepoch 0/100 return: -72.0\n","epoch 10/100 return: -66.0\n","epoch 20/100 return: -65.0\n","epoch 30/100 return: -89.0\n","epoch 40/100 return: -84.0\n","epoch 50/100 return: -92.0\n","epoch 60/100 return: -64.0\n","epoch 70/100 return: -71.0\n","epoch 80/100 return: -72.0\n","epoch 90/100 return: -79.0\n","###############    Reward for test environment for run 5: -81.92.   ###############\n","\n","\n","Run 6 out of 10\n","state_dim 10\n","epoch 9000/10000, policy loss 2.942956598417368e-07\tepoch 0/100 return: -72.0\n","epoch 10/100 return: -78.0\n","epoch 20/100 return: -89.0\n","epoch 30/100 return: -71.0\n","epoch 40/100 return: -81.0\n","epoch 50/100 return: -71.0\n","epoch 60/100 return: -86.0\n","epoch 70/100 return: -71.0\n","epoch 80/100 return: -77.0\n","epoch 90/100 return: -77.0\n","###############    Reward for test environment for run 6: -83.64.   ###############\n","\n","\n","Run 7 out of 10\n","state_dim 10\n","epoch 9000/10000, policy loss 2.495908347555087e-06\tepoch 0/100 return: -73.0\n","epoch 10/100 return: -134.0\n","epoch 20/100 return: -72.0\n","epoch 30/100 return: -95.0\n","epoch 40/100 return: -95.0\n","epoch 50/100 return: -92.0\n","epoch 60/100 return: -73.0\n","epoch 70/100 return: -193.0\n","epoch 80/100 return: -78.0\n","epoch 90/100 return: -65.0\n","###############    Reward for test environment for run 7: -102.52.   ###############\n","\n","\n","Run 8 out of 10\n","state_dim 10\n","epoch 9000/10000, policy loss 4.377205016226071e-07\tepoch 0/100 return: -72.0\n","epoch 10/100 return: -96.0\n","epoch 20/100 return: -72.0\n","epoch 30/100 return: -72.0\n","epoch 40/100 return: -73.0\n","epoch 50/100 return: -66.0\n","epoch 60/100 return: -85.0\n","epoch 70/100 return: -82.0\n","epoch 80/100 return: -64.0\n","epoch 90/100 return: -96.0\n","###############    Reward for test environment for run 8: -92.2.   ###############\n","\n","\n","Run 9 out of 10\n","state_dim 10\n","epoch 9000/10000, policy loss 2.5890693677865784e-07\tepoch 0/100 return: -95.0\n","epoch 10/100 return: -80.0\n","epoch 20/100 return: -72.0\n","epoch 30/100 return: -116.0\n","epoch 40/100 return: -155.0\n","epoch 50/100 return: -77.0\n","epoch 60/100 return: -76.0\n","epoch 70/100 return: -89.0\n","epoch 80/100 return: -87.0\n","epoch 90/100 return: -79.0\n","###############    Reward for test environment for run 9: -84.17.   ###############\n","\n","\n","Run 10 out of 10\n","state_dim 10\n","epoch 9000/10000, policy loss 2.2910484176463797e-07\tepoch 0/100 return: -87.0\n","epoch 10/100 return: -80.0\n","epoch 20/100 return: -70.0\n","epoch 30/100 return: -71.0\n","epoch 40/100 return: -78.0\n","epoch 50/100 return: -79.0\n","epoch 60/100 return: -78.0\n","epoch 70/100 return: -79.0\n","epoch 80/100 return: -87.0\n","epoch 90/100 return: -69.0\n","###############    Reward for test environment for run 10: -83.87.   ###############\n","\n","\n","Average reward for 10 repetitions: -86.517\n","ALL RESULTS TRAIL: [-86.517]\n","Config: {'ENV': 'Acrobot-v1', 'ALG': 'FINAL_Apr24_BCStudent_noconfound_origindata_trajnum5', 'NUM_TRAJS_GIVEN': 5, 'NUM_TRAINING_ENVS': 2, 'NOISE_DIM': 4, 'REP_SIZE': 16, 'TRAJ_SHIFT': 5, 'SAMPLING_RATE': 5, 'NUM_STEPS_TRAIN': 10000, 'NUM_TRAJS_VALID': 100, 'NUM_REPETITIONS': 10, 'BATCH_SIZE': 64, 'MLP_WIDTHS': 64, 'ADAM_ALPHA': 0.001, 'SGLD_BUFFER_SIZE': 10000, 'SGLD_LEARN_RATE': 0.01, 'SGLD_NOISE_COEF': 0.01, 'SGLD_NUM_STEPS': 100, 'SGLD_REINIT_FREQ': 0.05, 'NUM_STEPS_TRAIN_ENERGY_MODEL': 1000, 'TRIAL': 0, 'METHOD': 'BC-noconfound', 'EXPERT_ALG': 'dqn'}\n","Trial number 0\n","config method =  BC-noconfound\n","config env =  Acrobot-v1\n","Run 1 out of 10\n","state_dim 10\n","epoch 9000/10000, policy loss 0.0\tepoch 0/100 return: -174.0\n","epoch 10/100 return: -90.0\n","epoch 20/100 return: -64.0\n","epoch 30/100 return: -100.0\n","epoch 40/100 return: -84.0\n","epoch 50/100 return: -82.0\n","epoch 60/100 return: -146.0\n","epoch 70/100 return: -157.0\n","epoch 80/100 return: -65.0\n","epoch 90/100 return: -75.0\n","###############    Reward for test environment for run 1: -83.95.   ###############\n","\n","\n","Run 2 out of 10\n","state_dim 10\n","epoch 9000/10000, policy loss 6.258447911022813e-07\tepoch 0/100 return: -154.0\n","epoch 10/100 return: -202.0\n","epoch 20/100 return: -79.0\n","epoch 30/100 return: -91.0\n","epoch 40/100 return: -95.0\n","epoch 50/100 return: -77.0\n","epoch 60/100 return: -78.0\n","epoch 70/100 return: -90.0\n","epoch 80/100 return: -80.0\n","epoch 90/100 return: -227.0\n","###############    Reward for test environment for run 2: -86.6.   ###############\n","\n","\n","Run 3 out of 10\n","state_dim 10\n","epoch 9000/10000, policy loss 1.016996293401462e-06\tepoch 0/100 return: -74.0\n","epoch 10/100 return: -73.0\n","epoch 20/100 return: -70.0\n","epoch 30/100 return: -72.0\n","epoch 40/100 return: -79.0\n","epoch 50/100 return: -66.0\n","epoch 60/100 return: -83.0\n","epoch 70/100 return: -82.0\n","epoch 80/100 return: -89.0\n","epoch 90/100 return: -78.0\n","###############    Reward for test environment for run 3: -80.3.   ###############\n","\n","\n","Run 4 out of 10\n","state_dim 10\n","epoch 9000/10000, policy loss 5.56929308004328e-07\tepoch 0/100 return: -72.0\n","epoch 10/100 return: -72.0\n","epoch 20/100 return: -65.0\n","epoch 30/100 return: -71.0\n","epoch 40/100 return: -73.0\n","epoch 50/100 return: -73.0\n","epoch 60/100 return: -65.0\n","epoch 70/100 return: -70.0\n","epoch 80/100 return: -66.0\n","epoch 90/100 return: -77.0\n","###############    Reward for test environment for run 4: -81.92.   ###############\n","\n","\n","Run 5 out of 10\n","state_dim 10\n","epoch 9000/10000, policy loss 2.5610040665924316e-06\tepoch 0/100 return: -72.0\n","epoch 10/100 return: -72.0\n","epoch 20/100 return: -87.0\n","epoch 30/100 return: -85.0\n","epoch 40/100 return: -192.0\n","epoch 50/100 return: -88.0\n","epoch 60/100 return: -77.0\n","epoch 70/100 return: -72.0\n","epoch 80/100 return: -79.0\n","epoch 90/100 return: -73.0\n","###############    Reward for test environment for run 5: -83.9.   ###############\n","\n","\n","Run 6 out of 10\n","state_dim 10\n","epoch 9000/10000, policy loss 1.0132720262845396e-06\tepoch 0/100 return: -71.0\n","epoch 10/100 return: -79.0\n","epoch 20/100 return: -80.0\n","epoch 30/100 return: -69.0\n","epoch 40/100 return: -93.0\n","epoch 50/100 return: -77.0\n","epoch 60/100 return: -77.0\n","epoch 70/100 return: -70.0\n","epoch 80/100 return: -76.0\n","epoch 90/100 return: -93.0\n","###############    Reward for test environment for run 6: -82.48.   ###############\n","\n","\n","Run 7 out of 10\n","state_dim 10\n","epoch 9000/10000, policy loss 1.84401457659078e-07\tepoch 0/100 return: -86.0\n","epoch 10/100 return: -83.0\n","epoch 20/100 return: -75.0\n","epoch 30/100 return: -90.0\n","epoch 40/100 return: -98.0\n","epoch 50/100 return: -96.0\n","epoch 60/100 return: -81.0\n","epoch 70/100 return: -86.0\n","epoch 80/100 return: -90.0\n","epoch 90/100 return: -80.0\n","###############    Reward for test environment for run 7: -86.39.   ###############\n","\n","\n","Run 8 out of 10\n","state_dim 10\n","epoch 9000/10000, policy loss 2.3841775487198902e-07\tepoch 0/100 return: -70.0\n","epoch 10/100 return: -70.0\n","epoch 20/100 return: -71.0\n","epoch 30/100 return: -103.0\n","epoch 40/100 return: -77.0\n","epoch 50/100 return: -89.0\n","epoch 60/100 return: -71.0\n","epoch 70/100 return: -116.0\n","epoch 80/100 return: -92.0\n","epoch 90/100 return: -85.0\n","###############    Reward for test environment for run 8: -85.09.   ###############\n","\n","\n","Run 9 out of 10\n","state_dim 10\n","epoch 9000/10000, policy loss 6.966225782889524e-07\tepoch 0/100 return: -73.0\n","epoch 10/100 return: -98.0\n","epoch 20/100 return: -214.0\n","epoch 30/100 return: -88.0\n","epoch 40/100 return: -73.0\n","epoch 50/100 return: -95.0\n","epoch 60/100 return: -89.0\n","epoch 70/100 return: -89.0\n","epoch 80/100 return: -87.0\n","epoch 90/100 return: -72.0\n","###############    Reward for test environment for run 9: -95.32.   ###############\n","\n","\n","Run 10 out of 10\n","state_dim 10\n","epoch 9000/10000, policy loss 4.58208830877993e-07\tepoch 0/100 return: -85.0\n","epoch 10/100 return: -72.0\n","epoch 20/100 return: -197.0\n","epoch 30/100 return: -87.0\n","epoch 40/100 return: -86.0\n","epoch 50/100 return: -83.0\n","epoch 60/100 return: -81.0\n","epoch 70/100 return: -80.0\n","epoch 80/100 return: -69.0\n","epoch 90/100 return: -72.0\n","###############    Reward for test environment for run 10: -87.79.   ###############\n","\n","\n","Average reward for 10 repetitions: -85.374\n","ALL RESULTS TRAIL: [-85.374]\n","Config: {'ENV': 'Acrobot-v1', 'ALG': 'FINAL_Apr24_BCStudent_noconfound_origindata_trajnum6', 'NUM_TRAJS_GIVEN': 6, 'NUM_TRAINING_ENVS': 2, 'NOISE_DIM': 4, 'REP_SIZE': 16, 'TRAJ_SHIFT': 6, 'SAMPLING_RATE': 5, 'NUM_STEPS_TRAIN': 10000, 'NUM_TRAJS_VALID': 100, 'NUM_REPETITIONS': 10, 'BATCH_SIZE': 64, 'MLP_WIDTHS': 64, 'ADAM_ALPHA': 0.001, 'SGLD_BUFFER_SIZE': 10000, 'SGLD_LEARN_RATE': 0.01, 'SGLD_NOISE_COEF': 0.01, 'SGLD_NUM_STEPS': 100, 'SGLD_REINIT_FREQ': 0.05, 'NUM_STEPS_TRAIN_ENERGY_MODEL': 1000, 'TRIAL': 0, 'METHOD': 'BC-noconfound', 'EXPERT_ALG': 'dqn'}\n","Trial number 0\n","config method =  BC-noconfound\n","config env =  Acrobot-v1\n","Run 1 out of 10\n","state_dim 10\n","epoch 9000/10000, policy loss 3.166493911521684e-08\tepoch 0/100 return: -78.0\n","epoch 10/100 return: -84.0\n","epoch 20/100 return: -73.0\n","epoch 30/100 return: -66.0\n","epoch 40/100 return: -90.0\n","epoch 50/100 return: -93.0\n","epoch 60/100 return: -92.0\n","epoch 70/100 return: -73.0\n","epoch 80/100 return: -82.0\n","epoch 90/100 return: -64.0\n","###############    Reward for test environment for run 1: -79.95.   ###############\n","\n","\n","Run 2 out of 10\n","state_dim 10\n","epoch 9000/10000, policy loss 4.036250174976885e-06\tepoch 0/100 return: -78.0\n","epoch 10/100 return: -73.0\n","epoch 20/100 return: -89.0\n","epoch 30/100 return: -69.0\n","epoch 40/100 return: -80.0\n","epoch 50/100 return: -73.0\n","epoch 60/100 return: -77.0\n","epoch 70/100 return: -94.0\n","epoch 80/100 return: -74.0\n","epoch 90/100 return: -78.0\n","###############    Reward for test environment for run 2: -82.99.   ###############\n","\n","\n","Run 3 out of 10\n","state_dim 10\n","epoch 9000/10000, policy loss 8.75431283020589e-07\tepoch 0/100 return: -91.0\n","epoch 10/100 return: -76.0\n","epoch 20/100 return: -64.0\n","epoch 30/100 return: -65.0\n","epoch 40/100 return: -64.0\n","epoch 50/100 return: -78.0\n","epoch 60/100 return: -73.0\n","epoch 70/100 return: -86.0\n","epoch 80/100 return: -73.0\n","epoch 90/100 return: -115.0\n","###############    Reward for test environment for run 3: -84.37.   ###############\n","\n","\n","Run 4 out of 10\n","state_dim 10\n","epoch 9000/10000, policy loss 0.012967615388333797\tepoch 0/100 return: -86.0\n","epoch 10/100 return: -94.0\n","epoch 20/100 return: -84.0\n","epoch 30/100 return: -77.0\n","epoch 40/100 return: -78.0\n","epoch 50/100 return: -91.0\n","epoch 60/100 return: -82.0\n","epoch 70/100 return: -78.0\n","epoch 80/100 return: -77.0\n","epoch 90/100 return: -78.0\n","###############    Reward for test environment for run 4: -83.57.   ###############\n","\n","\n","Run 5 out of 10\n","state_dim 10\n","epoch 9000/10000, policy loss 1.9147769307892304e-06\tepoch 0/100 return: -76.0\n","epoch 10/100 return: -95.0\n","epoch 20/100 return: -71.0\n","epoch 30/100 return: -98.0\n","epoch 40/100 return: -96.0\n","epoch 50/100 return: -87.0\n","epoch 60/100 return: -71.0\n","epoch 70/100 return: -73.0\n","epoch 80/100 return: -85.0\n","epoch 90/100 return: -64.0\n","###############    Reward for test environment for run 5: -84.22.   ###############\n","\n","\n","Run 6 out of 10\n","state_dim 10\n","epoch 9000/10000, policy loss 4.3026849994021177e-07\tepoch 0/100 return: -77.0\n","epoch 10/100 return: -121.0\n","epoch 20/100 return: -139.0\n","epoch 30/100 return: -77.0\n","epoch 40/100 return: -194.0\n","epoch 50/100 return: -72.0\n","epoch 60/100 return: -79.0\n","epoch 70/100 return: -77.0\n","epoch 80/100 return: -80.0\n","epoch 90/100 return: -79.0\n","###############    Reward for test environment for run 6: -86.56.   ###############\n","\n","\n","Run 7 out of 10\n","state_dim 10\n","epoch 9000/10000, policy loss 2.9615952712447324e-07\tepoch 0/100 return: -70.0\n","epoch 10/100 return: -77.0\n","epoch 20/100 return: -80.0\n","epoch 30/100 return: -102.0\n","epoch 40/100 return: -77.0\n","epoch 50/100 return: -84.0\n","epoch 60/100 return: -72.0\n","epoch 70/100 return: -77.0\n","epoch 80/100 return: -70.0\n","epoch 90/100 return: -75.0\n","###############    Reward for test environment for run 7: -81.35.   ###############\n","\n","\n","Run 8 out of 10\n","state_dim 10\n","epoch 9000/10000, policy loss 6.426103027479257e-07\tepoch 0/100 return: -77.0\n","epoch 10/100 return: -72.0\n","epoch 20/100 return: -186.0\n","epoch 30/100 return: -75.0\n","epoch 40/100 return: -84.0\n","epoch 50/100 return: -85.0\n","epoch 60/100 return: -87.0\n","epoch 70/100 return: -70.0\n","epoch 80/100 return: -72.0\n","epoch 90/100 return: -70.0\n","###############    Reward for test environment for run 8: -84.01.   ###############\n","\n","\n","Run 9 out of 10\n","state_dim 10\n","epoch 9000/10000, policy loss 1.0300256008122233e-06\tepoch 0/100 return: -88.0\n","epoch 10/100 return: -77.0\n","epoch 20/100 return: -77.0\n","epoch 30/100 return: -90.0\n","epoch 40/100 return: -73.0\n","epoch 50/100 return: -72.0\n","epoch 60/100 return: -71.0\n","epoch 70/100 return: -73.0\n","epoch 80/100 return: -72.0\n","epoch 90/100 return: -72.0\n","###############    Reward for test environment for run 9: -81.19.   ###############\n","\n","\n","Run 10 out of 10\n","state_dim 10\n","epoch 9000/10000, policy loss 5.960407065686013e-07\tepoch 0/100 return: -85.0\n","epoch 10/100 return: -87.0\n","epoch 20/100 return: -92.0\n","epoch 30/100 return: -86.0\n","epoch 40/100 return: -78.0\n","epoch 50/100 return: -73.0\n","epoch 60/100 return: -91.0\n","epoch 70/100 return: -97.0\n","epoch 80/100 return: -98.0\n","epoch 90/100 return: -73.0\n","###############    Reward for test environment for run 10: -87.1.   ###############\n","\n","\n","Average reward for 10 repetitions: -83.531\n","ALL RESULTS TRAIL: [-83.531]\n","Config: {'ENV': 'Acrobot-v1', 'ALG': 'FINAL_Apr24_BCStudent_noconfound_origindata_trajnum7', 'NUM_TRAJS_GIVEN': 7, 'NUM_TRAINING_ENVS': 2, 'NOISE_DIM': 4, 'REP_SIZE': 16, 'TRAJ_SHIFT': 7, 'SAMPLING_RATE': 5, 'NUM_STEPS_TRAIN': 10000, 'NUM_TRAJS_VALID': 100, 'NUM_REPETITIONS': 10, 'BATCH_SIZE': 64, 'MLP_WIDTHS': 64, 'ADAM_ALPHA': 0.001, 'SGLD_BUFFER_SIZE': 10000, 'SGLD_LEARN_RATE': 0.01, 'SGLD_NOISE_COEF': 0.01, 'SGLD_NUM_STEPS': 100, 'SGLD_REINIT_FREQ': 0.05, 'NUM_STEPS_TRAIN_ENERGY_MODEL': 1000, 'TRIAL': 0, 'METHOD': 'BC-noconfound', 'EXPERT_ALG': 'dqn'}\n","Trial number 0\n","config method =  BC-noconfound\n","config env =  Acrobot-v1\n","Run 1 out of 10\n","state_dim 10\n","epoch 9000/10000, policy loss 4.0046691651696165e-07\tepoch 0/100 return: -98.0\n","epoch 10/100 return: -87.0\n","epoch 20/100 return: -112.0\n","epoch 30/100 return: -109.0\n","epoch 40/100 return: -73.0\n","epoch 50/100 return: -71.0\n","epoch 60/100 return: -91.0\n","epoch 70/100 return: -72.0\n","epoch 80/100 return: -73.0\n","epoch 90/100 return: -111.0\n","###############    Reward for test environment for run 1: -83.57.   ###############\n","\n","\n","Run 2 out of 10\n","state_dim 10\n","epoch 9000/10000, policy loss 1.2127993613830768e-05\tepoch 0/100 return: -72.0\n","epoch 10/100 return: -89.0\n","epoch 20/100 return: -74.0\n","epoch 30/100 return: -64.0\n","epoch 40/100 return: -69.0\n","epoch 50/100 return: -84.0\n","epoch 60/100 return: -86.0\n","epoch 70/100 return: -76.0\n","epoch 80/100 return: -102.0\n","epoch 90/100 return: -86.0\n","###############    Reward for test environment for run 2: -82.45.   ###############\n","\n","\n","Run 3 out of 10\n","state_dim 10\n","epoch 9000/10000, policy loss 4.170075953879859e-06\tepoch 0/100 return: -73.0\n","epoch 10/100 return: -80.0\n","epoch 20/100 return: -93.0\n","epoch 30/100 return: -79.0\n","epoch 40/100 return: -85.0\n","epoch 50/100 return: -97.0\n","epoch 60/100 return: -75.0\n","epoch 70/100 return: -81.0\n","epoch 80/100 return: -82.0\n","epoch 90/100 return: -84.0\n","###############    Reward for test environment for run 3: -82.6.   ###############\n","\n","\n","Run 4 out of 10\n","state_dim 10\n","epoch 9000/10000, policy loss 5.29887529410189e-06\tepoch 0/100 return: -66.0\n","epoch 10/100 return: -73.0\n","epoch 20/100 return: -100.0\n","epoch 30/100 return: -202.0\n","epoch 40/100 return: -72.0\n","epoch 50/100 return: -73.0\n","epoch 60/100 return: -84.0\n","epoch 70/100 return: -101.0\n","epoch 80/100 return: -76.0\n","epoch 90/100 return: -72.0\n","###############    Reward for test environment for run 4: -102.39.   ###############\n","\n","\n","Run 5 out of 10\n","state_dim 10\n","epoch 9000/10000, policy loss 5.283945574774407e-05\tepoch 0/100 return: -76.0\n","epoch 10/100 return: -72.0\n","epoch 20/100 return: -73.0\n","epoch 30/100 return: -73.0\n","epoch 40/100 return: -65.0\n","epoch 50/100 return: -65.0\n","epoch 60/100 return: -79.0\n","epoch 70/100 return: -96.0\n","epoch 80/100 return: -103.0\n","epoch 90/100 return: -105.0\n","###############    Reward for test environment for run 5: -81.75.   ###############\n","\n","\n","Run 6 out of 10\n","state_dim 10\n","epoch 9000/10000, policy loss 2.549886175984284e-06\tepoch 0/100 return: -77.0\n","epoch 10/100 return: -77.0\n","epoch 20/100 return: -79.0\n","epoch 30/100 return: -77.0\n","epoch 40/100 return: -72.0\n","epoch 50/100 return: -78.0\n","epoch 60/100 return: -77.0\n","epoch 70/100 return: -85.0\n","epoch 80/100 return: -97.0\n","epoch 90/100 return: -77.0\n","###############    Reward for test environment for run 6: -80.43.   ###############\n","\n","\n","Run 7 out of 10\n","state_dim 10\n","epoch 9000/10000, policy loss 5.550629680328711e-07\tepoch 0/100 return: -73.0\n","epoch 10/100 return: -72.0\n","epoch 20/100 return: -80.0\n","epoch 30/100 return: -81.0\n","epoch 40/100 return: -73.0\n","epoch 50/100 return: -84.0\n","epoch 60/100 return: -74.0\n","epoch 70/100 return: -90.0\n","epoch 80/100 return: -79.0\n","epoch 90/100 return: -95.0\n","###############    Reward for test environment for run 7: -81.54.   ###############\n","\n","\n","Run 8 out of 10\n","state_dim 10\n","epoch 9000/10000, policy loss 1.0803316996543799e-07\tepoch 0/100 return: -71.0\n","epoch 10/100 return: -98.0\n","epoch 20/100 return: -77.0\n","epoch 30/100 return: -77.0\n","epoch 40/100 return: -77.0\n","epoch 50/100 return: -72.0\n","epoch 60/100 return: -78.0\n","epoch 70/100 return: -78.0\n","epoch 80/100 return: -73.0\n","epoch 90/100 return: -90.0\n","###############    Reward for test environment for run 8: -81.43.   ###############\n","\n","\n","Run 9 out of 10\n","state_dim 10\n","epoch 9000/10000, policy loss 2.6298612283426337e-06\tepoch 0/100 return: -91.0\n","epoch 10/100 return: -87.0\n","epoch 20/100 return: -76.0\n","epoch 30/100 return: -84.0\n","epoch 40/100 return: -75.0\n","epoch 50/100 return: -73.0\n","epoch 60/100 return: -77.0\n","epoch 70/100 return: -75.0\n","epoch 80/100 return: -90.0\n","epoch 90/100 return: -78.0\n","###############    Reward for test environment for run 9: -85.22.   ###############\n","\n","\n","Run 10 out of 10\n","state_dim 10\n","epoch 9000/10000, policy loss 5.047722879680805e-07\tepoch 0/100 return: -90.0\n","epoch 10/100 return: -86.0\n","epoch 20/100 return: -78.0\n","epoch 30/100 return: -63.0\n","epoch 40/100 return: -80.0\n","epoch 50/100 return: -92.0\n","epoch 60/100 return: -79.0\n","epoch 70/100 return: -72.0\n","epoch 80/100 return: -79.0\n","epoch 90/100 return: -74.0\n","###############    Reward for test environment for run 10: -84.17.   ###############\n","\n","\n","Average reward for 10 repetitions: -84.555\n","ALL RESULTS TRAIL: [-84.555]\n","Config: {'ENV': 'Acrobot-v1', 'ALG': 'FINAL_Apr24_BCStudent_noconfound_origindata_trajnum8', 'NUM_TRAJS_GIVEN': 8, 'NUM_TRAINING_ENVS': 2, 'NOISE_DIM': 4, 'REP_SIZE': 16, 'TRAJ_SHIFT': 8, 'SAMPLING_RATE': 5, 'NUM_STEPS_TRAIN': 10000, 'NUM_TRAJS_VALID': 100, 'NUM_REPETITIONS': 10, 'BATCH_SIZE': 64, 'MLP_WIDTHS': 64, 'ADAM_ALPHA': 0.001, 'SGLD_BUFFER_SIZE': 10000, 'SGLD_LEARN_RATE': 0.01, 'SGLD_NOISE_COEF': 0.01, 'SGLD_NUM_STEPS': 100, 'SGLD_REINIT_FREQ': 0.05, 'NUM_STEPS_TRAIN_ENERGY_MODEL': 1000, 'TRIAL': 0, 'METHOD': 'BC-noconfound', 'EXPERT_ALG': 'dqn'}\n","Trial number 0\n","config method =  BC-noconfound\n","config env =  Acrobot-v1\n","Run 1 out of 10\n","state_dim 10\n","epoch 9000/10000, policy loss 6.87310830471688e-07\tepoch 0/100 return: -73.0\n","epoch 10/100 return: -205.0\n","epoch 20/100 return: -163.0\n","epoch 30/100 return: -72.0\n","epoch 40/100 return: -91.0\n","epoch 50/100 return: -65.0\n","epoch 60/100 return: -78.0\n","epoch 70/100 return: -87.0\n","epoch 80/100 return: -93.0\n","epoch 90/100 return: -64.0\n","###############    Reward for test environment for run 1: -87.4.   ###############\n","\n","\n","Run 2 out of 10\n","state_dim 10\n","epoch 9000/10000, policy loss 8.527397767466027e-06\tepoch 0/100 return: -92.0\n","epoch 10/100 return: -72.0\n","epoch 20/100 return: -85.0\n","epoch 30/100 return: -74.0\n","epoch 40/100 return: -80.0\n","epoch 50/100 return: -65.0\n","epoch 60/100 return: -96.0\n","epoch 70/100 return: -78.0\n","epoch 80/100 return: -73.0\n","epoch 90/100 return: -64.0\n","###############    Reward for test environment for run 2: -86.45.   ###############\n","\n","\n","Run 3 out of 10\n","state_dim 10\n","epoch 9000/10000, policy loss 0.000177817972144112\tepoch 0/100 return: -66.0\n","epoch 10/100 return: -81.0\n","epoch 20/100 return: -72.0\n","epoch 30/100 return: -84.0\n","epoch 40/100 return: -64.0\n","epoch 50/100 return: -92.0\n","epoch 60/100 return: -75.0\n","epoch 70/100 return: -72.0\n","epoch 80/100 return: -79.0\n","epoch 90/100 return: -74.0\n","###############    Reward for test environment for run 3: -79.08.   ###############\n","\n","\n","Run 4 out of 10\n","state_dim 10\n","epoch 9000/10000, policy loss 9.40761310630478e-06\tepoch 0/100 return: -88.0\n","epoch 10/100 return: -94.0\n","epoch 20/100 return: -78.0\n","epoch 30/100 return: -72.0\n","epoch 40/100 return: -84.0\n","epoch 50/100 return: -78.0\n","epoch 60/100 return: -74.0\n","epoch 70/100 return: -75.0\n","epoch 80/100 return: -65.0\n","epoch 90/100 return: -69.0\n","###############    Reward for test environment for run 4: -84.79.   ###############\n","\n","\n","Run 5 out of 10\n","state_dim 10\n","epoch 9000/10000, policy loss 1.0095426432599197e-06\tepoch 0/100 return: -87.0\n","epoch 10/100 return: -87.0\n","epoch 20/100 return: -78.0\n","epoch 30/100 return: -77.0\n","epoch 40/100 return: -86.0\n","epoch 50/100 return: -92.0\n","epoch 60/100 return: -85.0\n","epoch 70/100 return: -90.0\n","epoch 80/100 return: -89.0\n","epoch 90/100 return: -78.0\n","###############    Reward for test environment for run 5: -84.1.   ###############\n","\n","\n","Run 6 out of 10\n","state_dim 10\n","epoch 9000/10000, policy loss 3.203732887868682e-07\tepoch 0/100 return: -84.0\n","epoch 10/100 return: -92.0\n","epoch 20/100 return: -85.0\n","epoch 30/100 return: -99.0\n","epoch 40/100 return: -74.0\n","epoch 50/100 return: -84.0\n","epoch 60/100 return: -83.0\n","epoch 70/100 return: -85.0\n","epoch 80/100 return: -83.0\n","epoch 90/100 return: -86.0\n","###############    Reward for test environment for run 6: -86.5.   ###############\n","\n","\n","Run 7 out of 10\n","state_dim 10\n","epoch 9000/10000, policy loss 1.1982699106738437e-05\tepoch 0/100 return: -85.0\n","epoch 10/100 return: -84.0\n","epoch 20/100 return: -91.0\n","epoch 30/100 return: -83.0\n","epoch 40/100 return: -93.0\n","epoch 50/100 return: -77.0\n","epoch 60/100 return: -78.0\n","epoch 70/100 return: -90.0\n","epoch 80/100 return: -89.0\n","epoch 90/100 return: -92.0\n","###############    Reward for test environment for run 7: -82.87.   ###############\n","\n","\n","Run 8 out of 10\n","state_dim 10\n","epoch 9000/10000, policy loss 4.4887183321407065e-06\tepoch 0/100 return: -73.0\n","epoch 10/100 return: -80.0\n","epoch 20/100 return: -81.0\n","epoch 30/100 return: -70.0\n","epoch 40/100 return: -72.0\n","epoch 50/100 return: -78.0\n","epoch 60/100 return: -77.0\n","epoch 70/100 return: -115.0\n","epoch 80/100 return: -80.0\n","epoch 90/100 return: -78.0\n","###############    Reward for test environment for run 8: -85.29.   ###############\n","\n","\n","Run 9 out of 10\n","state_dim 10\n","epoch 9000/10000, policy loss 1.7815751562011428e-05\tepoch 0/100 return: -80.0\n","epoch 10/100 return: -101.0\n","epoch 20/100 return: -79.0\n","epoch 30/100 return: -89.0\n","epoch 40/100 return: -79.0\n","epoch 50/100 return: -94.0\n","epoch 60/100 return: -72.0\n","epoch 70/100 return: -92.0\n","epoch 80/100 return: -77.0\n","epoch 90/100 return: -89.0\n","###############    Reward for test environment for run 9: -83.87.   ###############\n","\n","\n","Run 10 out of 10\n","state_dim 10\n","epoch 9000/10000, policy loss 1.0804566045408137e-05\tepoch 0/100 return: -78.0\n","epoch 10/100 return: -69.0\n","epoch 20/100 return: -83.0\n","epoch 30/100 return: -71.0\n","epoch 40/100 return: -87.0\n","epoch 50/100 return: -88.0\n","epoch 60/100 return: -70.0\n","epoch 70/100 return: -72.0\n","epoch 80/100 return: -69.0\n","epoch 90/100 return: -89.0\n","###############    Reward for test environment for run 10: -79.98.   ###############\n","\n","\n","Average reward for 10 repetitions: -84.033\n","ALL RESULTS TRAIL: [-84.033]\n","Config: {'ENV': 'Acrobot-v1', 'ALG': 'FINAL_Apr24_BCStudent_noconfound_origindata_trajnum9', 'NUM_TRAJS_GIVEN': 9, 'NUM_TRAINING_ENVS': 2, 'NOISE_DIM': 4, 'REP_SIZE': 16, 'TRAJ_SHIFT': 9, 'SAMPLING_RATE': 5, 'NUM_STEPS_TRAIN': 10000, 'NUM_TRAJS_VALID': 100, 'NUM_REPETITIONS': 10, 'BATCH_SIZE': 64, 'MLP_WIDTHS': 64, 'ADAM_ALPHA': 0.001, 'SGLD_BUFFER_SIZE': 10000, 'SGLD_LEARN_RATE': 0.01, 'SGLD_NOISE_COEF': 0.01, 'SGLD_NUM_STEPS': 100, 'SGLD_REINIT_FREQ': 0.05, 'NUM_STEPS_TRAIN_ENERGY_MODEL': 1000, 'TRIAL': 0, 'METHOD': 'BC-noconfound', 'EXPERT_ALG': 'dqn'}\n","Trial number 0\n","config method =  BC-noconfound\n","config env =  Acrobot-v1\n","Run 1 out of 10\n","state_dim 10\n","epoch 9000/10000, policy loss 2.6712446924648248e-05\tepoch 0/100 return: -98.0\n","epoch 10/100 return: -86.0\n","epoch 20/100 return: -73.0\n","epoch 30/100 return: -73.0\n","epoch 40/100 return: -74.0\n","epoch 50/100 return: -73.0\n","epoch 60/100 return: -288.0\n","epoch 70/100 return: -70.0\n","epoch 80/100 return: -78.0\n","epoch 90/100 return: -65.0\n","###############    Reward for test environment for run 1: -82.47.   ###############\n","\n","\n","Run 2 out of 10\n","state_dim 10\n","epoch 9000/10000, policy loss 3.4381844216113677e-06\tepoch 0/100 return: -89.0\n","epoch 10/100 return: -104.0\n","epoch 20/100 return: -86.0\n","epoch 30/100 return: -82.0\n","epoch 40/100 return: -86.0\n","epoch 50/100 return: -94.0\n","epoch 60/100 return: -110.0\n","epoch 70/100 return: -85.0\n","epoch 80/100 return: -65.0\n","epoch 90/100 return: -82.0\n","###############    Reward for test environment for run 2: -86.77.   ###############\n","\n","\n","Run 3 out of 10\n","state_dim 10\n","epoch 9000/10000, policy loss 0.006497143767774105\tepoch 0/100 return: -89.0\n","epoch 10/100 return: -71.0\n","epoch 20/100 return: -85.0\n","epoch 30/100 return: -64.0\n","epoch 40/100 return: -74.0\n","epoch 50/100 return: -79.0\n","epoch 60/100 return: -115.0\n","epoch 70/100 return: -85.0\n","epoch 80/100 return: -79.0\n","epoch 90/100 return: -73.0\n","###############    Reward for test environment for run 3: -82.18.   ###############\n","\n","\n","Run 4 out of 10\n","state_dim 10\n","epoch 9000/10000, policy loss 2.4787394067971036e-05\tepoch 0/100 return: -65.0\n","epoch 10/100 return: -89.0\n","epoch 20/100 return: -76.0\n","epoch 30/100 return: -78.0\n","epoch 40/100 return: -93.0\n","epoch 50/100 return: -76.0\n","epoch 60/100 return: -84.0\n","epoch 70/100 return: -84.0\n","epoch 80/100 return: -85.0\n","epoch 90/100 return: -64.0\n","###############    Reward for test environment for run 4: -82.43.   ###############\n","\n","\n","Run 5 out of 10\n","state_dim 10\n","epoch 9000/10000, policy loss 9.14448537514545e-06\tepoch 0/100 return: -116.0\n","epoch 10/100 return: -77.0\n","epoch 20/100 return: -106.0\n","epoch 30/100 return: -78.0\n","epoch 40/100 return: -79.0\n","epoch 50/100 return: -71.0\n","epoch 60/100 return: -83.0\n","epoch 70/100 return: -97.0\n","epoch 80/100 return: -79.0\n","epoch 90/100 return: -90.0\n","###############    Reward for test environment for run 5: -82.51.   ###############\n","\n","\n","Run 6 out of 10\n","state_dim 10\n","epoch 9000/10000, policy loss 1.2274649634491652e-06\tepoch 0/100 return: -86.0\n","epoch 10/100 return: -66.0\n","epoch 20/100 return: -66.0\n","epoch 30/100 return: -78.0\n","epoch 40/100 return: -93.0\n","epoch 50/100 return: -91.0\n","epoch 60/100 return: -99.0\n","epoch 70/100 return: -213.0\n","epoch 80/100 return: -76.0\n","epoch 90/100 return: -65.0\n","###############    Reward for test environment for run 6: -90.74.   ###############\n","\n","\n","Run 7 out of 10\n","state_dim 10\n","epoch 9000/10000, policy loss 1.1890061614394654e-05\tepoch 0/100 return: -77.0\n","epoch 10/100 return: -83.0\n","epoch 20/100 return: -87.0\n","epoch 30/100 return: -122.0\n","epoch 40/100 return: -78.0\n","epoch 50/100 return: -78.0\n","epoch 60/100 return: -226.0\n","epoch 70/100 return: -92.0\n","epoch 80/100 return: -77.0\n","epoch 90/100 return: -163.0\n","###############    Reward for test environment for run 7: -83.27.   ###############\n","\n","\n","Run 8 out of 10\n","state_dim 10\n","epoch 9000/10000, policy loss 1.7956945157493465e-05\tepoch 0/100 return: -77.0\n","epoch 10/100 return: -98.0\n","epoch 20/100 return: -100.0\n","epoch 30/100 return: -90.0\n","epoch 40/100 return: -77.0\n","epoch 50/100 return: -77.0\n","epoch 60/100 return: -90.0\n","epoch 70/100 return: -78.0\n","epoch 80/100 return: -72.0\n","epoch 90/100 return: -78.0\n","###############    Reward for test environment for run 8: -82.59.   ###############\n","\n","\n","Run 9 out of 10\n","state_dim 10\n","epoch 9000/10000, policy loss 0.0033646090887486935\tepoch 0/100 return: -89.0\n","epoch 10/100 return: -84.0\n","epoch 20/100 return: -500.0\n","epoch 30/100 return: -65.0\n","epoch 40/100 return: -86.0\n","epoch 50/100 return: -79.0\n","epoch 60/100 return: -78.0\n","epoch 70/100 return: -72.0\n","epoch 80/100 return: -72.0\n","epoch 90/100 return: -72.0\n","###############    Reward for test environment for run 9: -85.84.   ###############\n","\n","\n","Run 10 out of 10\n","state_dim 10\n","epoch 9000/10000, policy loss 3.4645074720174307e-07\tepoch 0/100 return: -90.0\n","epoch 10/100 return: -77.0\n","epoch 20/100 return: -65.0\n","epoch 30/100 return: -90.0\n","epoch 40/100 return: -88.0\n","epoch 50/100 return: -73.0\n","epoch 60/100 return: -72.0\n","epoch 70/100 return: -131.0\n","epoch 80/100 return: -84.0\n","epoch 90/100 return: -78.0\n","###############    Reward for test environment for run 10: -81.9.   ###############\n","\n","\n","Average reward for 10 repetitions: -84.07000000000001\n","ALL RESULTS TRAIL: [-84.07000000000001]\n","Config: {'ENV': 'Acrobot-v1', 'ALG': 'FINAL_Apr24_BCStudent_noconfound_origindata_trajnum10', 'NUM_TRAJS_GIVEN': 10, 'NUM_TRAINING_ENVS': 2, 'NOISE_DIM': 4, 'REP_SIZE': 16, 'TRAJ_SHIFT': 10, 'SAMPLING_RATE': 5, 'NUM_STEPS_TRAIN': 10000, 'NUM_TRAJS_VALID': 100, 'NUM_REPETITIONS': 10, 'BATCH_SIZE': 64, 'MLP_WIDTHS': 64, 'ADAM_ALPHA': 0.001, 'SGLD_BUFFER_SIZE': 10000, 'SGLD_LEARN_RATE': 0.01, 'SGLD_NOISE_COEF': 0.01, 'SGLD_NUM_STEPS': 100, 'SGLD_REINIT_FREQ': 0.05, 'NUM_STEPS_TRAIN_ENERGY_MODEL': 1000, 'TRIAL': 0, 'METHOD': 'BC-noconfound', 'EXPERT_ALG': 'dqn'}\n","Trial number 0\n","config method =  BC-noconfound\n","config env =  Acrobot-v1\n","Run 1 out of 10\n","state_dim 10\n","epoch 9000/10000, policy loss 2.300314008607529e-06\tepoch 0/100 return: -79.0\n","epoch 10/100 return: -64.0\n","epoch 20/100 return: -76.0\n","epoch 30/100 return: -85.0\n","epoch 40/100 return: -65.0\n","epoch 50/100 return: -70.0\n","epoch 60/100 return: -71.0\n","epoch 70/100 return: -66.0\n","epoch 80/100 return: -74.0\n","epoch 90/100 return: -73.0\n","###############    Reward for test environment for run 1: -85.68.   ###############\n","\n","\n","Run 2 out of 10\n","state_dim 10\n","epoch 9000/10000, policy loss 0.0006355413934215903\tepoch 0/100 return: -79.0\n","epoch 10/100 return: -77.0\n","epoch 20/100 return: -64.0\n","epoch 30/100 return: -82.0\n","epoch 40/100 return: -65.0\n","epoch 50/100 return: -79.0\n","epoch 60/100 return: -90.0\n","epoch 70/100 return: -90.0\n","epoch 80/100 return: -89.0\n","epoch 90/100 return: -88.0\n","###############    Reward for test environment for run 2: -79.43.   ###############\n","\n","\n","Run 3 out of 10\n","state_dim 10\n","epoch 9000/10000, policy loss 9.927134669851512e-06\tepoch 0/100 return: -156.0\n","epoch 10/100 return: -74.0\n","epoch 20/100 return: -72.0\n","epoch 30/100 return: -72.0\n","epoch 40/100 return: -74.0\n","epoch 50/100 return: -72.0\n","epoch 60/100 return: -64.0\n","epoch 70/100 return: -72.0\n","epoch 80/100 return: -65.0\n","epoch 90/100 return: -84.0\n","###############    Reward for test environment for run 3: -81.82.   ###############\n","\n","\n","Run 4 out of 10\n","state_dim 10\n","epoch 9000/10000, policy loss 9.930892701959237e-06\tepoch 0/100 return: -84.0\n","epoch 10/100 return: -86.0\n","epoch 20/100 return: -78.0\n","epoch 30/100 return: -90.0\n","epoch 40/100 return: -81.0\n","epoch 50/100 return: -88.0\n","epoch 60/100 return: -84.0\n","epoch 70/100 return: -84.0\n","epoch 80/100 return: -70.0\n","epoch 90/100 return: -84.0\n","###############    Reward for test environment for run 4: -86.4.   ###############\n","\n","\n","Run 5 out of 10\n","state_dim 10\n","epoch 9000/10000, policy loss 3.838688826363068e-06\tepoch 0/100 return: -72.0\n","epoch 10/100 return: -73.0\n","epoch 20/100 return: -145.0\n","epoch 30/100 return: -95.0\n","epoch 40/100 return: -90.0\n","epoch 50/100 return: -76.0\n","epoch 60/100 return: -69.0\n","epoch 70/100 return: -64.0\n","epoch 80/100 return: -72.0\n","epoch 90/100 return: -85.0\n","###############    Reward for test environment for run 5: -84.14.   ###############\n","\n","\n","Run 6 out of 10\n","state_dim 10\n","epoch 9000/10000, policy loss 3.2593970900052227e-06\tepoch 0/100 return: -86.0\n","epoch 10/100 return: -152.0\n","epoch 20/100 return: -71.0\n","epoch 30/100 return: -64.0\n","epoch 40/100 return: -93.0\n","epoch 50/100 return: -72.0\n","epoch 60/100 return: -77.0\n","epoch 70/100 return: -72.0\n","epoch 80/100 return: -77.0\n","epoch 90/100 return: -69.0\n","###############    Reward for test environment for run 6: -85.38.   ###############\n","\n","\n","Run 7 out of 10\n","state_dim 10\n","epoch 9000/10000, policy loss 2.1753276087110862e-05\tepoch 0/100 return: -64.0\n","epoch 10/100 return: -90.0\n","epoch 20/100 return: -64.0\n","epoch 30/100 return: -78.0\n","epoch 40/100 return: -77.0\n","epoch 50/100 return: -71.0\n","epoch 60/100 return: -78.0\n","epoch 70/100 return: -126.0\n","epoch 80/100 return: -90.0\n","epoch 90/100 return: -84.0\n","###############    Reward for test environment for run 7: -81.69.   ###############\n","\n","\n","Run 8 out of 10\n","state_dim 10\n","epoch 9000/10000, policy loss 5.335676542017609e-05\tepoch 0/100 return: -79.0\n","epoch 10/100 return: -90.0\n","epoch 20/100 return: -93.0\n","epoch 30/100 return: -96.0\n","epoch 40/100 return: -89.0\n","epoch 50/100 return: -86.0\n","epoch 60/100 return: -85.0\n","epoch 70/100 return: -73.0\n","epoch 80/100 return: -84.0\n","epoch 90/100 return: -85.0\n","###############    Reward for test environment for run 8: -90.33.   ###############\n","\n","\n","Run 9 out of 10\n","state_dim 10\n","epoch 9000/10000, policy loss 6.891740440551075e-07\tepoch 0/100 return: -83.0\n","epoch 10/100 return: -86.0\n","epoch 20/100 return: -71.0\n","epoch 30/100 return: -80.0\n","epoch 40/100 return: -78.0\n","epoch 50/100 return: -116.0\n","epoch 60/100 return: -78.0\n","epoch 70/100 return: -77.0\n","epoch 80/100 return: -65.0\n","epoch 90/100 return: -91.0\n","###############    Reward for test environment for run 9: -84.22.   ###############\n","\n","\n","Run 10 out of 10\n","state_dim 10\n","epoch 9000/10000, policy loss 2.45823339355411e-05\tepoch 0/100 return: -202.0\n","epoch 10/100 return: -72.0\n","epoch 20/100 return: -91.0\n","epoch 30/100 return: -82.0\n","epoch 40/100 return: -77.0\n","epoch 50/100 return: -138.0\n","epoch 60/100 return: -96.0\n","epoch 70/100 return: -110.0\n","epoch 80/100 return: -93.0\n","epoch 90/100 return: -83.0\n","###############    Reward for test environment for run 10: -95.39.   ###############\n","\n","\n","Average reward for 10 repetitions: -85.44800000000001\n","ALL RESULTS TRAIL: [-85.44800000000001]\n","Config: {'ENV': 'Acrobot-v1', 'ALG': 'FINAL_Apr24_BCStudent_noconfound_origindata_trajnum20', 'NUM_TRAJS_GIVEN': 20, 'NUM_TRAINING_ENVS': 2, 'NOISE_DIM': 4, 'REP_SIZE': 16, 'TRAJ_SHIFT': 20, 'SAMPLING_RATE': 5, 'NUM_STEPS_TRAIN': 10000, 'NUM_TRAJS_VALID': 100, 'NUM_REPETITIONS': 10, 'BATCH_SIZE': 64, 'MLP_WIDTHS': 64, 'ADAM_ALPHA': 0.001, 'SGLD_BUFFER_SIZE': 10000, 'SGLD_LEARN_RATE': 0.01, 'SGLD_NOISE_COEF': 0.01, 'SGLD_NUM_STEPS': 100, 'SGLD_REINIT_FREQ': 0.05, 'NUM_STEPS_TRAIN_ENERGY_MODEL': 1000, 'TRIAL': 0, 'METHOD': 'BC-noconfound', 'EXPERT_ALG': 'dqn'}\n","Trial number 0\n","config method =  BC-noconfound\n","config env =  Acrobot-v1\n","Run 1 out of 10\n","state_dim 10\n","epoch 9000/10000, policy loss 0.01611904986202717\tepoch 0/100 return: -87.0\n","epoch 10/100 return: -82.0\n","epoch 20/100 return: -77.0\n","epoch 30/100 return: -77.0\n","epoch 40/100 return: -92.0\n","epoch 50/100 return: -74.0\n","epoch 60/100 return: -91.0\n","epoch 70/100 return: -85.0\n","epoch 80/100 return: -78.0\n","epoch 90/100 return: -78.0\n","###############    Reward for test environment for run 1: -84.89.   ###############\n","\n","\n","Run 2 out of 10\n","state_dim 10\n","epoch 9000/10000, policy loss 0.03390653431415558\tepoch 0/100 return: -77.0\n","epoch 10/100 return: -84.0\n","epoch 20/100 return: -73.0\n","epoch 30/100 return: -97.0\n","epoch 40/100 return: -76.0\n","epoch 50/100 return: -112.0\n","epoch 60/100 return: -112.0\n","epoch 70/100 return: -99.0\n","epoch 80/100 return: -87.0\n","epoch 90/100 return: -74.0\n","###############    Reward for test environment for run 2: -84.63.   ###############\n","\n","\n","Run 3 out of 10\n","state_dim 10\n","epoch 9000/10000, policy loss 0.0033050505444407463\tepoch 0/100 return: -88.0\n","epoch 10/100 return: -127.0\n","epoch 20/100 return: -80.0\n","epoch 30/100 return: -79.0\n","epoch 40/100 return: -90.0\n","epoch 50/100 return: -78.0\n","epoch 60/100 return: -85.0\n","epoch 70/100 return: -84.0\n","epoch 80/100 return: -82.0\n","epoch 90/100 return: -79.0\n","###############    Reward for test environment for run 3: -83.57.   ###############\n","\n","\n","Run 4 out of 10\n","state_dim 10\n","epoch 9000/10000, policy loss 0.000987996463663876\tepoch 0/100 return: -78.0\n","epoch 10/100 return: -70.0\n","epoch 20/100 return: -89.0\n","epoch 30/100 return: -79.0\n","epoch 40/100 return: -71.0\n","epoch 50/100 return: -71.0\n","epoch 60/100 return: -73.0\n","epoch 70/100 return: -159.0\n","epoch 80/100 return: -117.0\n","epoch 90/100 return: -73.0\n","###############    Reward for test environment for run 4: -84.32.   ###############\n","\n","\n","Run 5 out of 10\n","state_dim 10\n","epoch 9000/10000, policy loss 0.0012972147669643164\tepoch 0/100 return: -77.0\n","epoch 10/100 return: -83.0\n","epoch 20/100 return: -76.0\n","epoch 30/100 return: -87.0\n","epoch 40/100 return: -84.0\n","epoch 50/100 return: -77.0\n","epoch 60/100 return: -77.0\n","epoch 70/100 return: -96.0\n","epoch 80/100 return: -84.0\n","epoch 90/100 return: -83.0\n","###############    Reward for test environment for run 5: -85.56.   ###############\n","\n","\n","Run 6 out of 10\n","state_dim 10\n","epoch 9000/10000, policy loss 0.10056012868881226\tepoch 0/100 return: -90.0\n","epoch 10/100 return: -72.0\n","epoch 20/100 return: -72.0\n","epoch 30/100 return: -88.0\n","epoch 40/100 return: -82.0\n","epoch 50/100 return: -77.0\n","epoch 60/100 return: -69.0\n","epoch 70/100 return: -85.0\n","epoch 80/100 return: -71.0\n","epoch 90/100 return: -98.0\n","###############    Reward for test environment for run 6: -83.85.   ###############\n","\n","\n","Run 7 out of 10\n","state_dim 10\n","epoch 9000/10000, policy loss 0.021982342004776\tepoch 0/100 return: -85.0\n","epoch 10/100 return: -75.0\n","epoch 20/100 return: -78.0\n","epoch 30/100 return: -72.0\n","epoch 40/100 return: -92.0\n","epoch 50/100 return: -79.0\n","epoch 60/100 return: -78.0\n","epoch 70/100 return: -76.0\n","epoch 80/100 return: -71.0\n","epoch 90/100 return: -75.0\n","###############    Reward for test environment for run 7: -89.47.   ###############\n","\n","\n","Run 8 out of 10\n","state_dim 10\n","epoch 9000/10000, policy loss 0.009670337662100792\tepoch 0/100 return: -87.0\n","epoch 10/100 return: -78.0\n","epoch 20/100 return: -77.0\n","epoch 30/100 return: -84.0\n","epoch 40/100 return: -84.0\n","epoch 50/100 return: -78.0\n","epoch 60/100 return: -83.0\n","epoch 70/100 return: -78.0\n","epoch 80/100 return: -74.0\n","epoch 90/100 return: -90.0\n","###############    Reward for test environment for run 8: -81.42.   ###############\n","\n","\n","Run 9 out of 10\n","state_dim 10\n","epoch 9000/10000, policy loss 0.0963728129863739\tepoch 0/100 return: -77.0\n","epoch 10/100 return: -78.0\n","epoch 20/100 return: -73.0\n","epoch 30/100 return: -84.0\n","epoch 40/100 return: -77.0\n","epoch 50/100 return: -78.0\n","epoch 60/100 return: -162.0\n","epoch 70/100 return: -78.0\n","epoch 80/100 return: -71.0\n","epoch 90/100 return: -77.0\n","###############    Reward for test environment for run 9: -86.86.   ###############\n","\n","\n","Run 10 out of 10\n","state_dim 10\n","epoch 9000/10000, policy loss 0.07511892914772034\tepoch 0/100 return: -76.0\n","epoch 10/100 return: -148.0\n","epoch 20/100 return: -78.0\n","epoch 30/100 return: -86.0\n","epoch 40/100 return: -98.0\n","epoch 50/100 return: -78.0\n","epoch 60/100 return: -78.0\n","epoch 70/100 return: -79.0\n","epoch 80/100 return: -77.0\n","epoch 90/100 return: -85.0\n","###############    Reward for test environment for run 10: -83.32.   ###############\n","\n","\n","Average reward for 10 repetitions: -84.78899999999999\n","ALL RESULTS TRAIL: [-84.78899999999999]\n"]}]},{"cell_type":"code","source":[""],"metadata":{"id":"RZsZZEezony5"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["#10 Trails -- BC -- lunarlander"],"metadata":{"id":"rmZ8HxHvooip"}},{"cell_type":"code","source":["config['METHOD'] = \"BC-noconfound\"\n","config['ENV'] = 'LunarLander-v2'\n","\n","for traj_num in [2, 4, 8, 16, 32, 64, 128]:\n","    config[\"NUM_TRAJS_GIVEN\"] = traj_num\n","    config[\"TRAJ_SHIFT\"] = traj_num\n","\n","\n","    config['ALG'] = \"FINAL_Apr24_BCStudent_noconfound_origindata_trajnum\" + str(traj_num)\n","\n","\n","    ###############.  settings   ###############\n","    #config['ALG'] = \"BCStudent_Apr19_replicatedata\"\n","    #config['METHOD'] = \"BC\"\n","    #config['ENV'] == \"CartPole-v1\"\n","    #config['ENV'] == \"LunarLander-v2\"\n","    #config[\"NUM_TRAJS_GIVEN\"] = 20\n","    #config[\"TRAJ_SHIFT\"] = 20\n","    ###############.  settings   ###############\n","\n","\n","\n","    if config['METHOD'] == 'BCIRM':\n","        config['l2_regularizer_weight'] = 0.001\n","        config['penalty_weight'] = 10000\n","        config['penalty_anneal_iters'] = 100\n","\n","    all_results_trail = []\n","\n","    for trail in range(1): \n","        config['TRIAL'] = trail \n","\n","\n","        ###############.  start a trail   ###############\n","\n","        config[\"EXPERT_ALG\"] = yaml.load(open(\"testing/config.yml\"), Loader=yaml.FullLoader)[config[\"ENV\"]]\n","        print(\"Config: %s\" % config)\n","\n","        TRIAL = config[\"TRIAL\"] #args.trial\n","        print(\"Trial number %s\" % TRIAL)\n","\n","        results_dir_base = \"testing/results/\"\n","        results_dir = os.path.join(results_dir_base, config[\"ENV\"], str(config[\"NUM_TRAJS_GIVEN\"]), config[\"ALG\"])\n","\n","        if not os.path.exists(results_dir):\n","            os.makedirs(results_dir)\n","\n","        config_file = \"trial_\" + str(TRIAL) + \"_\" + \"config.pkl\"\n","\n","        results_file_name = \"trial_\" + str(TRIAL) + \"_\" + \"results.csv\"\n","        results_file_path = os.path.join(results_dir, results_file_name)\n","\n","        if os.path.exists(os.path.join(results_dir, config_file)):\n","            raise NameError(\"CONFIG file already exists %s. Choose a different trial number.\" % config_file)\n","        pickle.dump(config, open(os.path.join(results_dir, config_file), \"wb\"))\n","\n","\n","\n","\n","        ###############.  10 runs for each trail   ###############\n","\n","        print(\"config method = \", config['METHOD'])\n","        print(\"config env = \", config['ENV'])\n","\n","        for run_seed in range(config[\"NUM_REPETITIONS\"]):\n","            print(\"Run %s out of %s\" % (run_seed + 1, config[\"NUM_REPETITIONS\"]))\n","            student = make_student(run_seed, config)\n","            student.train(num_updates=config[\"NUM_STEPS_TRAIN\"])\n","\n","            env_wrapper_out_of_sample = EnvWrapper(\n","                env=gym.make(config[\"ENV\"]), mult_factor=get_test_mult_factors(config['NOISE_DIM'] - 1), idx=3, seed=1\n","            )\n","            action_match, return_mean, return_std = student.test(\n","                num_episodes=config[\"NUM_TRAJS_VALID\"], env_wrapper=env_wrapper_out_of_sample\n","            )\n","\n","            result = (action_match, return_mean, return_std)\n","            print(\"###############    Reward for test environment for run %s: %s.   ###############\\n\\n\" % (run_seed + 1, return_mean))\n","            save_results(results_file_path, run_seed, action_match, return_mean, return_std)\n","\n","        results_trial = pd.read_csv(\n","            \"testing/results/\"\n","            + config[\"ENV\"]\n","            + \"/\"\n","            + str(config[\"NUM_TRAJS_GIVEN\"])\n","            + \"/\"\n","            + config[\"ALG\"]\n","            + \"/trial_\"\n","            + str(TRIAL)\n","            + \"_results.csv\",\n","            header=None,\n","        )\n","\n","        print(\"Average reward for 10 repetitions: %s\" % np.mean(results_trial[2].values))\n","\n","        all_results_trail.append(np.mean(results_trial[2].values))\n","\n","    print(\"ALL RESULTS TRAIL:\" , all_results_trail)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"outputId":"47c28b2e-8a77-4de3-ac1b-d6f89992a7af","id":"4iIFxpDPooiq","executionInfo":{"status":"ok","timestamp":1650843548207,"user_tz":240,"elapsed":5008831,"user":{"displayName":"2 Lin","userId":"15119774230930098070"}}},"execution_count":20,"outputs":[{"output_type":"stream","name":"stdout","text":["Config: {'ENV': 'LunarLander-v2', 'ALG': 'FINAL_Apr24_BCStudent_noconfound_origindata_trajnum2', 'NUM_TRAJS_GIVEN': 2, 'NUM_TRAINING_ENVS': 2, 'NOISE_DIM': 4, 'REP_SIZE': 16, 'TRAJ_SHIFT': 2, 'SAMPLING_RATE': 5, 'NUM_STEPS_TRAIN': 10000, 'NUM_TRAJS_VALID': 100, 'NUM_REPETITIONS': 10, 'BATCH_SIZE': 64, 'MLP_WIDTHS': 64, 'ADAM_ALPHA': 0.001, 'SGLD_BUFFER_SIZE': 10000, 'SGLD_LEARN_RATE': 0.01, 'SGLD_NOISE_COEF': 0.01, 'SGLD_NUM_STEPS': 100, 'SGLD_REINIT_FREQ': 0.05, 'NUM_STEPS_TRAIN_ENERGY_MODEL': 1000, 'TRIAL': 0, 'METHOD': 'BC-noconfound', 'EXPERT_ALG': 'dqn'}\n","Trial number 0\n","config method =  BC-noconfound\n","config env =  LunarLander-v2\n","Run 1 out of 10\n","state_dim 12\n","epoch 9000/10000, policy loss 1.983453330467455e-05\tepoch 0/100 return: -28.782644948341\n","epoch 10/100 return: 8.492385935518541\n","epoch 20/100 return: -186.11793464344396\n","epoch 30/100 return: 252.22785153995096\n","epoch 40/100 return: 231.16537723827852\n","epoch 50/100 return: -38.16856126846257\n","epoch 60/100 return: -45.53362865233065\n","epoch 70/100 return: 268.5247461247171\n","epoch 80/100 return: -192.86148706384898\n","epoch 90/100 return: -142.14897935149855\n","###############    Reward for test environment for run 1: -46.734990689693674.   ###############\n","\n","\n","Run 2 out of 10\n","state_dim 12\n","epoch 9000/10000, policy loss 2.4455965103697963e-05\tepoch 0/100 return: -265.1465864258544\n","epoch 10/100 return: -306.0096575526967\n","epoch 20/100 return: -287.45457519908723\n","epoch 30/100 return: -300.5527076508994\n","epoch 40/100 return: -363.6921458630926\n","epoch 50/100 return: 209.61216696787477\n","epoch 60/100 return: -139.24236353028897\n","epoch 70/100 return: -223.18697613393402\n","epoch 80/100 return: -173.82280316913437\n","epoch 90/100 return: -308.0087880110657\n","###############    Reward for test environment for run 2: -184.34986958539065.   ###############\n","\n","\n","Run 3 out of 10\n","state_dim 12\n","epoch 9000/10000, policy loss 4.381889084470458e-05\tepoch 0/100 return: 277.74668363262276\n","epoch 10/100 return: -13.78545650443047\n","epoch 20/100 return: -280.0084947994422\n","epoch 30/100 return: -371.31976745140616\n","epoch 40/100 return: -458.6267406244193\n","epoch 50/100 return: 31.189464423458077\n","epoch 60/100 return: -460.99961804254656\n","epoch 70/100 return: -453.9840911265569\n","epoch 80/100 return: -269.55838658951996\n","epoch 90/100 return: -550.5267010957414\n","###############    Reward for test environment for run 3: -366.2766427858046.   ###############\n","\n","\n","Run 4 out of 10\n","state_dim 12\n","epoch 9000/10000, policy loss 3.0067461921134964e-05\tepoch 0/100 return: -0.9047505282141657\n","epoch 10/100 return: -119.9896316061555\n","epoch 20/100 return: -458.7458978460759\n","epoch 30/100 return: -481.84036668666283\n","epoch 40/100 return: -23.309864700308793\n","epoch 50/100 return: -308.0055611863953\n","epoch 60/100 return: -343.9869173016468\n","epoch 70/100 return: -577.4125652497094\n","epoch 80/100 return: -513.4329726003659\n","epoch 90/100 return: -617.2819117177934\n","###############    Reward for test environment for run 4: -315.440809921663.   ###############\n","\n","\n","Run 5 out of 10\n","state_dim 12\n","epoch 9000/10000, policy loss 0.1141875833272934\tepoch 0/100 return: 154.86300047281554\n","epoch 10/100 return: 230.71250299696115\n","epoch 20/100 return: 52.09812871663672\n","epoch 30/100 return: 62.28608411508978\n","epoch 40/100 return: 241.2282218500283\n","epoch 50/100 return: 39.43780707058346\n","epoch 60/100 return: 257.75125287526555\n","epoch 70/100 return: 77.89909174978618\n","epoch 80/100 return: 271.21319486169256\n","epoch 90/100 return: -162.68504492498357\n","###############    Reward for test environment for run 5: 125.42017279531714.   ###############\n","\n","\n","Run 6 out of 10\n","state_dim 12\n","epoch 9000/10000, policy loss 2.8849684895249084e-05\tepoch 0/100 return: -488.24977317442114\n","epoch 10/100 return: -40.03863388017142\n","epoch 20/100 return: -457.036589858442\n","epoch 30/100 return: -401.27427461015435\n","epoch 40/100 return: -230.22073613264078\n","epoch 50/100 return: -163.89224895927651\n","epoch 60/100 return: 12.567060039638747\n","epoch 70/100 return: -464.47221420134855\n","epoch 80/100 return: -254.21442565002863\n","epoch 90/100 return: -389.6858104324216\n","###############    Reward for test environment for run 6: -208.1634877043725.   ###############\n","\n","\n","Run 7 out of 10\n","state_dim 12\n","epoch 9000/10000, policy loss 1.6587071513640694e-05\tepoch 0/100 return: 28.62741679696765\n","epoch 10/100 return: -7.249499285154926\n","epoch 20/100 return: 42.439499841798494\n","epoch 30/100 return: -44.427986740709855\n","epoch 40/100 return: -80.03629539201673\n","epoch 50/100 return: -16.333152185941742\n","epoch 60/100 return: -67.02365130646001\n","epoch 70/100 return: 8.00722624590908\n","epoch 80/100 return: -78.0294389263553\n","epoch 90/100 return: 66.3492183905548\n","###############    Reward for test environment for run 7: 0.7505187635477977.   ###############\n","\n","\n","Run 8 out of 10\n","state_dim 12\n","epoch 9000/10000, policy loss 0.00112263229675591\tepoch 0/100 return: -480.3900024988697\n","epoch 10/100 return: -264.1689068535642\n","epoch 20/100 return: -366.32011821278115\n","epoch 30/100 return: -475.34157400407156\n","epoch 40/100 return: -451.20207028941013\n","epoch 50/100 return: -383.40401575129056\n","epoch 60/100 return: -328.2039048284421\n","epoch 70/100 return: -127.56293728399322\n","epoch 80/100 return: -394.1669572809044\n","epoch 90/100 return: -446.7462873868172\n","###############    Reward for test environment for run 8: -364.2659583915758.   ###############\n","\n","\n","Run 9 out of 10\n","state_dim 12\n","epoch 9000/10000, policy loss 2.1126710635144264e-05\tepoch 0/100 return: -563.7320941470871\n","epoch 10/100 return: -534.5295588323121\n","epoch 20/100 return: -325.0121430803566\n","epoch 30/100 return: -51.51238925527571\n","epoch 40/100 return: -231.8225828973555\n","epoch 50/100 return: -64.69609816741392\n","epoch 60/100 return: -304.15246298028023\n","epoch 70/100 return: -280.86314440013666\n","epoch 80/100 return: -288.5252958608426\n","epoch 90/100 return: -217.84553519167042\n","###############    Reward for test environment for run 9: -214.34721569699715.   ###############\n","\n","\n","Run 10 out of 10\n","state_dim 12\n","epoch 9000/10000, policy loss 7.597604962938931e-06\tepoch 0/100 return: -120.0362961807914\n","epoch 10/100 return: -66.78719766936052\n","epoch 20/100 return: -597.5544257364497\n","epoch 30/100 return: -6.32860618529152\n","epoch 40/100 return: -104.84670313397075\n","epoch 50/100 return: -96.0760934881023\n","epoch 60/100 return: 232.5868508178523\n","epoch 70/100 return: -559.7951641210443\n","epoch 80/100 return: 192.75246439966133\n","epoch 90/100 return: -6.923265573728671\n","###############    Reward for test environment for run 10: -105.47512188807843.   ###############\n","\n","\n","Average reward for 10 repetitions: -167.8883405104711\n","ALL RESULTS TRAIL: [-167.8883405104711]\n","Config: {'ENV': 'LunarLander-v2', 'ALG': 'FINAL_Apr24_BCStudent_noconfound_origindata_trajnum4', 'NUM_TRAJS_GIVEN': 4, 'NUM_TRAINING_ENVS': 2, 'NOISE_DIM': 4, 'REP_SIZE': 16, 'TRAJ_SHIFT': 4, 'SAMPLING_RATE': 5, 'NUM_STEPS_TRAIN': 10000, 'NUM_TRAJS_VALID': 100, 'NUM_REPETITIONS': 10, 'BATCH_SIZE': 64, 'MLP_WIDTHS': 64, 'ADAM_ALPHA': 0.001, 'SGLD_BUFFER_SIZE': 10000, 'SGLD_LEARN_RATE': 0.01, 'SGLD_NOISE_COEF': 0.01, 'SGLD_NUM_STEPS': 100, 'SGLD_REINIT_FREQ': 0.05, 'NUM_STEPS_TRAIN_ENERGY_MODEL': 1000, 'TRIAL': 0, 'METHOD': 'BC-noconfound', 'EXPERT_ALG': 'dqn'}\n","Trial number 0\n","config method =  BC-noconfound\n","config env =  LunarLander-v2\n","Run 1 out of 10\n","state_dim 12\n","epoch 9000/10000, policy loss 0.009957272559404373\tepoch 0/100 return: -417.7396473747757\n","epoch 10/100 return: -269.1402069020744\n","epoch 20/100 return: -250.90779745471576\n","epoch 30/100 return: -179.2012272490776\n","epoch 40/100 return: -485.8948070436144\n","epoch 50/100 return: -461.805095091227\n","epoch 60/100 return: -85.9103766988486\n","epoch 70/100 return: -332.17579156398006\n","epoch 80/100 return: 269.2598353946073\n","epoch 90/100 return: 231.17129739137422\n","###############    Reward for test environment for run 1: -207.39186098282985.   ###############\n","\n","\n","Run 2 out of 10\n","state_dim 12\n","epoch 9000/10000, policy loss 0.000578781240619719\tepoch 0/100 return: -375.8179949927658\n","epoch 10/100 return: -356.36950298579274\n","epoch 20/100 return: -66.44089148606646\n","epoch 30/100 return: 10.815670087601433\n","epoch 40/100 return: 38.59551463093072\n","epoch 50/100 return: 260.9525361576766\n","epoch 60/100 return: 248.3016253970805\n","epoch 70/100 return: -385.7032240799606\n","epoch 80/100 return: -210.16353124800997\n","epoch 90/100 return: -404.89465079563865\n","###############    Reward for test environment for run 2: -165.95324535231882.   ###############\n","\n","\n","Run 3 out of 10\n","state_dim 12\n","epoch 9000/10000, policy loss 0.03596283495426178\tepoch 0/100 return: 304.5172367877559\n","epoch 10/100 return: 247.55972430366984\n","epoch 20/100 return: 276.7474257740081\n","epoch 30/100 return: -261.3832175152147\n","epoch 40/100 return: 263.56265329883615\n","epoch 50/100 return: 283.27659811729706\n","epoch 60/100 return: 221.13836404856985\n","epoch 70/100 return: 282.1175511511975\n","epoch 80/100 return: -237.65100582782887\n","epoch 90/100 return: 255.8458236202801\n","###############    Reward for test environment for run 3: 98.13409054908219.   ###############\n","\n","\n","Run 4 out of 10\n","state_dim 12\n","epoch 9000/10000, policy loss 0.0013655046932399273\tepoch 0/100 return: -380.61813290454756\n","epoch 10/100 return: -33.68165097887545\n","epoch 20/100 return: 238.98987696132735\n","epoch 30/100 return: -49.927821975993126\n","epoch 40/100 return: -479.2625786098461\n","epoch 50/100 return: -433.37980023231944\n","epoch 60/100 return: -73.96406454213061\n","epoch 70/100 return: 20.688870051088003\n","epoch 80/100 return: 306.53986743759754\n","epoch 90/100 return: 44.952033903681325\n","###############    Reward for test environment for run 4: -160.8329822304573.   ###############\n","\n","\n","Run 5 out of 10\n","state_dim 12\n","epoch 9000/10000, policy loss 0.024431012570858\tepoch 0/100 return: -490.7961874673649\n","epoch 10/100 return: -68.64354028642981\n","epoch 20/100 return: 204.97159089866005\n","epoch 30/100 return: -245.7546051585848\n","epoch 40/100 return: -412.569816063959\n","epoch 50/100 return: -361.4339167114313\n","epoch 60/100 return: -611.0729858414842\n","epoch 70/100 return: -411.9538534417843\n","epoch 80/100 return: -564.601921356154\n","epoch 90/100 return: -681.9700715097806\n","###############    Reward for test environment for run 5: -372.1671767468035.   ###############\n","\n","\n","Run 6 out of 10\n","state_dim 12\n","epoch 9000/10000, policy loss 0.0003607085964176804\tepoch 0/100 return: -4.642431000873032\n","epoch 10/100 return: 14.305753000623326\n","epoch 20/100 return: 2.847872952113562\n","epoch 30/100 return: 192.88724815608055\n","epoch 40/100 return: 191.98663071606023\n","epoch 50/100 return: 21.27859799965529\n","epoch 60/100 return: 214.60233811987058\n","epoch 70/100 return: 231.8107837950401\n","epoch 80/100 return: 11.314551870127644\n","epoch 90/100 return: 163.776596925858\n","###############    Reward for test environment for run 6: 56.30087939644841.   ###############\n","\n","\n","Run 7 out of 10\n","state_dim 12\n","epoch 9000/10000, policy loss 0.0046075982972979546\tepoch 0/100 return: 51.54514924907082\n","epoch 10/100 return: 15.072271999511031\n","epoch 20/100 return: -248.74096803044262\n","epoch 30/100 return: 46.03294790181204\n","epoch 40/100 return: -276.84377391859346\n","epoch 50/100 return: -296.5299311345576\n","epoch 60/100 return: -344.3840547115884\n","epoch 70/100 return: -266.6587318280266\n","epoch 80/100 return: -348.0992011648205\n","epoch 90/100 return: 24.89094954108299\n","###############    Reward for test environment for run 7: -212.13025264836918.   ###############\n","\n","\n","Run 8 out of 10\n","state_dim 12\n","epoch 9000/10000, policy loss 0.0007774815312586725\tepoch 0/100 return: -222.56746841800515\n","epoch 10/100 return: -392.720315243345\n","epoch 20/100 return: -34.07964767354079\n","epoch 30/100 return: 265.95896837547707\n","epoch 40/100 return: -229.2597965485688\n","epoch 50/100 return: -154.203508855052\n","epoch 60/100 return: 268.5068449309032\n","epoch 70/100 return: 40.430274468122406\n","epoch 80/100 return: 251.89780252650272\n","epoch 90/100 return: 0.5880379192057603\n","###############    Reward for test environment for run 8: 29.645314231395425.   ###############\n","\n","\n","Run 9 out of 10\n","state_dim 12\n","epoch 9000/10000, policy loss 0.07929482311010361\tepoch 0/100 return: -611.5530557282332\n","epoch 10/100 return: -2.5263458060395863\n","epoch 20/100 return: -467.6750675506474\n","epoch 30/100 return: 31.362358286724714\n","epoch 40/100 return: -483.4963557289501\n","epoch 50/100 return: -333.19262715260845\n","epoch 60/100 return: -589.4014624006192\n","epoch 70/100 return: -454.6499411494592\n","epoch 80/100 return: 234.6401309415051\n","epoch 90/100 return: -398.2642557229014\n","###############    Reward for test environment for run 9: -194.72599692928287.   ###############\n","\n","\n","Run 10 out of 10\n","state_dim 12\n","epoch 9000/10000, policy loss 0.006434864364564419\tepoch 0/100 return: -5.5872251961027075\n","epoch 10/100 return: -63.92135486926017\n","epoch 20/100 return: 274.94519415714433\n","epoch 30/100 return: 28.17844834638106\n","epoch 40/100 return: 234.3669674821483\n","epoch 50/100 return: 156.7827636233237\n","epoch 60/100 return: 275.296493728428\n","epoch 70/100 return: -734.5323349917844\n","epoch 80/100 return: 267.4240621432599\n","epoch 90/100 return: 270.43556088065236\n","###############    Reward for test environment for run 10: -25.51585133904076.   ###############\n","\n","\n","Average reward for 10 repetitions: -115.46370820521763\n","ALL RESULTS TRAIL: [-115.46370820521763]\n","Config: {'ENV': 'LunarLander-v2', 'ALG': 'FINAL_Apr24_BCStudent_noconfound_origindata_trajnum8', 'NUM_TRAJS_GIVEN': 8, 'NUM_TRAINING_ENVS': 2, 'NOISE_DIM': 4, 'REP_SIZE': 16, 'TRAJ_SHIFT': 8, 'SAMPLING_RATE': 5, 'NUM_STEPS_TRAIN': 10000, 'NUM_TRAJS_VALID': 100, 'NUM_REPETITIONS': 10, 'BATCH_SIZE': 64, 'MLP_WIDTHS': 64, 'ADAM_ALPHA': 0.001, 'SGLD_BUFFER_SIZE': 10000, 'SGLD_LEARN_RATE': 0.01, 'SGLD_NOISE_COEF': 0.01, 'SGLD_NUM_STEPS': 100, 'SGLD_REINIT_FREQ': 0.05, 'NUM_STEPS_TRAIN_ENERGY_MODEL': 1000, 'TRIAL': 0, 'METHOD': 'BC-noconfound', 'EXPERT_ALG': 'dqn'}\n","Trial number 0\n","config method =  BC-noconfound\n","config env =  LunarLander-v2\n","Run 1 out of 10\n","state_dim 12\n","epoch 9000/10000, policy loss 0.018251726403832436\tepoch 0/100 return: 258.60812559147627\n","epoch 10/100 return: 253.4158738495969\n","epoch 20/100 return: 12.297549323649193\n","epoch 30/100 return: 10.448382119507613\n","epoch 40/100 return: 179.32193497753173\n","epoch 50/100 return: -134.8032171369471\n","epoch 60/100 return: 176.49316515610337\n","epoch 70/100 return: -58.27721026882532\n","epoch 80/100 return: 221.34675426663853\n","epoch 90/100 return: -213.88121216161954\n","###############    Reward for test environment for run 1: 74.90718225966893.   ###############\n","\n","\n","Run 2 out of 10\n","state_dim 12\n","epoch 9000/10000, policy loss 0.061323393136262894\tepoch 0/100 return: 268.1123102801094\n","epoch 10/100 return: 281.8711772829141\n","epoch 20/100 return: 234.97829111965504\n","epoch 30/100 return: 31.186326571381045\n","epoch 40/100 return: 39.72009813504829\n","epoch 50/100 return: 228.11614069246\n","epoch 60/100 return: 267.8437140913348\n","epoch 70/100 return: 247.1770390812979\n","epoch 80/100 return: 283.10628946787546\n","epoch 90/100 return: 278.28152691991215\n","###############    Reward for test environment for run 2: 119.71324184080333.   ###############\n","\n","\n","Run 3 out of 10\n","state_dim 12\n","epoch 9000/10000, policy loss 0.008002335205674171\tepoch 0/100 return: -51.09853911788939\n","epoch 10/100 return: 277.39655788120433\n","epoch 20/100 return: 247.6643266295086\n","epoch 30/100 return: 245.13434610981645\n","epoch 40/100 return: 269.96208861439356\n","epoch 50/100 return: 245.44529458201458\n","epoch 60/100 return: 185.86323827615223\n","epoch 70/100 return: 254.26320486521777\n","epoch 80/100 return: 246.7419395035816\n","epoch 90/100 return: 265.81030654955265\n","###############    Reward for test environment for run 3: 181.45659798722016.   ###############\n","\n","\n","Run 4 out of 10\n","state_dim 12\n","epoch 9000/10000, policy loss 0.014824860729277134\tepoch 0/100 return: -155.45514569331064\n","epoch 10/100 return: -56.181512947334106\n","epoch 20/100 return: -59.761837936165804\n","epoch 30/100 return: 35.68115279208911\n","epoch 40/100 return: -17.091322695499926\n","epoch 50/100 return: -113.02913564995217\n","epoch 60/100 return: -25.62959417515897\n","epoch 70/100 return: -118.6880640409432\n","epoch 80/100 return: -42.84033475289529\n","epoch 90/100 return: -9.790227742456665\n","###############    Reward for test environment for run 4: 25.701412867636808.   ###############\n","\n","\n","Run 5 out of 10\n","state_dim 12\n","epoch 9000/10000, policy loss 0.031333766877651215\tepoch 0/100 return: 66.21910988043223\n","epoch 10/100 return: 18.545257886620504\n","epoch 20/100 return: 275.7602520279366\n","epoch 30/100 return: 246.61117533068708\n","epoch 40/100 return: 237.20724200266028\n","epoch 50/100 return: 9.32173578295793\n","epoch 60/100 return: 237.18825411961043\n","epoch 70/100 return: -249.74162255238886\n","epoch 80/100 return: -15.548061922185113\n","epoch 90/100 return: -442.14238721835676\n","###############    Reward for test environment for run 5: 89.78276516801114.   ###############\n","\n","\n","Run 6 out of 10\n","state_dim 12\n","epoch 9000/10000, policy loss 0.00560276722535491\tepoch 0/100 return: 221.54176297189287\n","epoch 10/100 return: 244.69360237838012\n","epoch 20/100 return: 147.39942710799116\n","epoch 30/100 return: -14.149582047994498\n","epoch 40/100 return: 271.5574794178567\n","epoch 50/100 return: 234.4345091970316\n","epoch 60/100 return: 249.10210477161448\n","epoch 70/100 return: -32.45501013308215\n","epoch 80/100 return: 247.88703231859301\n","epoch 90/100 return: -112.82362494235548\n","###############    Reward for test environment for run 6: 153.32844924765513.   ###############\n","\n","\n","Run 7 out of 10\n","state_dim 12\n","epoch 9000/10000, policy loss 0.0987289547920227\tepoch 0/100 return: 271.5793954755826\n","epoch 10/100 return: 171.96198992393755\n","epoch 20/100 return: 241.31329140509945\n","epoch 30/100 return: 249.9954195730316\n","epoch 40/100 return: 221.21805410791865\n","epoch 50/100 return: 222.3478314927351\n","epoch 60/100 return: 195.19467500356365\n","epoch 70/100 return: 192.02287557752442\n","epoch 80/100 return: 212.277257311609\n","epoch 90/100 return: 220.45309500198357\n","###############    Reward for test environment for run 7: 171.29912483529424.   ###############\n","\n","\n","Run 8 out of 10\n","state_dim 12\n","epoch 9000/10000, policy loss 0.03804204240441322\tepoch 0/100 return: 269.19024874620675\n","epoch 10/100 return: -283.7257017637663\n","epoch 20/100 return: 284.9993300511503\n","epoch 30/100 return: 259.5135499878494\n","epoch 40/100 return: 256.6171061728911\n","epoch 50/100 return: 260.8688877902791\n","epoch 60/100 return: 266.1973599658631\n","epoch 70/100 return: 231.6325967847411\n","epoch 80/100 return: 263.19791431903207\n","epoch 90/100 return: -211.24922709177008\n","###############    Reward for test environment for run 8: 154.0078271448309.   ###############\n","\n","\n","Run 9 out of 10\n","state_dim 12\n","epoch 9000/10000, policy loss 0.005520906765013933\tepoch 0/100 return: -453.8874725708753\n","epoch 10/100 return: -5.436013239583474\n","epoch 20/100 return: 194.37422385903363\n","epoch 30/100 return: 5.071960715969411\n","epoch 40/100 return: 280.79426643459044\n","epoch 50/100 return: -56.26393460889659\n","epoch 60/100 return: -927.2223362412999\n","epoch 70/100 return: -337.8010739906543\n","epoch 80/100 return: 164.2561522256899\n","epoch 90/100 return: -269.31172253397756\n","###############    Reward for test environment for run 9: -36.18373518306246.   ###############\n","\n","\n","Run 10 out of 10\n","state_dim 12\n","epoch 9000/10000, policy loss 0.013298709876835346\tepoch 0/100 return: 220.43229316139684\n","epoch 10/100 return: -25.286550048444667\n","epoch 20/100 return: 8.946798220194594\n","epoch 30/100 return: -39.116260953369604\n","epoch 40/100 return: 253.27326872169272\n","epoch 50/100 return: -255.6196882166801\n","epoch 60/100 return: -264.74226817987255\n","epoch 70/100 return: 229.21915500287292\n","epoch 80/100 return: -1.5550379521725404\n","epoch 90/100 return: -54.124005843073164\n","###############    Reward for test environment for run 10: 32.14209289387666.   ###############\n","\n","\n","Average reward for 10 repetitions: 96.61549590619349\n","ALL RESULTS TRAIL: [96.61549590619349]\n","Config: {'ENV': 'LunarLander-v2', 'ALG': 'FINAL_Apr24_BCStudent_noconfound_origindata_trajnum16', 'NUM_TRAJS_GIVEN': 16, 'NUM_TRAINING_ENVS': 2, 'NOISE_DIM': 4, 'REP_SIZE': 16, 'TRAJ_SHIFT': 16, 'SAMPLING_RATE': 5, 'NUM_STEPS_TRAIN': 10000, 'NUM_TRAJS_VALID': 100, 'NUM_REPETITIONS': 10, 'BATCH_SIZE': 64, 'MLP_WIDTHS': 64, 'ADAM_ALPHA': 0.001, 'SGLD_BUFFER_SIZE': 10000, 'SGLD_LEARN_RATE': 0.01, 'SGLD_NOISE_COEF': 0.01, 'SGLD_NUM_STEPS': 100, 'SGLD_REINIT_FREQ': 0.05, 'NUM_STEPS_TRAIN_ENERGY_MODEL': 1000, 'TRIAL': 0, 'METHOD': 'BC-noconfound', 'EXPERT_ALG': 'dqn'}\n","Trial number 0\n","config method =  BC-noconfound\n","config env =  LunarLander-v2\n","Run 1 out of 10\n","state_dim 12\n","epoch 9000/10000, policy loss 0.16584226489067078\tepoch 0/100 return: 273.1612328050259\n","epoch 10/100 return: 268.75243950252013\n","epoch 20/100 return: 241.70863154513052\n","epoch 30/100 return: 227.93390198538492\n","epoch 40/100 return: 235.91988839446674\n","epoch 50/100 return: 302.0712722841454\n","epoch 60/100 return: 283.41460954312817\n","epoch 70/100 return: 300.04949583486314\n","epoch 80/100 return: 299.718213166355\n","epoch 90/100 return: 283.16159612130446\n","###############    Reward for test environment for run 1: 235.90995741263646.   ###############\n","\n","\n","Run 2 out of 10\n","state_dim 12\n","epoch 9000/10000, policy loss 0.18521077930927277\tepoch 0/100 return: 286.4193498485795\n","epoch 10/100 return: 223.69721449885128\n","epoch 20/100 return: 299.9385899224133\n","epoch 30/100 return: 298.98144330335265\n","epoch 40/100 return: 196.96027085952596\n","epoch 50/100 return: 11.234100813959888\n","epoch 60/100 return: 263.07885381253743\n","epoch 70/100 return: 237.2050234999818\n","epoch 80/100 return: -17.83358707204309\n","epoch 90/100 return: -50.86493710077219\n","###############    Reward for test environment for run 2: 207.16795470289765.   ###############\n","\n","\n","Run 3 out of 10\n","state_dim 12\n","epoch 9000/10000, policy loss 0.1128711998462677\tepoch 0/100 return: 250.01901510019428\n","epoch 10/100 return: 246.15053790343728\n","epoch 20/100 return: 282.50311806066975\n","epoch 30/100 return: 276.4021916656682\n","epoch 40/100 return: 290.4343826960765\n","epoch 50/100 return: 245.74636576507058\n","epoch 60/100 return: 249.08658517594327\n","epoch 70/100 return: 257.9369260075771\n","epoch 80/100 return: 263.50649206051764\n","epoch 90/100 return: 294.42294246399797\n","###############    Reward for test environment for run 3: 253.33123005282457.   ###############\n","\n","\n","Run 4 out of 10\n","state_dim 12\n","epoch 9000/10000, policy loss 0.16714774072170258\tepoch 0/100 return: 258.979644062888\n","epoch 10/100 return: -10.54169011610567\n","epoch 20/100 return: 303.3922538999137\n","epoch 30/100 return: -8.081539522298442\n","epoch 40/100 return: 258.56804718909655\n","epoch 50/100 return: 225.48004495579556\n","epoch 60/100 return: 243.58185273265133\n","epoch 70/100 return: 253.96885008350955\n","epoch 80/100 return: 265.2913250220334\n","epoch 90/100 return: 236.12638248754848\n","###############    Reward for test environment for run 4: 207.3994567994961.   ###############\n","\n","\n","Run 5 out of 10\n","state_dim 12\n","epoch 9000/10000, policy loss 0.10426127165555954\tepoch 0/100 return: 98.17913661069426\n","epoch 10/100 return: 285.30560934041205\n","epoch 20/100 return: 267.115607650684\n","epoch 30/100 return: 221.57485695961566\n","epoch 40/100 return: 249.88406690737025\n","epoch 50/100 return: 282.7807877820195\n","epoch 60/100 return: 258.26855038887345\n","epoch 70/100 return: 274.3213553240261\n","epoch 80/100 return: -110.33997443276915\n","epoch 90/100 return: 194.93065208283258\n","###############    Reward for test environment for run 5: 205.82938135551106.   ###############\n","\n","\n","Run 6 out of 10\n","state_dim 12\n","epoch 9000/10000, policy loss 0.12751641869544983\tepoch 0/100 return: 244.49566404978597\n","epoch 10/100 return: 176.95773256348025\n","epoch 20/100 return: 256.0864274784211\n","epoch 30/100 return: 266.82352877592604\n","epoch 40/100 return: 298.07124281754864\n","epoch 50/100 return: 245.24527279131877\n","epoch 60/100 return: 241.31610693833784\n","epoch 70/100 return: 271.70530932312664\n","epoch 80/100 return: 182.69840676373022\n","epoch 90/100 return: 183.22244181419023\n","###############    Reward for test environment for run 6: 209.35076390681073.   ###############\n","\n","\n","Run 7 out of 10\n","state_dim 12\n","epoch 9000/10000, policy loss 0.14073260128498077\tepoch 0/100 return: 244.342235119763\n","epoch 10/100 return: 302.82647372851284\n","epoch 20/100 return: 235.27852656820681\n","epoch 30/100 return: 274.30266620354945\n","epoch 40/100 return: 255.9283766694778\n","epoch 50/100 return: 239.411924140625\n","epoch 60/100 return: 291.38860295330346\n","epoch 70/100 return: 258.27198904177965\n","epoch 80/100 return: 248.52534571266105\n","epoch 90/100 return: 250.59941511064147\n","###############    Reward for test environment for run 7: 232.4727027705315.   ###############\n","\n","\n","Run 8 out of 10\n","state_dim 12\n","epoch 9000/10000, policy loss 0.10168943554162979\tepoch 0/100 return: 255.1637461592831\n","epoch 10/100 return: 260.7423627796114\n","epoch 20/100 return: 276.74057439627694\n","epoch 30/100 return: 258.8870331834511\n","epoch 40/100 return: 278.84526256167493\n","epoch 50/100 return: 270.73667028054376\n","epoch 60/100 return: 259.63213406073857\n","epoch 70/100 return: 239.7424288093306\n","epoch 80/100 return: 252.7998323665558\n","epoch 90/100 return: 256.0939584163575\n","###############    Reward for test environment for run 8: 165.21552497278833.   ###############\n","\n","\n","Run 9 out of 10\n","state_dim 12\n","epoch 9000/10000, policy loss 0.10987851768732071\tepoch 0/100 return: 256.23017992243444\n","epoch 10/100 return: 220.15762322983508\n","epoch 20/100 return: 292.5925816263301\n","epoch 30/100 return: 293.1799487889346\n","epoch 40/100 return: 259.0560829022289\n","epoch 50/100 return: 268.77882315240265\n","epoch 60/100 return: 244.92994704287653\n","epoch 70/100 return: 272.455286624095\n","epoch 80/100 return: 266.8462045331719\n","epoch 90/100 return: 262.67663264196324\n","###############    Reward for test environment for run 9: 246.87701042444482.   ###############\n","\n","\n","Run 10 out of 10\n","state_dim 12\n","epoch 9000/10000, policy loss 0.23643629252910614\tepoch 0/100 return: 267.89949858143336\n","epoch 10/100 return: 247.82887812848716\n","epoch 20/100 return: 213.22464506861166\n","epoch 30/100 return: 286.4618386049067\n","epoch 40/100 return: 133.9036023686848\n","epoch 50/100 return: 242.90405430941533\n","epoch 60/100 return: 260.3708770183067\n","epoch 70/100 return: -271.5091600818342\n","epoch 80/100 return: -789.5759444190378\n","epoch 90/100 return: 282.70194497942623\n","###############    Reward for test environment for run 10: 155.71573153914503.   ###############\n","\n","\n","Average reward for 10 repetitions: 211.9269713937086\n","ALL RESULTS TRAIL: [211.9269713937086]\n","Config: {'ENV': 'LunarLander-v2', 'ALG': 'FINAL_Apr24_BCStudent_noconfound_origindata_trajnum32', 'NUM_TRAJS_GIVEN': 32, 'NUM_TRAINING_ENVS': 2, 'NOISE_DIM': 4, 'REP_SIZE': 16, 'TRAJ_SHIFT': 32, 'SAMPLING_RATE': 5, 'NUM_STEPS_TRAIN': 10000, 'NUM_TRAJS_VALID': 100, 'NUM_REPETITIONS': 10, 'BATCH_SIZE': 64, 'MLP_WIDTHS': 64, 'ADAM_ALPHA': 0.001, 'SGLD_BUFFER_SIZE': 10000, 'SGLD_LEARN_RATE': 0.01, 'SGLD_NOISE_COEF': 0.01, 'SGLD_NUM_STEPS': 100, 'SGLD_REINIT_FREQ': 0.05, 'NUM_STEPS_TRAIN_ENERGY_MODEL': 1000, 'TRIAL': 0, 'METHOD': 'BC-noconfound', 'EXPERT_ALG': 'dqn'}\n","Trial number 0\n","config method =  BC-noconfound\n","config env =  LunarLander-v2\n","Run 1 out of 10\n","state_dim 12\n","epoch 9000/10000, policy loss 0.41224774718284607\tepoch 0/100 return: 224.30602951273016\n","epoch 10/100 return: 235.98201002589184\n","epoch 20/100 return: 266.511383857047\n","epoch 30/100 return: 259.8786330759895\n","epoch 40/100 return: 225.18363019293596\n","epoch 50/100 return: 261.6986432109281\n","epoch 60/100 return: 272.3582922132687\n","epoch 70/100 return: 300.1131549967702\n","epoch 80/100 return: 285.4005157041821\n","epoch 90/100 return: 284.03431606668687\n","###############    Reward for test environment for run 1: 251.21825750014847.   ###############\n","\n","\n","Run 2 out of 10\n","state_dim 12\n","epoch 9000/10000, policy loss 0.17507386207580566\tepoch 0/100 return: -489.2689854328031\n","epoch 10/100 return: 254.4226247655842\n","epoch 20/100 return: 303.30106503605\n","epoch 30/100 return: 266.6401948753979\n","epoch 40/100 return: 261.811781000191\n","epoch 50/100 return: 243.7306478549949\n","epoch 60/100 return: 249.09331409187232\n","epoch 70/100 return: 257.6594113609531\n","epoch 80/100 return: 265.1804997913009\n","epoch 90/100 return: 271.6946863678713\n","###############    Reward for test environment for run 2: 229.3137190437527.   ###############\n","\n","\n","Run 3 out of 10\n","state_dim 12\n","epoch 9000/10000, policy loss 0.18702150881290436\tepoch 0/100 return: 255.8437559931541\n","epoch 10/100 return: 276.3843613527464\n","epoch 20/100 return: -209.61685909523038\n","epoch 30/100 return: 280.95560250958204\n","epoch 40/100 return: 210.9100631711357\n","epoch 50/100 return: 233.33346958294797\n","epoch 60/100 return: 244.5624527558505\n","epoch 70/100 return: 239.46205756490923\n","epoch 80/100 return: -64.79966097139018\n","epoch 90/100 return: -287.6595383265127\n","###############    Reward for test environment for run 3: 155.70788883889225.   ###############\n","\n","\n","Run 4 out of 10\n","state_dim 12\n","epoch 9000/10000, policy loss 0.4265291094779968\tepoch 0/100 return: 260.7199844090996\n","epoch 10/100 return: 253.73869027286827\n","epoch 20/100 return: 256.3548874148684\n","epoch 30/100 return: 272.4803513072806\n","epoch 40/100 return: 240.15788725659638\n","epoch 50/100 return: 260.09351262810037\n","epoch 60/100 return: 132.52557680327612\n","epoch 70/100 return: 6.657516707054896\n","epoch 80/100 return: 237.45322723479012\n","epoch 90/100 return: 256.75645847371806\n","###############    Reward for test environment for run 4: 225.01327261288645.   ###############\n","\n","\n","Run 5 out of 10\n","state_dim 12\n","epoch 9000/10000, policy loss 0.09647846966981888\tepoch 0/100 return: 265.6728305641977\n","epoch 10/100 return: 139.7859901351043\n","epoch 20/100 return: 263.1913666583611\n","epoch 30/100 return: 246.75465188253185\n","epoch 40/100 return: 241.6969104788937\n","epoch 50/100 return: 258.35194357565274\n","epoch 60/100 return: 264.04138886140936\n","epoch 70/100 return: 244.083958781845\n","epoch 80/100 return: 278.80959435794307\n","epoch 90/100 return: 295.3504347001709\n","###############    Reward for test environment for run 5: 242.33924784606228.   ###############\n","\n","\n","Run 6 out of 10\n","state_dim 12\n","epoch 9000/10000, policy loss 0.25081029534339905\tepoch 0/100 return: 257.97236461392004\n","epoch 10/100 return: 242.15320020048412\n","epoch 20/100 return: 262.82958592972483\n","epoch 30/100 return: 261.99784162789376\n","epoch 40/100 return: 263.7768895510985\n","epoch 50/100 return: 267.99862117712735\n","epoch 60/100 return: 280.1109829970906\n","epoch 70/100 return: 245.60344238389357\n","epoch 80/100 return: 140.64845545788648\n","epoch 90/100 return: 259.154606389093\n","###############    Reward for test environment for run 6: 234.2923891041182.   ###############\n","\n","\n","Run 7 out of 10\n","state_dim 12\n","epoch 9000/10000, policy loss 0.22221413254737854\tepoch 0/100 return: 122.69020185382955\n","epoch 10/100 return: 213.09887278313295\n","epoch 20/100 return: 223.8766662142665\n","epoch 30/100 return: 51.179840547956246\n","epoch 40/100 return: 219.90547238779095\n","epoch 50/100 return: 290.5545712153395\n","epoch 60/100 return: 236.36280694508082\n","epoch 70/100 return: 269.7428586984876\n","epoch 80/100 return: 247.57478898017598\n","epoch 90/100 return: 272.3992788136156\n","###############    Reward for test environment for run 7: 228.26506516735796.   ###############\n","\n","\n","Run 8 out of 10\n","state_dim 12\n","epoch 9000/10000, policy loss 0.3790929317474365\tepoch 0/100 return: 277.2377747975236\n","epoch 10/100 return: 246.06127463945387\n","epoch 20/100 return: -2.7821711462109278\n","epoch 30/100 return: 140.48980854995298\n","epoch 40/100 return: 240.00593279364963\n","epoch 50/100 return: 260.9032993308955\n","epoch 60/100 return: 298.7089683657106\n","epoch 70/100 return: 285.04437457889827\n","epoch 80/100 return: 216.34574106953693\n","epoch 90/100 return: 284.6414986808637\n","###############    Reward for test environment for run 8: 245.12856009725382.   ###############\n","\n","\n","Run 9 out of 10\n","state_dim 12\n","epoch 9000/10000, policy loss 0.22078737616539001\tepoch 0/100 return: 258.9883983341027\n","epoch 10/100 return: -248.8327617262851\n","epoch 20/100 return: -207.97229847807074\n","epoch 30/100 return: 277.17125697513745\n","epoch 40/100 return: -240.1468466764475\n","epoch 50/100 return: -388.3408369387268\n","epoch 60/100 return: 253.76649859038687\n","epoch 70/100 return: -192.84417169886262\n","epoch 80/100 return: 264.19945307499705\n","epoch 90/100 return: -235.59592515401346\n","###############    Reward for test environment for run 9: 133.00421093807793.   ###############\n","\n","\n","Run 10 out of 10\n","state_dim 12\n","epoch 9000/10000, policy loss 0.2760311961174011\tepoch 0/100 return: 246.84812779477727\n","epoch 10/100 return: 238.5171600143324\n","epoch 20/100 return: 263.8186753868973\n","epoch 30/100 return: -355.56089243183567\n","epoch 40/100 return: 129.89576714818796\n","epoch 50/100 return: 297.06235967170267\n","epoch 60/100 return: 274.8790190783557\n","epoch 70/100 return: 269.0337951858006\n","epoch 80/100 return: 229.91973134389914\n","epoch 90/100 return: 248.56915812264526\n","###############    Reward for test environment for run 10: 221.57394765824108.   ###############\n","\n","\n","Average reward for 10 repetitions: 216.58565588067913\n","ALL RESULTS TRAIL: [216.58565588067913]\n","Config: {'ENV': 'LunarLander-v2', 'ALG': 'FINAL_Apr24_BCStudent_noconfound_origindata_trajnum64', 'NUM_TRAJS_GIVEN': 64, 'NUM_TRAINING_ENVS': 2, 'NOISE_DIM': 4, 'REP_SIZE': 16, 'TRAJ_SHIFT': 64, 'SAMPLING_RATE': 5, 'NUM_STEPS_TRAIN': 10000, 'NUM_TRAJS_VALID': 100, 'NUM_REPETITIONS': 10, 'BATCH_SIZE': 64, 'MLP_WIDTHS': 64, 'ADAM_ALPHA': 0.001, 'SGLD_BUFFER_SIZE': 10000, 'SGLD_LEARN_RATE': 0.01, 'SGLD_NOISE_COEF': 0.01, 'SGLD_NUM_STEPS': 100, 'SGLD_REINIT_FREQ': 0.05, 'NUM_STEPS_TRAIN_ENERGY_MODEL': 1000, 'TRIAL': 0, 'METHOD': 'BC-noconfound', 'EXPERT_ALG': 'dqn'}\n","Trial number 0\n","config method =  BC-noconfound\n","config env =  LunarLander-v2\n","Run 1 out of 10\n","state_dim 12\n","epoch 9000/10000, policy loss 0.36873194575309753\tepoch 0/100 return: 270.89859504681203\n","epoch 10/100 return: 277.0950691069632\n","epoch 20/100 return: 269.24697906881397\n","epoch 30/100 return: 313.8717398414753\n","epoch 40/100 return: 267.6565271386187\n","epoch 50/100 return: 257.9163451534905\n","epoch 60/100 return: 286.50810808620844\n","epoch 70/100 return: 270.12240261508776\n","epoch 80/100 return: 264.68766521421435\n","epoch 90/100 return: 236.93100788912503\n","###############    Reward for test environment for run 1: 264.5082624579462.   ###############\n","\n","\n","Run 2 out of 10\n","state_dim 12\n","epoch 9000/10000, policy loss 0.33734771609306335\tepoch 0/100 return: 269.25018528231817\n","epoch 10/100 return: 258.8805728622108\n","epoch 20/100 return: 240.07671329479376\n","epoch 30/100 return: 261.56428919064865\n","epoch 40/100 return: 270.43117152303125\n","epoch 50/100 return: 273.2450893178326\n","epoch 60/100 return: 290.09233693375506\n","epoch 70/100 return: 271.0653616480573\n","epoch 80/100 return: 298.99144791810966\n","epoch 90/100 return: 216.69425147642437\n","###############    Reward for test environment for run 2: 224.95781138697697.   ###############\n","\n","\n","Run 3 out of 10\n","state_dim 12\n","epoch 9000/10000, policy loss 0.28409087657928467\tepoch 0/100 return: 263.00709926471677\n","epoch 10/100 return: 277.4119019563095\n","epoch 20/100 return: 266.2329107757495\n","epoch 30/100 return: 246.1335016917484\n","epoch 40/100 return: 244.82189811000768\n","epoch 50/100 return: 241.2093752093724\n","epoch 60/100 return: 292.56467384070766\n","epoch 70/100 return: -147.28114139632845\n","epoch 80/100 return: 291.5973876590376\n","epoch 90/100 return: 236.15490244215073\n","###############    Reward for test environment for run 3: 250.24779932950366.   ###############\n","\n","\n","Run 4 out of 10\n","state_dim 12\n","epoch 9000/10000, policy loss 0.523560106754303\tepoch 0/100 return: 259.3262285833059\n","epoch 10/100 return: 279.3138046085548\n","epoch 20/100 return: 265.9885987180148\n","epoch 30/100 return: 264.1081413700515\n","epoch 40/100 return: 282.3288409887462\n","epoch 50/100 return: 271.6981787634553\n","epoch 60/100 return: 224.69989133180567\n","epoch 70/100 return: 126.40967279926849\n","epoch 80/100 return: 275.41129407888906\n","epoch 90/100 return: 298.9954862229265\n","###############    Reward for test environment for run 4: 242.57732916208158.   ###############\n","\n","\n","Run 5 out of 10\n","state_dim 12\n","epoch 9000/10000, policy loss 0.397283673286438\tepoch 0/100 return: -359.0032120587716\n","epoch 10/100 return: 246.38678903135906\n","epoch 20/100 return: 259.09672457583144\n","epoch 30/100 return: 298.69819795090046\n","epoch 40/100 return: 257.8514762946578\n","epoch 50/100 return: 214.82035778472323\n","epoch 60/100 return: 258.6365194329195\n","epoch 70/100 return: 282.11168673603527\n","epoch 80/100 return: -204.12805121313102\n","epoch 90/100 return: 280.7315105311285\n","###############    Reward for test environment for run 5: 175.21315570067466.   ###############\n","\n","\n","Run 6 out of 10\n","state_dim 12\n","epoch 9000/10000, policy loss 0.31246456503868103\tepoch 0/100 return: 241.21044095611853\n","epoch 10/100 return: 282.71375340517983\n","epoch 20/100 return: 270.23711452334055\n","epoch 30/100 return: 259.0739847626319\n","epoch 40/100 return: 130.11858712263148\n","epoch 50/100 return: 230.58248686104363\n","epoch 60/100 return: 278.79031356433313\n","epoch 70/100 return: 249.6546639303651\n","epoch 80/100 return: 259.23269998518197\n","epoch 90/100 return: 258.53227562009533\n","###############    Reward for test environment for run 6: 252.10322699660657.   ###############\n","\n","\n","Run 7 out of 10\n","state_dim 12\n","epoch 9000/10000, policy loss 0.22450247406959534\tepoch 0/100 return: 283.4609305006208\n","epoch 10/100 return: 242.669216564089\n","epoch 20/100 return: 256.95353637849325\n","epoch 30/100 return: 6.380091445346068\n","epoch 40/100 return: 203.6282694865435\n","epoch 50/100 return: 293.00551579611334\n","epoch 60/100 return: 291.12366659591817\n","epoch 70/100 return: 276.3474191406772\n","epoch 80/100 return: 256.8188915135942\n","epoch 90/100 return: 287.4310827130151\n","###############    Reward for test environment for run 7: 245.99904616764317.   ###############\n","\n","\n","Run 8 out of 10\n","state_dim 12\n","epoch 9000/10000, policy loss 0.303594172000885\tepoch 0/100 return: 269.0632634063973\n","epoch 10/100 return: 280.76526564552364\n","epoch 20/100 return: 268.2195143794346\n","epoch 30/100 return: 282.2083343315927\n","epoch 40/100 return: 280.057929374236\n","epoch 50/100 return: 147.0949961322188\n","epoch 60/100 return: 285.05528924858675\n","epoch 70/100 return: 290.0413350284167\n","epoch 80/100 return: 261.677628932796\n","epoch 90/100 return: 231.84293710779266\n","###############    Reward for test environment for run 8: 234.36301613203668.   ###############\n","\n","\n","Run 9 out of 10\n","state_dim 12\n","epoch 9000/10000, policy loss 0.3908320665359497\tepoch 0/100 return: 274.9504611618013\n","epoch 10/100 return: 261.38449988387345\n","epoch 20/100 return: 277.5584221780123\n","epoch 30/100 return: 277.2489947555377\n","epoch 40/100 return: 223.97542513385176\n","epoch 50/100 return: 261.263575339437\n","epoch 60/100 return: 278.7951867005135\n","epoch 70/100 return: -103.8260517823301\n","epoch 80/100 return: 281.9129256060297\n","epoch 90/100 return: 282.6629324415136\n","###############    Reward for test environment for run 9: 245.8568357779299.   ###############\n","\n","\n","Run 10 out of 10\n","state_dim 12\n","epoch 9000/10000, policy loss 0.4535144567489624\tepoch 0/100 return: 256.92416853373425\n","epoch 10/100 return: 243.50954745657825\n","epoch 20/100 return: 255.2060309921227\n","epoch 30/100 return: 258.8252273183821\n","epoch 40/100 return: 252.87016886995764\n","epoch 50/100 return: 212.38038945436858\n","epoch 60/100 return: 266.1112117383387\n","epoch 70/100 return: 260.50273833096895\n","epoch 80/100 return: 270.8022319564777\n","epoch 90/100 return: 265.47971641402273\n","###############    Reward for test environment for run 10: 238.6349539440097.   ###############\n","\n","\n","Average reward for 10 repetitions: 237.44614370554095\n","ALL RESULTS TRAIL: [237.44614370554095]\n","Config: {'ENV': 'LunarLander-v2', 'ALG': 'FINAL_Apr24_BCStudent_noconfound_origindata_trajnum128', 'NUM_TRAJS_GIVEN': 128, 'NUM_TRAINING_ENVS': 2, 'NOISE_DIM': 4, 'REP_SIZE': 16, 'TRAJ_SHIFT': 128, 'SAMPLING_RATE': 5, 'NUM_STEPS_TRAIN': 10000, 'NUM_TRAJS_VALID': 100, 'NUM_REPETITIONS': 10, 'BATCH_SIZE': 64, 'MLP_WIDTHS': 64, 'ADAM_ALPHA': 0.001, 'SGLD_BUFFER_SIZE': 10000, 'SGLD_LEARN_RATE': 0.01, 'SGLD_NOISE_COEF': 0.01, 'SGLD_NUM_STEPS': 100, 'SGLD_REINIT_FREQ': 0.05, 'NUM_STEPS_TRAIN_ENERGY_MODEL': 1000, 'TRIAL': 0, 'METHOD': 'BC-noconfound', 'EXPERT_ALG': 'dqn'}\n","Trial number 0\n","config method =  BC-noconfound\n","config env =  LunarLander-v2\n","Run 1 out of 10\n","state_dim 12\n","epoch 9000/10000, policy loss 0.5633370280265808\tepoch 0/100 return: 236.912843600385\n","epoch 10/100 return: 221.97508107277812\n","epoch 20/100 return: 260.2730357424799\n","epoch 30/100 return: 255.7712684493747\n","epoch 40/100 return: 264.99150080386926\n","epoch 50/100 return: 212.5255370124994\n","epoch 60/100 return: 144.53581176821814\n","epoch 70/100 return: 125.24142763555815\n","epoch 80/100 return: 252.47435645909974\n","epoch 90/100 return: 148.71108700663632\n","###############    Reward for test environment for run 1: 237.28136287524376.   ###############\n","\n","\n","Run 2 out of 10\n","state_dim 12\n","epoch 9000/10000, policy loss 0.47430020570755005\tepoch 0/100 return: 245.64488270050666\n","epoch 10/100 return: 285.5076406642109\n","epoch 20/100 return: 277.48190036343817\n","epoch 30/100 return: 280.18697472285714\n","epoch 40/100 return: 253.3041612965337\n","epoch 50/100 return: 274.6693683654922\n","epoch 60/100 return: 220.02444759070693\n","epoch 70/100 return: 209.8723814006113\n","epoch 80/100 return: 257.9587319295498\n","epoch 90/100 return: 273.61879927917175\n","###############    Reward for test environment for run 2: 248.49602086736067.   ###############\n","\n","\n","Run 3 out of 10\n","state_dim 12\n","epoch 9000/10000, policy loss 0.4663887619972229\tepoch 0/100 return: 276.1444414602398\n","epoch 10/100 return: 291.549023221429\n","epoch 20/100 return: 252.44901947806088\n","epoch 30/100 return: 279.76148395811185\n","epoch 40/100 return: 283.30900358232094\n","epoch 50/100 return: 115.55729275960785\n","epoch 60/100 return: 286.5111291151949\n","epoch 70/100 return: 241.45125632202218\n","epoch 80/100 return: 253.9254102886821\n","epoch 90/100 return: 239.2411152359516\n","###############    Reward for test environment for run 3: 251.5484070490562.   ###############\n","\n","\n","Run 4 out of 10\n","state_dim 12\n","epoch 9000/10000, policy loss 0.520449161529541\tepoch 0/100 return: 262.5012909936451\n","epoch 10/100 return: 277.49741896819734\n","epoch 20/100 return: 257.8629426664064\n","epoch 30/100 return: 273.7896530806578\n","epoch 40/100 return: 259.4572492986063\n","epoch 50/100 return: 265.526493053588\n","epoch 60/100 return: 206.13331633878403\n","epoch 70/100 return: 249.16161421071595\n","epoch 80/100 return: 266.77009262430875\n","epoch 90/100 return: 274.6832857033405\n","###############    Reward for test environment for run 4: 246.5156686100668.   ###############\n","\n","\n","Run 5 out of 10\n","state_dim 12\n","epoch 9000/10000, policy loss 0.319701611995697\tepoch 0/100 return: 280.3738764632202\n","epoch 10/100 return: 236.55908361570383\n","epoch 20/100 return: 261.1641510292674\n","epoch 30/100 return: 252.15292762197896\n","epoch 40/100 return: 259.1333332385267\n","epoch 50/100 return: 245.26907765538468\n","epoch 60/100 return: 263.1003810388091\n","epoch 70/100 return: 261.9018256196331\n","epoch 80/100 return: 288.97944582432484\n","epoch 90/100 return: 304.914730938075\n","###############    Reward for test environment for run 5: 250.34582878789814.   ###############\n","\n","\n","Run 6 out of 10\n","state_dim 12\n","epoch 9000/10000, policy loss 0.3028755486011505\tepoch 0/100 return: 253.07372678047668\n","epoch 10/100 return: 230.72009332977584\n","epoch 20/100 return: 290.19366329051\n","epoch 30/100 return: 282.57958985321346\n","epoch 40/100 return: 266.30854034744857\n","epoch 50/100 return: -66.92617376499021\n","epoch 60/100 return: 255.92973236450004\n","epoch 70/100 return: 218.5573194862638\n","epoch 80/100 return: 266.092315379637\n","epoch 90/100 return: 238.16784500768438\n","###############    Reward for test environment for run 6: 234.0848645315321.   ###############\n","\n","\n","Run 7 out of 10\n","state_dim 12\n","epoch 9000/10000, policy loss 0.3290610909461975\tepoch 0/100 return: 241.21696016899142\n","epoch 10/100 return: 279.34144583557446\n","epoch 20/100 return: 257.70840600377915\n","epoch 30/100 return: 269.58477529008667\n","epoch 40/100 return: 313.52691123955515\n","epoch 50/100 return: 264.23378688533717\n","epoch 60/100 return: 268.204173418012\n","epoch 70/100 return: 277.5515910825497\n","epoch 80/100 return: 276.17453507260257\n","epoch 90/100 return: 284.6568237998742\n","###############    Reward for test environment for run 7: 247.28747284079608.   ###############\n","\n","\n","Run 8 out of 10\n","state_dim 12\n","epoch 9000/10000, policy loss 0.5413573980331421\tepoch 0/100 return: 285.7782900818462\n","epoch 10/100 return: 276.6524615471729\n","epoch 20/100 return: 268.35433628424227\n","epoch 30/100 return: 299.2961937667766\n","epoch 40/100 return: 276.4021206881745\n","epoch 50/100 return: 274.23869282871783\n","epoch 60/100 return: 150.1084144015062\n","epoch 70/100 return: 248.32432954754876\n","epoch 80/100 return: 267.2824524022383\n","epoch 90/100 return: 261.5342944540537\n","###############    Reward for test environment for run 8: 247.1278157688864.   ###############\n","\n","\n","Run 9 out of 10\n","state_dim 12\n","epoch 9000/10000, policy loss 0.4341902732849121\tepoch 0/100 return: 285.0319395968671\n","epoch 10/100 return: 221.39472636553847\n","epoch 20/100 return: 281.81375585985626\n","epoch 30/100 return: 245.83779419735325\n","epoch 40/100 return: 273.71360399780224\n","epoch 50/100 return: 253.67876241844368\n","epoch 60/100 return: 234.98321094398034\n","epoch 70/100 return: 269.7862231720029\n","epoch 80/100 return: 279.44762745221504\n","epoch 90/100 return: 282.50007910963643\n","###############    Reward for test environment for run 9: 237.9659180536139.   ###############\n","\n","\n","Run 10 out of 10\n","state_dim 12\n","epoch 9000/10000, policy loss 0.35191723704338074\tepoch 0/100 return: 284.01297368121146\n","epoch 10/100 return: 288.1260664662825\n","epoch 20/100 return: 144.1308606560654\n","epoch 30/100 return: 261.6809623805883\n","epoch 40/100 return: 240.1204255808011\n","epoch 50/100 return: 270.40355232321474\n","epoch 60/100 return: 282.95268218113006\n","epoch 70/100 return: 212.46130422662964\n","epoch 80/100 return: 241.40113977507437\n","epoch 90/100 return: 242.00750793313173\n","###############    Reward for test environment for run 10: 243.08290609774522.   ###############\n","\n","\n","Average reward for 10 repetitions: 244.37362654821996\n","ALL RESULTS TRAIL: [244.37362654821996]\n"]}]},{"cell_type":"code","source":[""],"metadata":{"id":"K1KNb0-Dooiq"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[""],"metadata":{"id":"ORPRPX6naoql"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[""],"metadata":{"id":"dq08yZ2C6EhZ"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[""],"metadata":{"id":"jT1UWt646AK9"},"execution_count":null,"outputs":[]}]}